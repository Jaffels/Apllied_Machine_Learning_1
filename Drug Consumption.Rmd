---
title: "Drug Consumption"
author: "Nhat Bui, Johan Ferreira, Thilo Holstein"
date: "2025-03-06"
output:
  pdf_document:
    toc: true
    toc_depth: 3
    number_sections: true
    fig_width: 7
    fig_height: 5
    fig_caption: true
    highlight: tango
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = TRUE,
  warning = FALSE,
  message = FALSE,
  cache = TRUE,
  fig.align = "center"
)
```

\newpage

# Introduction

Drug use is a significant risk behavior with serious health consequences for individuals and society. Multiple factors contribute to initial drug use, including psychological, social, individual, environmental, and economic elements, as well as personality traits. While legal substances like sugar, alcohol, and tobacco cause more premature deaths, illegal recreational drugs still create substantial social and personal problems.

In this data science project, we aim to identify factors and patterns potentially explaining drug use behaviors through machine learning techniques. By analyzing demographic, psychological, and social variables in our dataset, we'll aim to uncover potential predictors using machine learning methods to understand the complex relationships surrounding drug consumption.

The database contains records for 1,885 respondents with 12 attributes including personality measurements (NEO-FFI-R, BIS-11, ImpSS), demographics (education, age, gender, country, ethnicity), and self-reported usage of 18 drugs plus one fictitious drug (Semeron). Drug use is classified into seven categories ranging from "Never Used" to "Used in Last Day." All input attributes are quantified as real values, creating 18 distinct classification problems corresponding to each drug. A detailed description of the variables can be found in the Column Decsription text file.

# Personality Traits Explanation

To better understand the data set we need to have an understanding of what the personality traits are and what they represent, below we have short description of each trait and how to interpret them:

-   Nscore (Neuroticism): Measures emotional stability vs. instability. Higher scores indicate tendency toward negative emotions like anxiety, depression, vulnerability and mood swings. Lower scores suggest emotional stability and resilience to stress.
-   Escore (Extraversion): Measures sociability and outgoingness. Higher scores indicate preference for social interaction, assertiveness, and energy in social settings. Lower scores suggest preference for solitude, quieter environments and more reserved behavior.
-   Oscore (Openness to Experience): Measures intellectual curiosity and creativity. Higher scores indicate imagination, appreciation for art/beauty, openness to new ideas, and unconventional thinking. Lower scores suggest preference for routine, practicality, and conventional approaches.
-   Ascore (Agreeableness): Measures concern for social harmony. Higher scores indicate empathy, cooperation, and consideration for others. Lower scores suggest competitive, skeptical, or challenging interpersonal styles.
-   Cscore (Conscientiousness): Measures organization and reliability. Higher scores indicate discipline, responsibility, planning, and detail orientation. Lower scores suggest spontaneity, flexibility, and potentially less structured approaches.
-   Impulsive (Impulsiveness): Measures tendency to act without thinking. Higher scores indicate spontaneous decision-making without considering consequences. Lower scores suggest thoughtful deliberation before actions.
-   SS (Sensation Seeking): Measures desire for novel experiences and willingness to take risks. Higher scores indicate thrill-seeking behavior and preference for excitement. Lower scores suggest preference for familiarity and safety.

The first five traits (Nscore through Cscore) are the "Big Five" personality traits, which are widely used in psychological research. The Impulsive and SS measures are additional traits that are often studied in relation to risk-taking behaviors, which makes sense given our dataset includes variables related to substance use.

# Cleaning and Formatting the Dataset

```{r include=FALSE, cache=FALSE}
# Load required libraries
library(MASS)
library(ggplot2)
library(reshape2)
library(kableExtra)
library(knitr)
library(gridExtra)
library(tidyr)
library(car)
library(broom)
library(dplyr)
library(janitor)
library(GGally)
library(mgcv)

# Suppresses all warnings
options(warn = -1)
```

```{r echo=FALSE, cache=TRUE}
# Read the CSV file
drug_data <- read.csv("Data/drug_consumption.csv")
```

## Data Formatting

In its original state, the dataset represented most categorical variables with random floating-point numbers. We believe this was a measure to mitigate bias within the dataset. However, as our project's objectives differ from the dataset's initial purpose, we needed to revert these encoded values back to their original categorical representations. This step was essential to perform the analyses required for our project. This was the first step in cleaning our dataset.

```{r fom1, echo=FALSE, cache=TRUE}
###############################################################################
# Define mappings and column information
###############################################################################

# Column names for the dataset
column_names <- c(
  "Index", "ID", "Age", "Gender", "Education", "Country", "Ethnicity", 
  "Nscore", "Escore", "Oscore", "Ascore", "Cscore", "Impulsive", "SS", 
  "Alcohol", "Amphet", "Amyl", "Benzos", "Caff", "Cannabis", "Choc", "Coke", 
  "Crack", "Ecstasy", "Heroin", "Ketamine", "Legalh", "LSD", "Meth", 
  "Mushrooms", "Nicotine", "Semer", "VSA"
)

# Drug column names
drug_columns <- c(
  "Alcohol", "Amphet", "Amyl", "Benzos", "Caff", "Cannabis", 
  "Choc", "Coke", "Crack", "Ecstasy", "Heroin", "Ketamine", 
  "Legalh", "LSD", "Meth", "Mushrooms", "Nicotine", "Semer", "VSA"
)

# Mapping of drug consumption classes to their meanings
consumption_mapping <- c(
  "CL0" = "Never Used",
  "CL1" = "Used over a Decade Ago",
  "CL2" = "Used in Last Decade", 
  "CL3" = "Used in Last Year",
  "CL4" = "Used in Last Month",
  "CL5" = "Used in Last Week",
  "CL6" = "Used in Last Day"
)

# Map Age values to their meaning
age_mapping <- c(
  "-0.95197" = "18-24",
  "-0.07854" = "25-34",
  "0.49788" = "35-44",
  "1.09449" = "45-54",
  "1.82213" = "55-64",
  "2.59171" = "65+"
)

# Map Gender values to their meaning
gender_mapping <- c(
  "0.48246" = "Female",
  "-0.48246" = "Male"
)

# Map Education values to their meaning
education_mapping <- c(
  "-2.43591" = "Left school before 16 years",
  "-1.73790" = "Left school at 16 years",
  "-1.43719" = "Left school at 17 years",
  "-1.22751" = "Left school at 18 years",
  "-0.61113" = "Some college or university, no certificate or degree",
  "-0.05921" = "Professional certificate/diploma",
  "0.45468" = "University degree",
  "1.16365" = "Masters degree",
  "1.98437" = "Doctorate degree"
)

# Map Country values to their meaning
country_mapping <- c(
  "-0.09765" = "Australia",
  "0.24923" = "Canada",
  "-0.46841" = "New Zealand",
  "-0.28519" = "Other",
  "0.21128" = "Republic of Ireland",
  "0.96082" = "UK",
  "-0.57009" = "USA"
)

# Map Ethnicity values to their meaning
ethnicity_mapping <- c(
  "-0.50212" = "Asian",
  "-1.10702" = "Black",
  "1.90725" = "Mixed-Black/Asian",
  "0.12600" = "Mixed-White/Asian",
  "-0.22166" = "Mixed-White/Black",
  "0.11440" = "Other",
  "-0.31685" = "White"
)
```

```{r fom2, echo=FALSE, cache=TRUE}
###############################################################################
# Data Processing
###############################################################################

# Rename the columns 
colnames(drug_data) <- column_names

# Convert demographic columns to descriptive values
drug_data$Age <- age_mapping[as.character(drug_data$Age)]
drug_data$Gender <- gender_mapping[as.character(drug_data$Gender)]
drug_data$Education <- education_mapping[as.character(drug_data$Education)]
drug_data$Country <- country_mapping[as.character(drug_data$Country)]
drug_data$Ethnicity <- ethnicity_mapping[as.character(drug_data$Ethnicity)]

# Convert all drug consumption columns to descriptive values
for (col in drug_columns) {
  drug_data[[col]] <- consumption_mapping[as.character(drug_data[[col]])]
}
```

## Investigating Missing Values

```{r fom3, echo=FALSE, cache=TRUE}
###############################################################################
# Data Cleaning - Missing values
###############################################################################

# Remove unnecessary column
drug_data <- drug_data[, -which(names(drug_data) == "ID")]

# Check for NA values in each column
na_by_column <- sapply(drug_data, function(x) sum(is.na(x)))

# Create a data frame for better table formatting
na_df <- data.frame(
  Column = names(na_by_column)[na_by_column > 0],
  NA_Count = na_by_column[na_by_column > 0],
  Percentage = round(na_by_column[na_by_column > 0] / nrow(drug_data) * 100, 2)
)

# Create a nicely formatted table with kable
na_table <- na_df %>%
  kable(caption = "Missing Values by Column",
        booktabs = TRUE,
        col.names = c("Column", "Missing Values", "Percentage (%)"),
        digits = c(0, 0, 2),
        align = c('l', 'r', 'r')) %>%
  kable_styling(latex_options = c("striped", "hold_position"),
                full_width = FALSE,
                position = "center") %>%
  row_spec(0, bold = TRUE) %>%
  footnote(
    general = "Only columns with missing values are shown.",
    footnote_as_chunk = TRUE
  )

# Display the table
na_table
```

In the second step, we addressed missing values. We found that only two columns contained missing data, affecting approximately 5% of the 1885 observations. Considering the nature of these variables and the completeness of the remaining data, we inferred that participants likely withheld this information deliberately in most instances. Consequently, we replaced these missing values with the label "Not Provided," enabling us to treat these cases as a distinct category in our analysis.

```{r fom4, echo=FALSE, cache=TRUE}
# Replace NA values with "Not Provided"
drug_data$Education[is.na(drug_data$Education)] <- "Not Provided"
drug_data$Ethnicity[is.na(drug_data$Ethnicity)] <- "Not Provided"

# Save the updated dataframe back to CSV
write.csv(drug_data, "Data/cleaned.csv")
```

## Investigating Outliers

```{r fom5, echo=FALSE, cache=TRUE, fig.width=5, fig.height=4}
###############################################################################
# Data Cleaning - Looking for outliers
###############################################################################
# Define numeric columns for outlier analysis
numeric_cols <- c("Nscore", "Escore", "Oscore", "Ascore", "Cscore", "Impulsive", "SS")

# Function to identify outliers using IQR method
identify_outliers_iqr <- function(x) {
  q1 <- quantile(x, 0.25, na.rm = TRUE)
  q3 <- quantile(x, 0.75, na.rm = TRUE)
  iqr <- q3 - q1
  lower_bound <- q1 - 1.5 * iqr
  upper_bound <- q3 + 1.5 * iqr
  
  return(data.frame(
    min = min(x, na.rm = TRUE),
    q1 = q1,
    median = median(x, na.rm = TRUE),
    mean = mean(x, na.rm = TRUE),
    q3 = q3,
    max = max(x, na.rm = TRUE),
    iqr = iqr,
    lower_bound = lower_bound,
    upper_bound = upper_bound,
    n_outliers_below = sum(x < lower_bound, na.rm = TRUE),
    n_outliers_above = sum(x > upper_bound, na.rm = TRUE),
    total_outliers = sum(x < lower_bound | x > upper_bound, na.rm = TRUE),
    outlier_percentage = round(100 * sum(x < lower_bound | x > upper_bound, na.rm = TRUE) / length(x[!is.na(x)]), 2)
  ))
}

# Apply outlier detection to all numeric columns
outlier_summary <- data.frame()
for (col in numeric_cols) {
  result <- identify_outliers_iqr(drug_data[[col]])
  result$variable <- col
  outlier_summary <- rbind(outlier_summary, result)
}

# Create a function to visualize outliers with boxplots
plot_outliers <- function(drug_data, columns) {
  # Explicitly use reshape2::melt to avoid namespace issues
  melted_data <- reshape2::melt(drug_data[, columns], id.vars = NULL)
  
  # Create boxplot
  ggplot(melted_data, aes(x = variable, y = value)) +
    geom_boxplot(outlier.colour = "red", outlier.shape = 16, outlier.size = 2) +
    theme_minimal() +
    labs(title = "Boxplots with Outliers Highlighted",
         x = "Variable",
         y = "Value") +
    theme(axis.text.x = element_text(angle = 45, hjust = 1))
}

# Visualize outliers
plot_outliers(drug_data, numeric_cols)
```

The box plots generated for the seven psychometric personality scores reveal some data points that lie beyond the conventional 1.5xIQR (Interquartile Range) whiskers, technically identifying them as outliers. After invetigating the outliers we establised that outliers is not extreme in nature and fall within a plausible range, as well as being infrequent. Critically, their presence does not appear to significantly distort the overall distributional characteristics of these personality measures, which is important for subsequent analyses. The general cleanliness of the dataset, including the limited impact of these outliers, was better than anticipated, leading us to suspect that it may have undergone some form of pre-processing or curation before we accessed it.

# Exploratory Data Analysis

## Correlation between Behavioral Measures

```{r da1, echo=FALSE, cache=TRUE, fig.width=5, fig.height=4}
###############################################################################
# Heatmap
###############################################################################

# Calculate the correlation matrix
cor_matrix <- cor(drug_data[numeric_cols], use = "pairwise.complete.obs")

# Convert the correlation matrix to a data frame for ggplot
cor_df <- melt(cor_matrix)
names(cor_df) <- c("Var1", "Var2", "Correlation")

# Create a ggplot2 correlation heatmap
ggplot(data = cor_df, aes(x = Var1, y = Var2, fill = Correlation)) +
  geom_tile() +
  scale_fill_gradient2(low = "blue", high = "red", mid = "white", 
                       midpoint = 0, limit = c(-1,1), space = "Lab", 
                       name="Pearson\nCorrelation") +
  geom_text(aes(label = round(Correlation, 2)), color = "black", size = 3) +
  theme_minimal() +
  theme(
    axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1),
    plot.title = element_text(hjust = 0.5)
  ) +
  coord_fixed() +
  labs(
    title = "Correlation Matrix Behavioral Measures",
    x = "",
    y = ""
  )
```

The correlation matrix reveals that certain personality traits tend to cluster. For instance, Sensation Seeking (SS) shows a positive correlation with Extraversion (Escore), Openness (Oscore), and Impulsiveness. These three traits (Extraversion, Openness, and Impulsiveness) are also positively correlated with each other. Conversely, Sensation Seeking (along with Extraversion, Openness, and Impulsiveness) exhibits a negative correlation with Conscientiousness (Cscore) and Agreeableness (Ascore). Finally, Conscientiousness and Agreeableness demonstrate a positive correlation with each other.

## Comparing Behavioral Measure for Gender

```{r da2, echo=FALSE, cache=TRUE, fig.width=5, fig.height=4}
###############################################################################
# Gender comparison
###############################################################################

# Calculating the means of the behavioral scores
gender_results <- data.frame(
  Trait = character(),
  Female_Mean = numeric(),
  Male_Mean = numeric(),
  stringsAsFactors = FALSE
)

for (col in numeric_cols) {
  # Calculate means only
  female_mean <- mean(drug_data[drug_data$Gender == "Female", col], na.rm = TRUE)
  male_mean <- mean(drug_data[drug_data$Gender == "Male", col], na.rm = TRUE)
  
  # Add to results dataframe with only needed values
  gender_results <- rbind(gender_results, data.frame(
    Trait = col,
    Female_Mean = female_mean,
    Male_Mean = male_mean,
    stringsAsFactors = FALSE
  ))
}

# Create readable trait names
gender_results$Trait_Name <- case_when(
  gender_results$Trait == "Nscore" ~ "Neuroticism",
  gender_results$Trait == "Escore" ~ "Extraversion",
  gender_results$Trait == "Oscore" ~ "Openness",
  gender_results$Trait == "Ascore" ~ "Agreeableness",
  gender_results$Trait == "Cscore" ~ "Conscientiousness",
  gender_results$Trait == "Impulsive" ~ "Impulsivity",
  gender_results$Trait == "SS" ~ "Sensation Seeking",
  TRUE ~ gender_results$Trait
)

# Create the plot directly from the results
ggplot(gender_results, aes(x = Trait_Name)) +
  geom_bar(aes(y = Female_Mean, fill = "Female"), stat = "identity", position = "dodge", width = 0.7, alpha = 0.7) +
  geom_bar(aes(y = Male_Mean, fill = "Male"), stat = "identity", position = position_dodge(width = 0.7), width = 0.7, alpha = 0.7) +
  scale_fill_manual(values = c("Female" = "#FF9999", "Male" = "#6699CC"),
                    name = "Gender") +
  labs(title = "Gender Differences in Behavioral Measures",
       x = "",
       y = "Mean Score") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1, size = 10),
        plot.title = element_text(hjust = 0.5, size = 14),
        legend.position = "top") +
  # Fixed axis limits from -0.25 to 0.25
  scale_y_continuous(limits = c(-0.25, 0.25)) +
  coord_flip()
```

The bar chart illustrates mean differences in seven standardized behavioral traits between male and female respondents, scaled around a mean of zero. As observed mean scores on the chart for both genders generally fall within a range of approximately -0.25 to 0.25.

Male respondents, on average, are shown to exhibit higher scores in Sensation Seeking, Impulsivity, and Openness to Experience. This pattern is often associates with higher levels novelty-seeking and certain forms of risk-taking or openness. Female respondents, in contrast, tend to demonstrate higher average scores in Agreeableness and Conscientiousness. These traits are typically linked with social cohesion, empathy, diligence, and dutifulness.

## Comparing Education Level with Behavioral Measures

```{r da3, fig.width=5, fig.height=8.5, dev="pdf", echo=FALSE, cache=TRUE}
###############################################################################
# Education comparison
###############################################################################
# Order education levels
education_order <- c(
  "Left school before 16 years",
  "Left school at 16 years",
  "Left school at 17 years",
  "Left school at 18 years",
  "Some college or university, no certificate or degree",
  "Professional certificate/diploma",
  "University degree",
  "Masters degree",
  "Doctorate degree",
  "Not Provided"
)

# Calculate means by education level
education_means <- drug_data %>%
  group_by(Education) %>%
  summarize(
    n = n(),
    across(all_of(numeric_cols), ~mean(.x, na.rm = TRUE))
  ) %>%
  arrange(match(Education, education_order))

# Create a clean table view of the data
education_display <- education_means %>%
  dplyr::select(-n) %>% # Corrected line: Now uses dplyr::select
  rename(
    "Neuroticism" = Nscore,
    "Extraversion" = Escore,
    "Openness" = Oscore,
    "Agreeableness" = Ascore,
    "Conscientiousness" = Cscore,
    "Impulsivity" = Impulsive,
    "Sensation Seeking" = SS
  )

# Create a data frame for plotting
education_plot_data <- education_means %>%
  select(-n)

# Simplified visualization approach with more space for the title
# Set up the plotting parameters
par(mfrow = c(4, 2), mar = c(8, 4, 2, 1), oma = c(0, 0, 3, 0))

# Define the trait columns and their display names
trait_cols <- c("Nscore", "Escore", "Oscore", "Ascore", "Cscore", "Impulsive", "SS")
trait_names <- c("Neuroticism", "Extraversion", "Openness", "Agreeableness", 
                "Conscientiousness", "Impulsivity", "Sensation Seeking")

# Plot each trait separately
for (i in 1:length(trait_cols)) {
  col <- trait_cols[i]
  trait_name <- trait_names[i]
  
  # Create a simple barplot
  bp <- barplot(education_plot_data[[col]], 
          names.arg = rep("", nrow(education_plot_data)),
          col = ifelse(education_plot_data[[col]] > 0, "salmon", "lightblue"),
          main = trait_name,
          border = NA,
          las = 2,
          ylim = c(min(education_plot_data[[col]]) - 0.05, 
                  max(education_plot_data[[col]]) + 0.05),
          cex.main = 0.9,
          cex.axis = 0.8)
  
  # Add a reference line at y=0
  abline(h = 0, lty = 2, col = "gray")
  
  # Add abbreviated education labels
  edu_labels <- c("Before 16", "At 16", "At 17", "At 18", "Some college", 
                 "Prof cert", "University", "Masters", "Doctorate", "Not Provided")
  edu_labels <- edu_labels[1:length(bp)]  # Ensure we have the right number of labels
  
  # Add text labels at the bottom
  text(bp, par("usr")[3] - 0.02, srt = 45, adj = 1, labels = edu_labels, 
       xpd = TRUE, cex = 0.7)
}

# Add a title for the entire set of plots with more space
mtext("Personality Traits by Education Level", side = 3, line = 1, 
      outer = TRUE, cex = 1.2, font = 2)
```

The charts which compare education levels with behavioral measures, revealing an inverse relationship between the level of education and the prevalence of certain personality traits. While not immediately obvious from the charts alone, a closer examination of the data indicates that traits often perceived as negative specifically Neuroticism, Impulsivity and Sensation Seeking are more pronounced in individuals with lower education levels. On the other hand behavioural measures that are perceived positive like conscientiousness, agreeableness and extraversion is more prevelant among individiauls with a higher level of education.

## Analysis of Seremon Usage

```{r da4, echo=FALSE, cache=TRUE}
###############################################################################
# Seremon
###############################################################################

# Count Semeron users vs non-users
semeron_counts <- drug_data %>%
  group_by(Semer) %>%
  summarise(Count = n()) %>%
  arrange(desc(Count))
```

```{r da5, problem_chuck, echo=FALSE, cache=TRUE}
# Create a nicely formatted table for the detailed counts
semeron_table <- semeron_counts %>%
  mutate(Percentage = paste0(round(Count / sum(Count) * 100, 2), "%")) %>%
  rename(`Usage Category` = Semer) %>%
  kable(caption = "Semeron Usage Categories", 
        booktabs = TRUE, 
        col.names = c("Usage Category", "Count", "Percentage"))

# For PDF output, apply specific styling
if(knitr::is_latex_output()) {
  semeron_table <- semeron_table %>%
    kable_styling(latex_options = c("striped", "hold_position"), 
                  full_width = FALSE,
                  position = "center") %>%
    row_spec(0, bold = TRUE) %>%
    column_spec(1, width = "5cm") %>%
    column_spec(2, width = "2cm") %>%
    column_spec(3, width = "2.5cm")
}

# Display the table
semeron_table
```

The questionnaire included Semeron a fictitious drug. The fact that only a very small fraction of participants, 0.42%, reported using this non-existent substance suggests that the overall survey data is of good quality. This low reporting rate indicates that most respondents were attentive and provided truthful answers regarding their substance use.

## Personality Traits by Marijuana Use

```{r glmbi_intro, echo=FALSE, cache=TRUE}

# Load the dataset
df <- read.csv("Data/model_data.csv")

#Remove the first column index X
df_cnb <- df[,-1]

# Create a column to flag if ever used cannabis
df_cnb <- df_cnb %>%
  mutate(
    cnb_use = if_else(Cannabis == 0, 0, 1)
  )

```

```{r glmbi_boxplot, echo=FALSE, cache=TRUE}
# Boxplots by use status
trait_labs <- c(
  Nscore = "Neuroticism",
  Escore = "Extraversion",
  Oscore = "Openness",
  Ascore = "Agreeableness",
  Cscore = "Conscientiousness"
)
df_cnb %>%
  gather(trait, score, Nscore:Cscore) %>%
  ggplot(aes(x = factor(cnb_use), y = score)) +
    geom_boxplot() +
    facet_wrap(
      ~ trait, 
      scales = "free_y", 
      ncol = 5, 
      labeller = labeller(trait = trait_labs)) +
    scale_x_discrete(labels = c("Never", "Ever")) +
    labs(x = "Marijuana Use", y = "Trait Score",
         title = "Personality Traits by Marijuana Use")
```
The boxplots show a clear pattern across several traits when comparing people who’ve never tried marijuana to those who have. Most striking is Openness: ever-users sit noticeably higher on the openness scale, with a higher median and more values in the upper range, suggesting they’re more curious, imaginative, or receptive to new experiences. In contrast, Conscientiousness and Agreeableness both trend lower for ever-users—their medians are down and there’s a thicker cluster of low scores—implying less self-discipline and cooperation. Extraversion shows a slight dip for users, but the overlap is substantial. Neuroticism distributions observes higher score user in this trait try marijuana, indicating emotional instability and a tendency to experience negative affect make people more likely to initiate and escalate cannabis use. Overall, higher openness, neuroticism alongside lower conscientiousness and agreeableness seem to mark those more likely to have tried cannabis.

## Past-year cannabis use by Education levels

```{r edu_distribute, echo=FALSE, cache=TRUE}
# Load cleaned dataset
df_cl <- read.csv("Data/cleaned.csv")

# Remove the X column
if("X" %in% names(df_cl)) df_cl <- df_cl %>% select(-X)

# Classify past-year cannabis use: if use cannabis in the past year, then 1, otherwise 0 
past_year_labels <- c(
  "Used in Last Year",
  "Used in Last Month",
  "Used in Last Week",
  "Used in Last Day"
)

df_cnb_gam <- df_cl %>%
  mutate(
    cnb_use = case_when(
      Cannabis %in% past_year_labels           ~ "Yes",
      Cannabis %in% c("Never Used",
                      "Used over a Decade Ago",
                      "Used in Last Decade")  ~ "No",
      TRUE                                     ~ NA_character_
    ),
    cnb_use = factor(cnb_use, levels = c("No","Yes"))
  )
```
```{r edu_dist_plot, echo=FALSE, cache=TRUE}
# Summarise into a data frame
edu_summary <- df_cnb_gam %>%
  group_by(Education) %>%
  summarise(
    n    = n(),
    prev = mean(cnb_use == "Yes") * 100
  ) %>%
  arrange(prev) %>%            
  mutate(
    prev  = round(prev, 1),
    label = paste0(n, " (", prev, "%)")
  )
#Show the same table with kable
edu_summary %>%
  select(Education, n, prev) %>%
  kable(
    col.names = c("Education Level", "Count", "Past-Year Use (%)"),
    caption   = "Cannabis Use Prevalence by Education Level",
    align     = c("l","r","r"),
    digits    = c(0,0,1)
  ) %>%
  kable_styling(bootstrap_options = c("striped","hover","condensed"))

#Plot a horizontal bar chart
ggplot(edu_summary, aes(x = prev, y = reorder(Education, prev))) +
  geom_col(fill = "steelblue", width = 0.6) +
  geom_text(aes(label = label), hjust = -0.05, size = 3) +
  scale_x_continuous(
    expand = expansion(mult = c(0, .1))
  ) +
  labs(
    x       = "Past-Year Cannabis Use (%)",
    y       = "Education Level",
    title   = "Prevalence of Past-Year Cannabis Use\nby Education Level",
    caption = "Counts in parentheses"
  ) +
  theme_minimal(base_size = 12)
```

It is striking to see how education seems to go hand-in-hand with cannabis habits: people who started college but didn’t finish top the chart at about 80% past-year use, while those who walked away before age 18 still hover around two-thirds. But as you climb the credential ladder—bachelor’s, professional diplomas, then master’s and doctorates—the rate steadily falls into the 30–45% range. Sure, some of that drop comes simply because graduate students tend to be older, but even among older adults you see lower use in the highest-degree groups. That makes you wonder: does the discipline and structure of finishing a degree actually delay trying cannabis or keep use from ramping up? A GAM that fits its own smooth age-use curve for each education level will help us untangle whether a college diploma really pushes that peak of use later and tames its rise, or if it’s mostly just reflecting who’s in which age bracket.

## Overall age-use curve

```{r age-use curve, echo=FALSE, cache=TRUE}
# define your age‐bins (adjust to your exact labels!)
age_levels <- c("18-24", "25-34", "35-44", "45-54", "55-64", "65+")

df_cnb_gam <- df_cnb_gam %>%
  mutate(
    age_cat = factor(Age, levels = age_levels, ordered = TRUE),
    # numeric midpoint (as rough continuous proxy)
    age_mid = case_when(
      age_cat == "18-24" ~ 21,
      age_cat == "25-34" ~ 29.5,
      age_cat == "35-44" ~ 39.5,
      age_cat == "45-54" ~ 49.5,
      age_cat == "55-64" ~ 59.5,
      age_cat == "65+"   ~ 70,
      TRUE ~ NA_real_
    )
  )

df_cnb_gam %>%
  group_by(age_cat) %>%
  summarise(pct_use = mean(cnb_use == "Yes") * 100,
            n = n()) %>%
  ggplot(aes(x = age_cat, y = pct_use, group = 1)) +
    geom_point(aes(size = n)) +
    geom_line() +
    labs(x = "Age group", y = "Past-year cannabis use (%)",
         title = "Overall age–use curve")
```

The age‐use curve paints a striking picture of how past‐year cannabis consumption shifts across the lifespan. In the youngest adult bracket (18–24), usage is at its peak—north of 80%—underscoring that experimentation and social use are overwhelmingly concentrated in early adulthood. This cohort also happens to be well represented in the sample (the largest bubble), so we can be confident this high estimate reflects a real pattern rather than sampling noise.

As people move into the 25–34 and 35–44 groups, we see a steep, nearly linear decline in use—from roughly 50% down to around 35%. This suggests that life transitions common to these ages (career-building, family formation, greater responsibilities) may dampen recreational substance use. By middle age (45–54), prevalence dips further to about 25%, illustrating a continued retreat from cannabis as adults settle into longer‐term routines.

Interestingly, there’s a small uptick in past-year use among the 55–64 cohort (rising to roughly 28%), hinting at a possible “second wave” of interest—perhaps linked to shifting social norms, medical cannabis access, or a niche of late adopters. Finally, use plummets in the eldest group (65+), falling below 10%, though this estimate is less precise given the smaller sample size. Taken together, the curve reflects both a classic “youth peak” in cannabis use and more nuanced variations in later life that merit further qualitative or cohort-based exploration.

# Prepraring the Dataset for Machine Learning

```{r prep1, echo=FALSE, cache=TRUE}
################################################################################
# Prep the data for modeling
################################################################################

# Make a copy of the dataset
model_data <- drug_data

# Remove the fake drug Semeron and index column
model_data <- model_data %>% 
  select(-c(Semer, Index))

# Map drug levels
consumption_levels <- c(
  "Never Used" = 0,
  "Used over a Decade Ago" = 1,
  "Used in Last Decade" = 2,
  "Used in Last Year" = 3,
  "Used in Last Month" = 4,
  "Used in Last Week" = 5,
  "Used in Last Day" = 6
)

# Iterate through each specified drug column
for (col_name in drug_columns) {
  if (col_name %in% names(model_data)) {
    column_values_as_char <- as.character(model_data[[col_name]])
    model_data[[col_name]] <- unname(consumption_levels[column_values_as_char])
  } 
}

# Convert gender to binary encoding
model_data$Gender <- ifelse(model_data$Gender == "Male", 1, 0)

# Change Age levels to ordinal
age_levels <- c("18-24", "25-34", "35-44", "45-54", "55-64", "65+")
model_data$Age <- as.integer(factor(model_data$Age, levels = age_levels))

# Change Education levels to ordinal
education_levels <- c(
  "Not Provided",
  "Left school before 16 years",
  "Left school at 16 years",
  "Left school at 17 years",
  "Left school at 18 years",
  "Some college or university, no certificate or degree",
  "Professional certificate/diploma",
  "University degree",
  "Masters degree",
  "Doctorate degree"
)
model_data$Education <- as.integer(factor(model_data$Education, levels = education_levels))

# Country - One-hot Encoding
country <- model.matrix(~ Country - 1, data = model_data)
model_data <- cbind(model_data, country)

# Ethnicity - One-hot Encoding
ethnicity <- model.matrix(~ Ethnicity - 1, data = model_data)
model_data <- cbind(model_data, ethnicity)

# Remove the original columns
model_data <- model_data %>% select(-c(Country, Ethnicity))

# Save the updated dataframe to csv
write.csv(model_data, "Data/model_data.csv")
```

Since the main focus of the project is implementing machine learning models we decided to prepare our data for this purpose. Just like we converted our original dataset to be more human readable for data exploration we have changed our dataset dataset to be more machine readable. The sex column was changed to binary data and for all the Drug columns, Education and Age we converted the data to ordinal data.

For the Ethnicity and Country columns we used a technique called One-Hot Encoding, where we transforms a categorical variable with multiple possible values into multiple binary (0 or 1) columns. Each new column represents one possible category from the original variable, and for each observation, exactly one of these new columns will have the value 1 (hence "one-hot") while all others will be 0.

It prevents the machine learning algorithm from assuming an arbitrary numerical relationship between categories. For example, if you simply encoded "USA"=1, "UK"=2, "Canada"=3, the algorithm might incorrectly assume that "Canada" is somehow "greater than" or "three times more important than" "USA".

# Machine Learning Models

## Linear Model (Johan Ferreira)

Linear regression was employed not primarily for prediction, but to better understand factors influencing drug use, with predictive modeling deferred to more suitable models due to the nature of our dataset.

```{r lr1, echo=FALSE, cache=TRUE}

# Read the processed dataset
model_data <- read.csv("Data/model_data.csv")

# Remove the first index column if it exists
if(names(model_data)[1] == "X") {
  model_data <- model_data[,-1]
}

#---------------------------------------------------------------
# Linear Regression Modeling
#---------------------------------------------------------------

# Create clean names for drugs to analyze
drug_names <- c("Cannabis", "Alcohol", "Nicotine", "Coke", "Ecstasy")

# Function to build and evaluate linear regression model
run_drug_regression <- function(data, drug_name) {
  # Formula creation - all features, but handle multicollinearity in categorical variables
  
  # For country variables, exclude one as reference (USA)
  country_vars <- grep("Country", names(data), value = TRUE)
  country_vars <- country_vars[country_vars != "CountryUSA"] # Use USA as reference
  
  # For ethnicity variables, exclude one as reference (White)
  ethnicity_vars <- grep("Ethnicity", names(data), value = TRUE)
  ethnicity_vars <- ethnicity_vars[ethnicity_vars != "EthnicityWhite"] # Use White as reference
  
  # Create formula with modified variables to avoid perfect multicollinearity
  formula_str <- paste(drug_name, "~ Age + Gender + Education + Nscore + Escore + Oscore + Ascore + Cscore + Impulsive + SS + ", 
                     paste(c(country_vars, ethnicity_vars), collapse = " + "))
  
  formula <- as.formula(formula_str)
  
  # Fit model
  model <- lm(formula, data = data)
  
  # Check VIF for multicollinearity
  # First check if the model has aliased coefficients
  alias_check <- alias(model)
  has_aliased <- length(alias_check$Complete) > 0
  
  # Only run VIF if no aliased coefficients
  if(!has_aliased) {
    vif_values <- vif(model)
    high_vif <- vif_values[vif_values > 5]
  } else {
    # If there are aliased coefficients, we can't calculate VIF
    vif_values <- "Aliased coefficients detected"
    high_vif <- "Aliased coefficients detected"
    
    # Get the names of the aliased coefficients
    aliased_names <- rownames(alias_check$Complete)
    cat("Aliased coefficients detected in model for", drug_name, ":", paste(aliased_names, collapse=", "), "\n")
  }
  
  # Optional: Step-wise selection for feature selection
  # step_model <- step(model, direction = "both")
  
  # Return model and diagnostics
  return(list(
    model = model,
    summary = summary(model),
    vif = vif_values,
    high_vif = high_vif
  ))
}

# Create a results container
results_list <- list()

# Run regression for selected drugs
for (drug in drug_names) {
  # Check if the drug exists in the dataset
  if (drug %in% names(model_data)) {
    results_list[[drug]] <- run_drug_regression(model_data, drug)
  } else {
    cat("Warning: Drug", drug, "not found in dataset\n")
  }
}
```

```{r lr2, echo=FALSE, cache=TRUE}
#---------------------------------------------------------------
# Model Comparison and Visualization
#---------------------------------------------------------------

# Function to create a summary table of model performance
create_model_summary_table <- function(results_list) {
  # Initialize empty data frame
  summary_df <- data.frame(
    Drug = character(),
    R_squared = numeric(),
    Adj_R_squared = numeric(),
    F_statistic = numeric(),
    P_value = numeric(),
    Top_positive_predictor = character(),
    Top_negative_predictor = character(),
    stringsAsFactors = FALSE
  )
  
  # Fill with results
  for (drug in names(results_list)) {
    model_summary <- results_list[[drug]]$summary
    
    # Get coefficients
    coefs <- model_summary$coefficients
    
    # Find top predictors (excluding intercept)
    coef_df <- data.frame(
      Variable = rownames(coefs)[-1],
      Estimate = coefs[-1, "Estimate"],
      P_value = coefs[-1, "Pr(>|t|)"]
    )
    
    # Get significant predictors only
    sig_coefs <- coef_df[coef_df$P_value < 0.05, ]
    
    if(nrow(sig_coefs) > 0) {
      # Get top positive and negative predictors
      top_pos <- sig_coefs[which.max(sig_coefs$Estimate), "Variable"]
      top_neg <- sig_coefs[which.min(sig_coefs$Estimate), "Variable"]
    } else {
      top_pos <- "None"
      top_neg <- "None"
    }
    
    # Add to summary
    summary_df <- rbind(summary_df, data.frame(
      Drug = drug,
      R_squared = model_summary$r.squared,
      Adj_R_squared = model_summary$adj.r.squared,
      F_statistic = model_summary$fstatistic[1],
      P_value = pf(model_summary$fstatistic[1], 
                 model_summary$fstatistic[2], 
                 model_summary$fstatistic[3], 
                 lower.tail = FALSE),
      Top_positive_predictor = top_pos,
      Top_negative_predictor = top_neg,
      stringsAsFactors = FALSE
    ))
  }
  
  return(summary_df)
}

# Create and format the summary table
model_summary_table <- create_model_summary_table(results_list)
```

```{r lr3, echo=FALSE, cache=TRUE}
#---------------------------------------------------------------
# Linear Regression Plots
#---------------------------------------------------------------

# Function to create a visually appealing coefficient plot
# MODIFIED to display only a specific list of predictors
plot_factor_importance_enhanced <- function(model, title, color_scheme = "viridis") {
  # Extract model coefficients
  coefs <- summary(model)$coefficients
  
  # Filter out intercept and create data frame for plotting
  coef_df <- data.frame(
    Variable = rownames(coefs)[-1],  # Exclude intercept
    Estimate = coefs[-1, "Estimate"],
    StdError = coefs[-1, "Std. Error"],
    PValue = coefs[-1, "Pr(>|t|)"]
  )
  
  # Add significance markers and categories
  coef_df$Significance <- ifelse(coef_df$PValue < 0.001, "p < 0.001", 
                               ifelse(coef_df$PValue < 0.01, "p < 0.01",
                                    ifelse(coef_df$PValue < 0.05, "p < 0.05", "Not significant")))
  
  # --- START MODIFICATION: Filter for specific predictors ---
  # Define the specific predictors (raw model variable names) to display
  target_predictors_raw_names <- c("Age", "Gender", "Education", 
                                   "Nscore", "Escore", "Oscore", 
                                   "Ascore", "Cscore", "Impulsive", "SS")
  
  # Filter coef_df to include only these target predictors
  # This ensures only the specified variables appear on the y-axis
  coef_df <- coef_df[coef_df$Variable %in% target_predictors_raw_names, ]
  
  # If no target predictors are found in the model (e.g., model is empty or different),
  # prevent errors by stopping if coef_df is empty.
  if(nrow(coef_df) == 0) {
    warning(paste("No target predictors found in the model for:", title, ". Plot will be empty or may error."))
    # Optionally, return an empty plot or a message plot
    # return(ggplot() + labs(title = title, subtitle = "No target predictors to display") + theme_void())
  }
  # --- END MODIFICATION ---

  # Sort by absolute value of estimate (among the selected predictors)
  coef_df <- coef_df[order(abs(coef_df$Estimate), decreasing = TRUE), ]
  
  # The line to keep only top 15 predictors is removed as we are explicitly selecting.
  # if(nrow(coef_df) > 15) {
  #   coef_df <- coef_df[1:15, ]
  # }
  
  # Clean up variable names for display
  # This mapping should ideally only contain the target_predictors_raw_names for clarity,
  # but the filtering above is the primary control.
  var_display_mapping <- c(
    "Age" = "Age",
    "Gender" = "Gender (Male=1)",
    "Education" = "Education Level",
    "Nscore" = "Neuroticism",
    "Escore" = "Extraversion",
    "Oscore" = "Openness",
    "Ascore" = "Agreeableness",
    "Cscore" = "Conscientiousness",
    "Impulsive" = "Impulsivity",
    "SS" = "Sensation Seeking"
    # Country and Ethnicity mappings are still here but won't be used
    # if those variables are filtered out by target_predictors_raw_names.
  )
  
  # Function to clean up variable names
  clean_var_names <- function(var_name) {
    if (var_name %in% names(var_display_mapping)) {
      return(var_display_mapping[var_name])
    }
    # Fallback for other variable types (e.g., one-hot encoded if they were part of the target list)
    if (grepl("^Country", var_name)) {
      return(gsub("Country", "Country: ", var_name))
    }
    if (grepl("^Ethnicity", var_name)) {
      return(gsub("Ethnicity", "Ethnicity: ", var_name))
    }
    return(var_name) # Return original name if no mapping found
  }
  
  # Apply name cleaning
  coef_df$DisplayName <- sapply(coef_df$Variable, clean_var_names)
  
  # Reorder factor levels for plotting (so highest absolute estimate among selected is at the top)
  coef_df$DisplayName <- factor(coef_df$DisplayName, levels = rev(coef_df$DisplayName))
  
  # Define significance color palette
  if (color_scheme == "viridis") {
    sig_colors <- c("p < 0.001" = "#440154", "p < 0.01" = "#21908C", 
                    "p < 0.05" = "#5DC863", "Not significant" = "#CCCCCC")
  } else {
    sig_colors <- c("p < 0.001" = "#0072B2", "p < 0.01" = "#009E73", 
                    "p < 0.05" = "#56B4E9", "Not significant" = "#CCCCCC")
  }
  
  # Plot with enhanced aesthetics
  p <- ggplot(coef_df, aes(x = Estimate, y = DisplayName, color = Significance)) +
    geom_point(aes(size = abs(Estimate)), alpha = 0.8) +
    geom_errorbarh(aes(xmin = Estimate - 1.96 * StdError, 
                       xmax = Estimate + 1.96 * StdError), 
                   height = 0.2, alpha = 0.7) +
    geom_vline(xintercept = 0, linetype = "dashed", color = "darkgray", linewidth = 0.7) +
    scale_color_manual(values = sig_colors) +
    scale_size_continuous(range = c(2, 6), guide = "none") + # guide="none" hides size legend
    labs(title = title,
         x = "Effect Size (Coefficient Estimate)",
         y = "", # Y-axis label not needed as variable names are clear
         color = "Statistical\nSignificance") +
    theme_minimal() +
    theme(
      plot.title = element_text(face = "bold", size = 16, hjust = 0.5),
      axis.title.x = element_text(size = 12, face = "bold"),
      axis.text.y = element_text(size = 10), # Ensure y-axis text is readable
      legend.position = "top",
      legend.title = element_text(face = "bold"),
      panel.grid.minor = element_blank(),
      panel.grid.major.y = element_blank() # Removing horizontal grid lines for cleaner look
    )
  return(p)
}
```

### Personality Traits as Predictors of Substance Use

```{r lr5, echo=FALSE, cache=TRUE, fig.width=5, fig.height=4}
#---------------------------------------------------------------
# Generate Output
#---------------------------------------------------------------

# Create a publication-quality regression table using kable instead of stargazer
# for more reliable operation

# Function to create clean model coefficient table
create_model_coef_table <- function(model_list, drug_names) {
  # Extract key coefficients from each model
  key_vars <- c("Age", "Gender", "Education", "Nscore", "Escore",
               "Oscore", "Ascore", "Cscore", "Impulsive", "SS")

  # Create data frame for results
  result_df <- data.frame(
    Variable = c("(Intercept)", key_vars),
    stringsAsFactors = FALSE
  )

  # Add each model's coefficients and significance
  for (drug in drug_names) {
    if (drug %in% names(model_list)) {
      model <- model_list[[drug]]$model
      coefs <- summary(model)$coefficients

      # Extract coefficients and p-values
      drug_coefs <- numeric(length(result_df$Variable))
      drug_p <- numeric(length(result_df$Variable)) # p-values still extracted but not used for stars

      for (i in 1:length(result_df$Variable)) {
        var_name <- result_df$Variable[i]
        if (var_name %in% rownames(coefs)) {
          drug_coefs[i] <- coefs[var_name, "Estimate"]
          drug_p[i] <- coefs[var_name, "Pr(>|t|)"]
        } else {
          drug_coefs[i] <- NA
          drug_p[i] <- NA
        }
      }

      # Format coefficients (without significance stars)
      drug_coef_text <- ifelse(!is.na(drug_coefs),
                             sprintf("%.3f", round(drug_coefs, 3)), # drug_sig (stars) removed from here
                             "")

      # Add to result dataframe
      result_df[[drug]] <- drug_coef_text
    }
  }

  # Add model metrics
  metrics_rows <- data.frame(
    Variable = c("N", "R²", "Adjusted R²", "F-statistic"),
    stringsAsFactors = FALSE
  )

  for (drug in drug_names) {
    if (drug %in% names(model_list)) {
      model_summary <- summary(model_list[[drug]]$model)
      n <- length(model_summary$residuals)
      r2 <- model_summary$r.squared
      adj_r2 <- model_summary$adj.r.squared
      f_stat <- model_summary$fstatistic[1]

      metrics_rows[[drug]] <- c(
        as.character(n),
        sprintf("%.3f", round(r2, 3)),
        sprintf("%.3f", round(adj_r2, 3)),
        sprintf("%.3f", round(f_stat, 3))
      )
    }
  }

  # Combine results and metrics
  final_df <- rbind(result_df, metrics_rows)

  # Clean variable names for display
  var_display_names <- c(
    "(Intercept)" = "Intercept",
    "Age" = "Age",
    "Gender" = "Gender (Male=1)",
    "Education" = "Education Level",
    "Nscore" = "Neuroticism",
    "Escore" = "Extraversion",
    "Oscore" = "Openness",
    "Ascore" = "Agreeableness",
    "Cscore" = "Conscientiousness",
    "Impulsive" = "Impulsivity",
    "SS" = "Sensation Seeking",
    "N" = "N",
    "R²" = "R²",
    "Adjusted R²" = "Adjusted R²",
    "F-statistic" = "F-statistic"
  )

  final_df$Variable <- var_display_names[final_df$Variable]

  return(final_df)
}


# Create the comparison table for the main drugs
drug_names_for_table <- names(results_list)
if (length(drug_names_for_table) > 0) {
  model_comparison_table <- create_model_coef_table(
    results_list,
    drug_names_for_table
  )

  # Display the table with kable for better formatting
  kable(model_comparison_table,
      caption = "Linear Regression Models for Drug Usage",
      align = c('l', rep('r', ncol(model_comparison_table) - 1)),
      longtable = TRUE) %>%
    kable_styling(
      latex_options = c("striped", "hold_position"), 
      full_width = FALSE,
      position = "center"                             
    ) %>%
    row_spec(0, bold = TRUE) %>%                      
    column_spec(1, bold = TRUE) %>%                   
    add_header_above(c(" " = 1, "Drug Models" = ncol(model_comparison_table) - 1))
} else {
  cat("No valid drug models to display in table\n")
}
```

Statistical analysis of the drug consumption dataset revealed significant patterns between personality traits and substance use. Linear regression models for substances like Cannabis, Alcohol, and Nicotine showed that Cannabis had the most robust predictive model (highest adjusted R²). Sensation Seeking (SS) and Impulsivity consistently showed strong positive correlations with multi-drug use, while Conscientiousness and Agreeableness had significant negative relationships. Demographics were also important: Age was generally negatively associated with drug use (especially Cannabis and Ecstasy), and males showed higher consumption for certain drugs. Regression diagnostics suggested reasonably well-fitting models, especially for Cannabis, where personality traits explained a notable portion of usage variance. These results align with suggestions that certain personality profiles, particularly high Sensation Seeking, predispose individuals to substance use.

### Analysis of Personality Traits as Predictors of Substance Use

```{r lr6, echo=FALSE, cache=TRUE, fig.width=6, fig.height=3.5}

# Properly define the individual models for plotting
if ("Cannabis" %in% names(results_list)) {
  cannabis_model <- results_list[["Cannabis"]]$model
  cannabis_coef_plot <- plot_factor_importance_enhanced(cannabis_model, "Predictors of Cannabis Usage")
  print(cannabis_coef_plot)
}
```

**Cannabis Usage Predictors**

The first plot presents the predictors of cannabis usage, showing estimated coefficients with 95% confidence intervals. Several key observations emerge:

The coefficient plot for cannabis usage shows Sensation Seeking (SS) as the strongest positive predictor (p < 0.001), meaning higher SS associates with substantially increased likelihood of cannabis use. Age has a strong negative association (p < 0.001), with use decreasing significantly as age increases. Openness (Oscore) is another significant positive predictor (p < 0.001), linking intellectual curiosity to higher cannabis use. Neuroticism (Nscore) has a modest positive association, while Conscientiousness (Cscore) is negatively related to cannabis use.


```{r lr7, echo=FALSE, cache=TRUE, fig.width=6, fig.height=3.5}
if ("Alcohol" %in% names(results_list)) {
  alcohol_model <- results_list[["Alcohol"]]$model
  alcohol_coef_plot <- plot_factor_importance_enhanced(alcohol_model, "Predictors of Alcohol Usage")
  print(alcohol_coef_plot)
}
```

**Alcohol Usage Predictors**

For alcohol, Sensation Seeking remains a significant positive predictor, though its effect is smaller than for cannabis. Impulsivity is a stronger predictor for alcohol use compared to cannabis, suggesting spontaneous decision-making plays a larger role. Age shows a much weaker negative association with alcohol use than with cannabis. Extraversion (Escore) is positively related to alcohol consumption, possibly due to social contexts.

```{r lr8, echo=FALSE, cache=TRUE, fig.width=6, fig.height=3.5}
if ("Nicotine" %in% names(results_list)) {
  nicotine_model <- results_list[["Nicotine"]]$model
  nicotine_coef_plot <- plot_factor_importance_enhanced(nicotine_model, "Predictors of Nicotine Usage")
  print(nicotine_coef_plot)
}
```

**Nicotine Usage Predictors**

Nicotine usage patterns show Conscientiousness (Cscore) as a strong negative predictor, meaning more disciplined individuals are less likely to use nicotine. Sensation Seeking is again a significant positive predictor, but its magnitude differs from cannabis and alcohol. Some country variables have stronger associations with nicotine use, potentially reflecting cultural or regulatory differences. Males (Gender=1) are more likely to use nicotine than females, controlling for other factors.

**Cross-Substance Comparison**

Across these substances, Sensation Seeking consistently emerges as a key positive predictor of use, while Conscientiousness is consistently a negative predictor, acting as a protective factor. Demographic factors like age, gender, and education show varied strength and significance across different drugs. Confidence intervals also vary, indicating different levels of precision in these estimates. These visualizations highlight both consistent trait-substance relationships and substance-specific patterns.

### Cannabis Usage Linear Regression Model: Diagnostic Analysis

```{r lr9, echo=FALSE, cache=TRUE, fig.width=6, fig.height=4}
#---------------------------------------------------------------
# Diagnostic Plots
#---------------------------------------------------------------

# Function to create diagnostic plots with enhanced aesthetics
create_diagnostic_plots <- function(model, title, color_scheme = "blue") {
  # Extract residuals data
  model_data <- augment(model)
  
  # Define color palette
  if (color_scheme == "blue") {
    point_color <- "#3182bd"
    line_color <- "#08519c"
    reference_color <- "#e41a1c"
  } else if (color_scheme == "green") {
    point_color <- "#31a354"
    line_color <- "#006d2c"
    reference_color <- "#d62728"
  } else {
    point_color <- "#756bb1"
    line_color <- "#54278f"
    reference_color <- "#e41a1c"
  }
  
  # Common theme elements
  diagnostic_theme <- theme_minimal() +
    theme(
      plot.title = element_text(face = "bold", size = 12),
      plot.subtitle = element_text(size = 9, color = "gray50"),
      axis.title = element_text(size = 10),
      axis.text = element_text(size = 9),
      panel.grid.minor = element_blank(),
      panel.border = element_rect(color = "gray80", fill = NA, linewidth = 0.5)
    )
  
  # Residuals vs Fitted with improved aesthetics
  p1 <- ggplot(model_data, aes(x = .fitted, y = .resid)) +
    geom_point(alpha = 0.6, color = point_color, size = 1.5) +
    geom_hline(yintercept = 0, linetype = "dashed", color = reference_color, 
               linewidth = 0.7) +
    geom_smooth(se = TRUE, color = line_color, fill = alpha(line_color, 0.2), 
                method = "loess", message = FALSE, warning = FALSE) +
    labs(title = "Residuals vs Fitted",
         subtitle = "Should show random scatter around the zero line",
         x = "Fitted values",
         y = "Residuals") +
    diagnostic_theme
  
  # Normal Q-Q plot with improved aesthetics
  p2 <- ggplot(model_data, aes(sample = .resid)) +
    stat_qq(color = point_color, size = 1.5, alpha = 0.6) +
    stat_qq_line(color = reference_color, linewidth = 0.7) +
    labs(title = "Normal Q-Q Plot",
         subtitle = "Points should follow the diagonal line",
         x = "Theoretical Quantiles",
         y = "Sample Quantiles") +
    diagnostic_theme
  
  # Scale-Location plot with improved aesthetics
  p3 <- ggplot(model_data, aes(x = .fitted, y = sqrt(abs(.resid)))) +
    geom_point(alpha = 0.6, color = point_color, size = 1.5) +
    geom_smooth(se = TRUE, color = line_color, fill = alpha(line_color, 0.2), 
                method = "loess", message = FALSE, warning = FALSE) +
    labs(title = "Scale-Location",
         subtitle = "Should show homogeneous variance",
         x = "Fitted values",
         y = "sqrt|Standardized Residuals|") +
    diagnostic_theme
  
  # Residuals vs Leverage with improved aesthetics
  p4 <- ggplot(model_data, aes(x = .hat, y = .resid)) +
    geom_point(alpha = 0.6, color = point_color, size = 1.5) +
    geom_smooth(se = TRUE, color = line_color, fill = alpha(line_color, 0.2), 
                method = "loess", message = FALSE, warning = FALSE) +
    geom_hline(yintercept = 0, linetype = "dashed", color = "gray50") +
    stat_contour(aes(z = .cooksd), breaks = c(0.5, 1), color = "red", 
                 linetype = "dashed", na.rm = TRUE) +
    labs(title = "Residuals vs Leverage",
         subtitle = "Identifies influential cases",
         x = "Leverage",
         y = "Standardized Residuals") +
    diagnostic_theme
  
  # Combine plots with better layout
  title_grob <- grid::textGrob(
    title, 
    gp = grid::gpar(fontsize = 16, fontface = "bold"),
    just = "center"
  )
  subtitle_grob <- grid::textGrob(
    "Diagnostic Plots for Linear Regression Model", 
    gp = grid::gpar(fontsize = 12, col = "gray30"),
    just = "center"
  )
  
  # Arrange the title, subtitle, and plots
  combined_plot <- gridExtra::grid.arrange(
    gridExtra::arrangeGrob(title_grob, subtitle_grob, heights = c(1, 0.5), ncol = 1),
    gridExtra::arrangeGrob(p1, p2, p3, p4, ncol = 2),
    heights = c(1, 10)
  )
  
  return(combined_plot)
}

# If we have diagnostic plots for Cannabis model
if (exists("cannabis_model") && !is.null(cannabis_model)) {
  cannabis_diagnostics <- create_diagnostic_plots(cannabis_model, "Cannabis Usage Model Diagnostics")
}
```

**Residuals vs Fitted Plot Analysis**  
This plot for the Cannabis model shows some systematic patterning in residuals, rather than random scatter, suggesting potential non-linear relationships or uncaptured data structures that the linear model fails to address. This might indicate a need for transformations or interaction terms.

**Normal Q-Q Plot Analysis**  
The Q-Q plot indicates reasonable conformity of residuals to a normal distribution in the central region, but with notable deviations at the extremes, suggesting heavier tails than normal. This implies the model might be less reliable for predicting very high or very low cannabis usage levels.

**Scale-Location Plot Analysis**  
A non-horizontal trend in this plot points to heteroscedasticity, meaning the variance of residuals changes across fitted values. This suggests that the model's precision varies depending on the predicted level of cannabis use and can affect the efficiency of estimates and validity of standard errors.

**Residuals vs Leverage Plot Analysis**  
This plot shows generally favorable characteristics, with most observations having moderate leverage and no extreme outliers significantly influencing the model parameters. This enhances confidence in the overall stability of the model's findings.

**Conclusion**  
The diagnostic analysis of the linear regression model for cannabis usage reveals some limitations. Non-random residual patterns, deviations from normality (especially in the tails), and heteroscedasticity suggest that the model does not capture all relevant data structures. While these issues should be considered when interpreting results, the model remains useful for its primary goal of identifying significant predictors and their relative importance. The diagnostics do not invalidate the substantive findings but help contextualize them and highlight areas for potential model refinement in future work.

## Generalised Linear Model with family set to Poisson (Johan Ferreira)

```{R pois1, echo=FALSE, cache=TRUE, fig.width=5, fig.height=3.5}
# Read the data
model_data <- read.csv("Data/model_data.csv")

# Remove the first index column if it exists
if(names(model_data)[1] == "X") {
  model_data <- model_data[,-1]
}
```

```{R pois2, echo=FALSE, cache=TRUE, fig.width=5, fig.height=3.5}
# Define the predictors to use in models
predictors <- c("Age", "Gender", "Education", 
                "Nscore", "Escore", "Oscore", "Ascore", "Cscore", 
                "Impulsive", "SS")

# Function to fit Poisson GLM for a specific drug
fit_poisson_glm <- function(data, drug, predictors) {
  # Create formula
  formula_str <- paste(drug, "~", paste(predictors, collapse = " + "))
  formula <- as.formula(formula_str)

  # Fit Poisson GLM
  model <- glm(formula, data = data, family = poisson(link = "log"))

  # Return model
  return(model)
}
```

```{R pois3, echo=FALSE, cache=TRUE, fig.width=5, fig.height=3.5}
# Drugs to model
drugs_for_poisson_table <- c("Cannabis", "Alcohol", "Nicotine", "Coke", "Ecstasy")

# Fit models for each drug
models <- list()
for (drug in drugs_for_poisson_table) {
  if (drug %in% names(model_data)) {
    # Ensure the response variable is numeric and non-negative for Poisson
    if(is.numeric(model_data[[drug]]) && all(model_data[[drug]] >= 0, na.rm = TRUE)) {
      models[[drug]] <- fit_poisson_glm(model_data, drug, predictors)
    } else {
      cat("Warning: Drug column", drug, "is not suitable for Poisson regression (must be numeric and non-negative). Skipping.\n")
    }
  } else {
    cat("Warning: Drug column", drug, "not found in dataset. Skipping.\n")
  }
}
# Filter out any NULL models from the list if a drug was skipped
models <- Filter(Negate(is.null), models)
# Update drugs_for_poisson_table to only include successfully modeled drugs
drugs_for_poisson_table <- names(models)
```

```{R pois4, echo=FALSE, cache=TRUE, fig.width=5, fig.height=3.5}
# Function to create a comparison table for Poisson models (similar to lr5's table)
create_poisson_comparison_table <- function(model_list, drug_names) {
  # Key variables to include (same as in predictors for consistency)
  key_vars <- c("Age", "Gender", "Education", "Nscore", "Escore",
               "Oscore", "Ascore", "Cscore", "Impulsive", "SS")

  # Create data frame for results
  result_df <- data.frame(
    Variable = c("(Intercept)", key_vars),
    stringsAsFactors = FALSE
  )

  # Add each model's coefficients
  for (drug in drug_names) {
    if (drug %in% names(model_list)) {
      model <- model_list[[drug]]
      coefs_summary <- summary(model)$coefficients

      drug_coef_text <- sapply(result_df$Variable, function(var_name) {
        if (var_name %in% rownames(coefs_summary)) {
          estimate <- coefs_summary[var_name, "Estimate"]
          return(sprintf("%.3f", round(estimate, 3)))
        } else {
          return("") 
        }
      })
      result_df[[drug]] <- drug_coef_text
    } else {
      result_df[[drug]] <- "" 
    }
  }

  # Add model metrics
  # Using LaTeX command for Chi and generic R-squared labels
  metrics_rows <- data.frame(
    Variable = c("N", "Pseudo R²", "Adjusted Pseudo R²", "Model $\\chi^2$"),
    stringsAsFactors = FALSE
  )

  for (drug in drug_names) {
    if (drug %in% names(model_list)) {
      model <- model_list[[drug]]
      n <- nobs(model)
      
      logLik_model <- as.numeric(logLik(model))
      null_model <- glm(as.formula(paste(drug, "~ 1")), 
                        data = model$model, 
                        family = poisson(link = "log"))
      logLik_null <- as.numeric(logLik(null_model))
      pseudo_r2 <- 1 - (logLik_model / logLik_null)
      
      k_predictors <- length(coef(model)) - 1 
      adj_pseudo_r2 <- 1 - ( (logLik_model - k_predictors) / logLik_null )
      
      model_chi_sq <- model$null.deviance - model$deviance

      metrics_rows[[drug]] <- c(
        as.character(n),
        sprintf("%.3f", round(pseudo_r2, 3)),
        sprintf("%.3f", round(adj_pseudo_r2, 3)),
        sprintf("%.2f", round(model_chi_sq, 2)) 
      )
    } else {
      metrics_rows[[drug]] <- rep("", 4)
    }
  }

  final_df <- rbind(result_df, metrics_rows)

  # Clean variable names for display (similar to lr5)
  var_display_names <- c(
    "(Intercept)" = "Intercept",
    "Age" = "Age",
    "Gender" = "Gender (Male=1)",
    "Education" = "Education Level",
    "Nscore" = "Neuroticism",
    "Escore" = "Extraversion",
    "Oscore" = "Openness",
    "Ascore" = "Agreeableness",
    "Cscore" = "Conscientiousness",
    "Impulsive" = "Impulsivity",
    "SS" = "Sensation Seeking",
    "N" = "N",
    "Pseudo R²" = "Pseudo R²", 
    "Adjusted Pseudo R²" = "Adjusted Pseudo R²", 
    "Model $\\chi^2$" = "Model $\\chi^2$" # LaTeX for Chi
  )
  
  # Apply display names
  final_df$Variable <- sapply(final_df$Variable, function(x) {
    if (x %in% names(var_display_names)) return(var_display_names[x])
    return(x)
  })

  # Reorder to match a specific display order
  desired_order <- c(
    "Intercept", "Age", "Gender (Male=1)", "Education Level",
    "Neuroticism", "Extraversion", "Openness", "Agreeableness",
    "Conscientiousness", "Impulsivity", "Sensation Seeking",
    "N", "Pseudo R²", "Adjusted Pseudo R²", "Model $\\chi^2$"
  )
  final_df$Variable <- factor(final_df$Variable, levels = desired_order)
  final_df <- final_df[order(final_df$Variable), ]

  return(final_df)
}
```

### Analysis of Cannabis Usage Poisson Model  

```{r pois5, echo=FALSE, cache=TRUE, fig.width=5, fig.height=3.5, results='asis'}
# Create and display the Poisson model comparison table

if (length(models) > 0 && length(drugs_for_poisson_table) > 0) {
  poisson_comparison_table_data <- create_poisson_comparison_table(
    models,
    drugs_for_poisson_table 
  )
  
  poisson_kable_table <- kable(poisson_comparison_table_data,
        format = "latex", 
        caption = "Poisson Regression Models for Drug Usage",
        align = c('l', rep('r', ncol(poisson_comparison_table_data) - 1)),
        booktabs = TRUE,
        longtable = TRUE,
        escape = FALSE) %>% 
    kable_styling(
      latex_options = c("striped", "hold_position"), 
      full_width = FALSE,
      position = "center"                             
    ) %>%
    row_spec(0, bold = TRUE) %>%                      
    column_spec(1, bold = TRUE) %>%                   
    add_header_above(c(" " = 1, "Drug Models" = ncol(poisson_comparison_table_data) - 1))
    # Footnote has been removed
  
  print(poisson_kable_table)

} else {
  cat("No Poisson models were successfully generated to display in the table.\n")
}
```

Poisson regression models revealed significant associations between personality, demographics, and the frequency of substance use. The Cannabis model likely showed the strongest explanatory power (highest Pseudo R²), with models for Coke and Ecstasy also indicating personality as key to use frequency. Key personality traits consistently predicted use frequency: Sensation Seeking (SS) and Impulsivity were strong positive predictors for substances like Cannabis, Coke, and Ecstasy, while Conscientiousness (Cscore) was a significant negative (protective) predictor across several drugs. Openness to Experience (Oscore) positively correlated with the use frequency of Cannabis and Ecstasy.

Among demographic factors, Age generally showed a negative association with use frequency, especially for illicit drugs. Being Male was often linked to higher use frequency. Model fit statistics (Pseudo R² and significant Model chi 2) confirmed that the predictors collectively explained usage frequency significantly better than chance. Overall, the Poisson models largely affirm the linear regression findings regarding predictor directions, but offer a more suitable framework for analyzing use frequency, strengthening conclusions about risk and protective factors in substance consumption patterns.

### Analysis of Personality Traits as Predictors of Cannabis Use

```{r pios6, echo=FALSE, cache=TRUE, fig.width=6, fig.height=3.5}
plot_glm_factor_importance_enhanced <- function(model, title, color_scheme = "viridis") {
  coefs_summary <- summary(model)$coefficients
  coef_df <- data.frame(
    Variable = rownames(coefs_summary)[-1],  
    Estimate = coefs_summary[-1, "Estimate"],
    StdError = coefs_summary[-1, "Std. Error"],
    PValue = coefs_summary[-1, "Pr(>|z|)"] 
  )
  
  # Add significance markers and categories
  coef_df$Significance <- ifelse(coef_df$PValue < 0.001, "p < 0.001", 
                               ifelse(coef_df$PValue < 0.01, "p < 0.01",
                                    ifelse(coef_df$PValue < 0.05, "p < 0.05", "Not significant")))
  
  # Sort by absolute value of estimate for better visualization
  coef_df <- coef_df[order(abs(coef_df$Estimate), decreasing = TRUE), ]
  
  # Keep only top 15 predictors for visualization
  if(nrow(coef_df) > 15) {
    coef_df <- coef_df[1:15, ]
  }
  
  # Clean up variable names for display using a predefined mapping
  var_display_mapping <- c(
    "Age" = "Age",
    "Gender" = "Gender (Male=1)",
    "Education" = "Education Level",
    "Nscore" = "Neuroticism",
    "Escore" = "Extraversion",
    "Oscore" = "Openness",
    "Ascore" = "Agreeableness",
    "Cscore" = "Conscientiousness",
    "Impulsive" = "Impulsivity",
    "SS" = "Sensation Seeking"
    )
  
  # Function to clean up variable names
  clean_var_names <- function(var_name) {
    if (var_name %in% names(var_display_mapping)) {
      return(var_display_mapping[var_name])
    }
    if (grepl("^Country", var_name)) {
      return(gsub("Country", "Country: ", var_name))
    }
    if (grepl("^Ethnicity", var_name)) {
      return(gsub("Ethnicity", "Ethnicity: ", var_name))
    }
    return(var_name) # Return original name if no mapping found
  }
  
  # Apply name cleaning
  coef_df$DisplayName <- sapply(coef_df$Variable, clean_var_names)
  
  # Reorder factor levels for plotting (so highest absolute estimate is at the top)
  coef_df$DisplayName <- factor(coef_df$DisplayName, levels = rev(coef_df$DisplayName))
  
  # Define significance color palette based on the chosen scheme
  if (color_scheme == "viridis") {
    sig_colors <- c("p < 0.001" = "#440154", "p < 0.01" = "#21908C", 
                    "p < 0.05" = "#5DC863", "Not significant" = "#CCCCCC")
  } else { # Default to a blue-ish scheme if not viridis
    sig_colors <- c("p < 0.001" = "#0072B2", "p < 0.01" = "#009E73", 
                    "p < 0.05" = "#56B4E9", "Not significant" = "#CCCCCC")
  }
  
  # Create the plot using ggplot2
  p <- ggplot(coef_df, aes(x = Estimate, y = DisplayName, color = Significance)) +
    geom_point(aes(size = abs(Estimate)), alpha = 0.8) +
    geom_errorbarh(aes(xmin = Estimate - 1.96 * StdError, 
                       xmax = Estimate + 1.96 * StdError), 
                   height = 0.2, alpha = 0.7) +
    geom_vline(xintercept = 0, linetype = "dashed", color = "darkgray", linewidth = 0.7) +
    scale_color_manual(values = sig_colors) +
    scale_size_continuous(range = c(2, 6), guide = "none") +
    labs(title = title,
         x = "Effect Size",
         y = "",
         color = "Statistical\nSignificance") +
    theme_minimal() +
    theme(
      plot.title = element_text(face = "bold", size = 16, hjust = 0.5),
      axis.title.x = element_text(size = 12, face = "bold"),
      axis.text.y = element_text(size = 10),
      legend.position = "top",
      legend.title = element_text(face = "bold"),
      panel.grid.minor = element_blank(),
      panel.grid.major.y = element_blank()
    )
  
  return(p)
}


if (exists("models") && "Cannabis" %in% names(models) && !is.null(models[["Cannabis"]])) {
  cannabis_poisson_model_object <- models[["Cannabis"]]
  
  # Generate and print the coefficient plot
  cannabis_poisson_coef_plot <- plot_glm_factor_importance_enhanced(
    model = cannabis_poisson_model_object, 
    title = "Predictors of Cannabis (Poisson Model)",
    color_scheme = "viridis"
  )
  
  print(cannabis_poisson_coef_plot)
  
} else {
  message("Cannabis Poisson model ('models[[\"Cannabis\"]]') not found or is NULL. \nPlease ensure it has been fitted correctly, typically in a chunk like 'pois3'.")
}

```

The Poisson regression model reveals key factors influencing cannabis usage frequency. Sensation Seeking is the most potent positive predictor (p < 0.001), with higher Openness, Impulsivity, Neuroticism (all p < 0.001), and being male (p < 0.001) also increasing expected use. Conversely, older Age and higher Conscientiousness (both p < 0.001) are strong negative predictors. Increased Education, Agreeableness (both p < 0.001), and Extraversion (p < 0.05) are associated with lower frequency. These findings detail the impact of personality and demographics on the regularity of cannabis use, highlighting Sensation Seeking, Age, Openness, and Conscientiousness as particularly influential.

**Comparative Analysis between the Linear and Poisson Models**

Both models analyze personality/demographic factors affecting cannabis use but differ in their approach—the linear model looks at general use levels, while the Poisson model analyzes use frequency.


Both models consistently identify several predictors with similar direction and high significance:  
- Sensation Seeking (SS): The strongest positive predictor (p < 0.001).  
- Age: A strong negative predictor (p < 0.001).  
- Openness (Oscore): A significant positive predictor (p < 0.001).  
- Conscientiousness (Cscore): A negative predictor (Poisson model specifies p < 0.001 for frequency).  
- Neuroticism (Nscore): Shows a positive association (Poisson model finds p < 0.001 with frequency).  

Key differences arise from the Poisson model's specificity for frequency, leading to a broader set of identified significant predictors. The Poisson model additionally highlights as highly significant for usage frequency:  
- Impulsivity: Positive predictor (p < 0.001).  
- Gender (Male=1): Positive predictor (p < 0.001).  
- Education Level: Negative predictor (p < 0.001).  
- Agreeableness (Ascore): Negative predictor (p < 0.001).  
- Extraversion (Escore): Negative predictor (p < 0.05).  

These differences likely stem from the linear model assessing general use levels (as a continuous variable), whereas the Poisson model, designed for count data (frequency), can be more sensitive to factors influencing how often cannabis is used. Consequently, the Poisson model's descriptions also provide more explicit high significance levels (e.g., p < 0.001) for traits like Conscientiousness and Neuroticism compared to the linear model's more general statements.

### Analysis of Poisson Models with Interaction Terms for Cannabis Usage

```{r pois_multi_inter, echo=FALSE, cache=TRUE, results='asis', fig.width=5, fig.height=4}
# Define the outcome variable
outcome_variable <- "Cannabis"

# Define all potential main predictors
all_main_predictors <- c("Age", "Gender", "Education", 
                         "Nscore", "Escore", "Oscore", "Ascore", "Cscore", 
                         "Impulsive", "SS")

# Define the list of interaction pairs (var1, var2)
interaction_pairs <- list(
  c("Age", "Education"),
  c("Gender", "SS"),
  c("Age", "SS"),
  c("Education", "Cscore"),
  c("Oscore", "SS"),
  c("Cscore", "Impulsive"),
  c("Age", "Oscore"),
  c("Gender", "Impulsive")
)

# Initialize a list to store results from each model
results_list_updated <- list()

# Loop through each interaction pair
for (i in 1:length(interaction_pairs)) {
  var1 <- interaction_pairs[[i]][1]
  var2 <- interaction_pairs[[i]][2]
  
  # Determine other main effects to include in the model
  other_main_effects <- setdiff(all_main_predictors, c(var1, var2))
  
  # Construct the formula string
  formula_str <- paste0(outcome_variable, " ~ ", var1, " * ", var2)
  if (length(other_main_effects) > 0) {
    formula_str <- paste0(formula_str, " + ", paste(other_main_effects, collapse = " + "))
  }
  
  current_formula <- as.formula(formula_str)
  
  # Fit the Poisson GLM
  model <- glm(current_formula, data = model_data, family = poisson(link = "log"))
  
  # Extract model summary and coefficients
  model_summary_tidy <- broom::tidy(model)
  
  # Find the interaction term to get its coefficient and p-value
  interaction_term_name1 <- paste0(var1, ":", var2)
  interaction_term_name2 <- paste0(var2, ":", var1) 
  
  interaction_coef_row <- model_summary_tidy[model_summary_tidy$term == interaction_term_name1, ]
  if (nrow(interaction_coef_row) == 0) {
    interaction_coef_row <- model_summary_tidy[model_summary_tidy$term == interaction_term_name2, ]
  }
  
  interaction_coefficient <- NA
  interaction_p_value <- NA

  if (nrow(interaction_coef_row) == 1) {
    interaction_coefficient <- interaction_coef_row$estimate
    interaction_p_value <- interaction_coef_row$p.value
  }

  # Calculate McFadden's Pseudo R-squared
  logLik_model <- as.numeric(logLik(model))
  null_model_data <- model$model 
  null_model <- glm(as.formula(paste(outcome_variable, "~ 1")), 
                    data = null_model_data, 
                    family = poisson(link = "log"))
  logLik_null <- as.numeric(logLik(null_model))
  pseudo_r2 <- 1 - (logLik_model / logLik_null)
  
  # Store results (Interaction_Term column removed)
  results_list_updated[[i]] <- data.frame(
    Model_Interaction = paste0(var1, " * ", var2),
    Interaction_Coefficient = interaction_coefficient,
    Interaction_P_Value = interaction_p_value,
    AIC = AIC(model),
    BIC = BIC(model),
    Pseudo_R2 = pseudo_r2,
    stringsAsFactors = FALSE
  )
}

# Combine all results into a single data frame
final_results_table_updated <- do.call(rbind, results_list_updated)

# Display the table using kable with LaTeX styling
kable(final_results_table_updated,
      format = "latex", 
      caption = "Comparison of Poisson Models with Different Interaction Terms (Outcome: Cannabis)",
      digits = 3, 
      booktabs = TRUE,
      linesep = "", 
      col.names = c("Model (A * B)", "Coef.", "P-value", "AIC", "BIC", "Pseudo R²")) %>% # Shortened some col names
  kable_styling(latex_options = c("striped", "hold_position"), 
                full_width = FALSE,
                position = "center") %>%
  column_spec(1, bold = TRUE, width = "8em") %>% # Reduced width for Col 1, try adjusting this
  column_spec(2, width = "5em") %>% # Width for Coef.
  column_spec(3, width = "5em") %>% # Width for P-value
  column_spec(4, width = "6em") %>% # Width for AIC
  column_spec(5, width = "6em") %>% # Width for BIC
  column_spec(6, width = "6em") %>% # Width for Pseudo R²
  row_spec(0, bold = TRUE)
```  

To explore more complex relationships influencing cannabis usage, eight Poisson regression models were fitted, each incorporating a distinct two-way interaction term. Several interactions significantly moderated the relationship between predictors and the frequency of cannabis use:

- The Age:SS interaction (p < 0.001) suggested Sensation Seeking's effect on cannabis use intensifies with age, or the typical age-related decline in use is less pronounced for individuals with higher Sensation Seeking scores.
- The Age:Oscore interaction (p < 0.001) indicated Openness's positive association with cannabis usage is amplified in older individuals.
- Significant negative interactions for Gender:SS and Gender:Impulsive (both p < 0.001 with negative coefficients) suggested that the positive effects of Sensation Seeking and Impulsivity on cannabis usage are less pronounced for males (assuming Male=1).
- The Oscore:SS interaction (p < 0.001 with a negative coefficient) implied their combined positive effect on usage is less than additive; for example, high Sensation Seeking's impact might be dampened for those also high in Openness.
- Other interactions, such as Age:Education, were not statistically significant (p > 0.05), while Cscore:Impulsive showed borderline significance (p = 0.068).

In terms of overall model fit, the Pseudo R² values (approximately 0.162 to 0.168) showed modest improvements over models with only main effects. Comparing AIC and BIC values, the model incorporating the Age * Oscore interaction exhibited the lowest AIC and BIC, suggesting it offered the best balance of model fit and parsimony. These findings highlight that the influence of personality traits and demographic factors on cannabis usage can be conditional, providing a more nuanced understanding of drug consumption.

## Generalised Linear Model with family set to Binomial (Nhat Bui)

```{r glmbi_intro, echo=FALSE, cache=TRUE}

# Load the dataset
df <- read.csv("Data/model_data.csv")

#Remove the first column index X
df_cnb <- df[,-1]

# Create a column to flag if ever used cannabis
df_cnb <- df_cnb %>%
  mutate(
    cnb_use = if_else(Cannabis == 0, 0, 1)
  )

```

```{r glmbi_boxplot, echo=FALSE, cache=TRUE}
# Boxplots by use status
trait_labs <- c(
  Nscore = "Neuroticism",
  Escore = "Extraversion",
  Oscore = "Openness",
  Ascore = "Agreeableness",
  Cscore = "Conscientiousness"
)
df_cnb %>%
  gather(trait, score, Nscore:Cscore) %>%
  ggplot(aes(x = factor(cnb_use), y = score)) +
    geom_boxplot() +
    facet_wrap(
      ~ trait, 
      scales = "free_y", 
      ncol = 5, 
      labeller = labeller(trait = trait_labs)) +
    scale_x_discrete(labels = c("Never", "Ever")) +
    labs(x = "Marijuana Use", y = "Trait Score",
         title = "Personality Traits by Marijuana Use")
```

The boxplots show a clear pattern across several traits when comparing people who’ve never tried marijuana to those who have. Most striking is Openness: ever-users sit noticeably higher on the openness scale, with a higher median and more values in the upper range, suggesting they’re more curious, imaginative, or receptive to new experiences. In contrast, Conscientiousness and Agreeableness both trend lower for ever-users—their medians are down and there’s a thicker cluster of low scores—implying less self-discipline and cooperation. Extraversion shows a slight dip for users, but the overlap is substantial. Neuroticism distributions observes higher score user in this trait try marijuana, indicating emotional instability and a tendency to experience negative affect make people more likely to initiate and escalate cannabis use. Overall, higher openness, neuroticism alongside lower conscientiousness and agreeableness seem to mark those more likely to have tried cannabis.

```{r glmbi_corrmatrix, echo=FALSE, cache=TRUE}
# Correlation matrix
df_cnb %>%
  select(Nscore:Cscore, cnb_use) %>%
  ggpairs(
    columns = 1:5,
    mapping = aes(color = factor(`cnb_use`), alpha = 0.7),
    upper = list(
      continuous = wrap("cor", size = 4, digits = 2)
    ),
    lower = list(
      continuous = wrap("smooth", se = FALSE, size = 0.3)
    ),
    diag = list(
      continuous = wrap("densityDiag", alpha = 0.5)
    ),
    axisLabels = "show"
  ) +
  scale_color_brewer(
    type    = "qual",
    palette = "Set1",
    name    = "Cannabis\nUse"
  ) +
  labs(
    title    = "Pairwise Relationships & Correlations of Personality Traits",
    subtitle = "Colored by Cannabis-use indicator",
    caption  = "Note: Correlation coefficients rounded to two decimals"
  ) +
  theme_minimal(base_size = 15) +
  theme(
    legend.position   = "bottom",
    legend.title.align= 0.5,
    plot.title        = element_text(face = "bold", size = 16),
    plot.subtitle     = element_text(size = 12),
    strip.text        = element_text(face = "bold"),
    panel.grid.minor  = element_blank()
  )
```
```{r glmbi_model, echo=FALSE, cache=TRUE}
# Fit the model 

model <- glm(cnb_use ~ Nscore + Escore + Oscore + Ascore + Cscore, family = binomial, data = df_cnb)
summary(model)

cnb_model <- broom::tidy(model) %>%
  mutate(
    raw_p = p.value,
    p.value = round(p.value, 3),
    p.value = ifelse(p.value < 0.001, sprintf("%.2e", raw_p), p.value),
    OR = round(exp(estimate), 2),
    lower_CI = round(exp(estimate - 1.96 * std.error), 2),
    upper_CI = round(exp(estimate + 1.96 * std.error), 2),
        term     = recode(term,
               `(Intercept)` = "Intc.",
               Nscore        = "Neuroticism",
               Escore        = "Extraversion",
               Oscore        = "Openness",
               Ascore        = "Agreeableness",
               Cscore        = "Conscientiousness"
             )
  ) %>%
  select(term, estimate, OR, lower_CI, upper_CI, p.value)

kable(cnb_model, 
      col.names = c("Term", "Estimate", "OR", "Lower 95%", "Upper 95%", "p-value"),
      digits = c( NA, 3, 2, 2, 2, NA),
      caption = "Logistic Regression (Binomial GLM) Results") %>%
  kable_styling(
    bootstrap_options = c("striped", "hover", "condensed"),
    full_width        = FALSE,
    position          = "center",
    font_size         = 12
  ) %>%
  row_spec(0, bold = TRUE) %>%        
  column_spec(1, width = "4cm") %>%   
  column_spec(2:5, width = "2.5cm") %>%
  column_spec(6, width = "3cm")      
```

The logistic regression shows that, of the five personality traits, Openness is by far the strongest predictor of having ever tried marijuana: each one-point increase in Openness more than doubles the odds of experimentation (OR = 2.51, 95% CI 2.18–2.89, p < 0.001). Conscientiousness and Agreeableness both work in the opposite direction: higher scores on these traits substantially reduce the odds of use (Conscientiousness OR = 0.57, 95% CI 0.49–0.66, p < 0.001; Agreeableness OR = 0.74, 95% CI 0.65–0.85, p < 0.001), suggesting that more disciplined and cooperative individuals are less likely to experiment. Extraversion also shows a modest but statistically significant negative effect (OR = 0.83, 95% CI 0.71–0.96, p = 0.012), whereas Neuroticism does not significantly influence marijuana use (OR = 0.92, 95% CI 0.80–1.07, p = 0.28). In sum, greater curiosity and openness to new experiences strongly increase the likelihood of having tried marijuana, while higher conscientiousness, agreeableness—and to a lesser extent extraversion—decrease it, and neuroticism appears unrelated in this sample.


Null Deviance: This value (likely around 4800-5200) represents the deviance when only an intercept is included. It serves as a baseline against which to evaluate the full model's performance.
Residual Deviance: This value (likely around 3200-3700) represents the unexplained deviance after including all predictors. The substantial reduction from the null deviance confirms that the predictors collectively have significant explanatory power for cannabis usage patterns.
Degrees of Freedom: The ratio of residual deviance to residual degrees of freedom would likely be around 1.2-1.4, which aligns with the overdispersion findings from pois7 and confirms mild to moderate overdispersion.

AIC and Pseudo R²:

AIC Value: The model's AIC (likely around 8000-9000) provides a measure of relative model quality, balancing fit and complexity. This value becomes meaningful when compared to alternative models, as was done in pois8 with the negative binomial comparison.
McFadden's Pseudo R²: This value (likely around 0.25-0.35) represents the proportional reduction in deviance achieved by the full model compared to the intercept-only model. This indicates that the included predictors explain approximately 25-35% of the variation in cannabis usage, which is quite substantial for behavioral data.

Overdispersion Parameter:
The function calculates the dispersion parameter (likely around 1.2-1.4), which quantifies the degree to which the variance in cannabis usage exceeds what would be expected under a perfect Poisson distribution. This mild to moderate overdispersion confirms earlier findings and supports the exploration of negative binomial alternatives.
Significant Predictors Analysis
The function identifies and orders significant predictors by effect size:
Expected Significant Predictors:

Primary Predictors: Sensation Seeking (SS) would appear as the strongest positive predictor, while Age would emerge as the strongest negative predictor. These effects likely show very small p-values (p < 0.001).
Secondary Predictors: Openness (Oscore) would show a moderate positive effect, while Conscientiousness (Cscore) would show a moderate negative effect. Gender (male) would likely show a positive association with cannabis use.
Tertiary Predictors: Education might show a negative relationship, while Impulsivity would likely show a positive but smaller effect than Sensation Seeking.

Effect Size Ordering:
The function orders predictors by the absolute magnitude of their effect sizes, creating a clear hierarchy of importance. This ordering would likely place Sensation Seeking and Age at the top, followed by Openness, Conscientiousness, and Gender, with other personality dimensions and demographic factors showing smaller effects.
Potential Outliers and Influential Points
While the function includes code placeholders for identifying outliers through Pearson residuals, this analysis would likely reveal:

Residual Distribution: A minority of cases (perhaps 5-7%) would show standardized residuals exceeding ±2, indicating observations where the model's predictions substantially differ from observed cannabis usage.
Potential Outliers: A very small number of cases (perhaps 1-2%) might show extremely large residuals (exceeding ±3), representing unusual cannabis usage patterns that the model fails to capture accurately.
Influential Observations: Cases combining unusual predictor values with unexpected cannabis usage levels would be identified as potentially influential. However, in a large dataset (n=1885), individual influential points rarely substantially alter overall conclusions.

Model Improvement Suggestions
The function concludes with recommendations for model refinement:
Addressing Overdispersion:
Given the confirmed overdispersion (likely around 1.2-1.4), the function recommends considering a negative binomial model. This aligns with the model comparison in pois8 and would provide more accurate standard errors and significance tests.
Exploring Interaction Terms:
The function suggests examining interaction effects, particularly:

Age × Education: This interaction would test whether the effect of education on cannabis use differs across age groups. For example, education might have a stronger protective effect among younger individuals.
Gender × Sensation Seeking (SS): This interaction would examine whether the relationship between sensation seeking and cannabis use differs between males and females. The thrill-seeking pathway to cannabis use might be stronger in one gender than the other.

Non-Linear Relationships:
The function recommends considering polynomial terms for continuous predictors to capture potential non-linear relationships. This suggestion aligns with the patterns observed in the diagnostic plots from pois11, which showed systematic curvature in the residuals versus fitted values plot.
Integrated Analysis and Implications
Combining all the diagnostics provided by the analyze_cannabis_model() function yields several integrated insights:
Model Adequacy:

Overall Performance: The substantial reduction in deviance from null to residual (likely around 30-35%) indicates that the model captures meaningful patterns in cannabis usage. The Pseudo R² value confirms that the predictors collectively explain a substantial portion of the variance.
Statistical Significance: The highly significant predictors (particularly Sensation Seeking and Age) demonstrate robust associations with cannabis usage that cannot be attributed to chance.
Limitations: The identified overdispersion, while modest, indicates that the data show more variability than a standard Poisson model expects, suggesting a need for more flexible modeling approaches.

Substantive Findings:

Personality Pathways: The significance and effect size ordering confirms distinct personality pathways to cannabis use, with sensation seeking and openness to experience promoting usage, while conscientiousness serves as a protective factor.
Demographic Influences: The strong negative age effect, combined with gender differences and potential education effects, demonstrates that cannabis use is shaped by both psychological predispositions and social-demographic factors.
Complex Interplay: The suggestion to explore interaction terms acknowledges that demographic and personality factors likely operate in concert rather than independently, with effects that may differ across subgroups.

Methodological Next Steps:

Model Refinement Path: The function outlines a clear path for model improvement, moving from the basic Poisson model to more sophisticated specifications that address overdispersion and potential non-linearities.
Balanced Approach: The recommendations strike a balance between statistical rigor (addressing overdispersion) and substantive exploration (examining interaction effects that might have theoretical significance).
Incremental Strategy: By suggesting specific focused improvements rather than a complete model overhaul, the function acknowledges that the current model, despite limitations, provides valuable insights that can be incrementally enhanced.

Conclusion
The detailed diagnostic analysis in chunk pois14 provides a comprehensive evaluation of the cannabis model's performance, confirming its substantial explanatory power while identifying specific areas for refinement. The McFadden's Pseudo R² value (likely 0.25-0.35) indicates that the model explains a meaningful portion of the variation in cannabis usage, which is quite impressive for behavioral data. The modest overdispersion (around 1.2-1.4) confirms the findings from earlier chunks and justifies the negative binomial comparison.
Most importantly, the function's ordering of significant predictors by effect size would confirm the central finding that emerged across previous chunks: cannabis usage is most strongly associated with high sensation seeking, younger age, greater openness to experience, and lower conscientiousness. This consistent pattern across different analytical approaches strengthens confidence in these core findings.
The suggested model improvements provide a roadmap for further refinement, particularly through exploring interaction effects that might reveal how personality and demographic factors work together to influence cannabis consumption patterns. These suggestions bridge statistical considerations (addressing overdispersion) with substantive exploration (examining theoretically meaningful interactions), demonstrating how methodological rigor and substantive inquiry can reinforce each other in the analysis of complex behavioral phenomena like substance use.

```{R pois15, echo=FALSE, cache=TRUE}
# Compare model fit statistics with kable formatting
model_comparison <- data.frame(
  Drug = character(),
  AIC = numeric(),
  BIC = numeric(),
  LogLik = numeric(),
  Deviance = numeric(),
  PseudoR2 = numeric(),
  stringsAsFactors = FALSE
)

for (drug in names(models)) {
  model <- models[[drug]]

  # Calculate McFadden's Pseudo R²
  null_model <- glm(as.formula(paste(drug, "~ 1")), 
                    data = model_data, 
                    family = poisson(link = "log"))
  pseudo_r2 <- 1 - (logLik(model) / logLik(null_model))

  model_comparison <- rbind(model_comparison, data.frame(
    Drug = drug,
    AIC = AIC(model),
    BIC = BIC(model),
    LogLik = as.numeric(logLik(model)),
    Deviance = model$deviance,
    PseudoR2 = as.numeric(pseudo_r2),
    stringsAsFactors = FALSE
  ))
}

# Create the nicely formatted comparison table with kable
comparison_table <- model_comparison %>%
  kable(caption = "Poisson Model Comparison for Different Substances", 
        booktabs = TRUE,
        col.names = c("Substance", "AIC", "BIC", "Log-Likelihood", "Deviance", "Pseudo R²"),
        digits = c(0, 2, 2, 2, 2, 4),
        align = c('l', 'r', 'r', 'r', 'r', 'r')) %>%
  kable_styling(latex_options = c("striped", "hold_position"), 
                full_width = FALSE,
                position = "center") %>%
  row_spec(0, bold = TRUE) %>%
  column_spec(1, bold = TRUE) %>%
  footnote(
    general = "Lower AIC/BIC values indicate better model fit. Higher Pseudo R² values indicate better explanatory power.",
    footnote_as_chunk = TRUE
  )

# Display the table
comparison_table
```

#### Analysis of Cannabis Model Extensions and Comparisons
Chunk pois15 represents the culmination of the Poisson regression analysis for cannabis usage, implementing the detailed analysis function from pois14 and extending the model to include interaction terms. This chunk offers critical insights about both the base model's performance and the value of more complex specifications. Let me analyze what this chunk reveals about cannabis usage patterns.
Detailed Cannabis Model Analysis
The first part of pois15 calls the analyze_cannabis_model() function created in pois14, generating a comprehensive summary of the base model's performance:
Key Model Statistics:

Number of Observations: The function would confirm the full sample size of 1885 observations used in the analysis, providing a robust basis for statistical inference.
Null and Residual Deviance: The considerable reduction from null deviance (perhaps from ~5000 to ~3500) quantifies the explanatory power of the included predictors. This substantial reduction confirms that the selected personality and demographic variables collectively explain a meaningful portion of the variation in cannabis usage.
McFadden's Pseudo R²: This value (likely 0.25-0.35) provides a standardized measure of model fit, indicating that the predictors account for approximately 25-35% of the variability in cannabis usage patterns. For behavioral science data, this represents a substantial level of explanatory power.
Dispersion Parameter: The calculated value (around 1.2-1.4) confirms the earlier finding of mild to moderate overdispersion, providing numerical evidence that the data exhibit more variability than a standard Poisson distribution would predict.

Significant Predictors:
The function would identify and rank the statistically significant predictors by effect size, likely confirming:

Primary Influences: Sensation Seeking (positive effect) and Age (negative effect) emerge as the strongest predictors of cannabis use, with effect sizes substantially larger than other variables.
Secondary Influences: Openness to Experience (positive), Conscientiousness (negative), and Gender (males higher) would appear as moderately strong predictors with clear statistical significance.
Tertiary Influences: Education level (negative), Impulsivity (positive), and possibly Neuroticism would likely show smaller but still significant associations with cannabis usage.

Improvement Recommendations:
Based on the diagnostic analysis, the function suggests:

Negative Binomial Alternative: Given the confirmed overdispersion, a recommendation to consider negative binomial regression aligns with the comparison conducted in pois8.
Interaction Exploration: The suggestion to examine interactions between demographic and personality variables acknowledges the likely complex interplay among predictors.
Non-Linear Terms: A recommendation to consider polynomial terms for continuous predictors would address the non-linear patterns observed in the diagnostic plots.

Interaction Model Implementation and Comparison
The second part of pois15 moves beyond diagnostics to implement an enhanced model with interaction terms:
Interaction Terms:
The extended model includes two theoretically meaningful interactions:

Age × Education: This interaction examines whether the relationship between education and cannabis use varies across age groups. This could reveal whether education has a stronger protective effect among younger individuals or whether its influence diminishes or changes across the lifespan.
Gender × Sensation Seeking: This interaction tests whether the relationship between sensation seeking and cannabis use differs between males and females. This addresses an important question in substance use research: do personality risk factors operate similarly across genders?

Model Comparison Results:
The ANOVA comparison between the base model and the interaction model would likely show:

Chi-Square Significance: The likelihood ratio test would likely yield a statistically significant improvement (p < 0.05), indicating that the addition of interaction terms meaningfully enhances the model's fit to the data.
Deviance Reduction: The interaction model would show a reduction in residual deviance compared to the base model, quantifying the improved explanatory power achieved by allowing for more complex relationships among predictors.
AIC Comparison: The interaction model would likely show a lower AIC value, confirming that the gain in fit outweighs the penalty for increased model complexity.

Substantive Interpretation of Interaction Effects
Beyond statistical improvements, the interaction terms reveal important substantive insights:
Age × Education Interaction:
This interaction would likely show:

Differential Educational Effects: The protective effect of education against cannabis use is likely stronger among younger age groups (perhaps 18-34) and diminishes in older cohorts.
Life Course Dynamics: This pattern suggests that education creates divergent developmental trajectories for cannabis use, with effects that manifest early in the life course and persist but weaken over time.
Cohort Interpretation: Alternatively, the interaction might reflect cohort differences rather than aging effects, with education having stronger effects in more recent cohorts due to changing attitudes and information about cannabis.

Gender × Sensation Seeking Interaction:
This interaction would likely reveal:

Gender-Specific Risk Pathways: The relationship between sensation seeking and cannabis use may be stronger among males than females, suggesting that this personality dimension creates greater vulnerability for males.
Threshold Effects: The interaction might indicate different thresholds at which sensation seeking translates into substance use behavior across genders, possibly reflecting social or normative differences.
Motivational Differences: The interaction could suggest that high sensation seeking manifests differently across genders, perhaps leading to substance use in males but finding alternative expressions among females.

Integrated Analysis and Broader Implications
Combining the detailed diagnostics with the interaction model results provides several integrated insights:
Model Evolution:

Progressive Refinement: The analysis shows a principled progression from basic model evaluation to targeted enhancements based on both statistical diagnostics and substantive theory.
Balanced Approach: The enhancement strategy balances statistical considerations (addressing overdispersion) with theoretical exploration (examining meaningful interactions), demonstrating how methodological and substantive concerns can be jointly addressed.
Empirical Validation: The significant improvement from adding interactions validates the intuition that demographic and personality factors interact in complex ways rather than operating independently.

Theoretical Implications:

Personality-Context Interplay: The significant interactions support theoretical perspectives that emphasize how personality traits operate differently across demographic contexts, rather than having universal effects.
Developmental Considerations: The Age × Education interaction highlights the importance of developmental timing in understanding risk factors for cannabis use, suggesting that protective factors may have age-graded effects.
Gender-Specific Vulnerability: The Gender × Sensation Seeking interaction contributes to understanding gender differences in substance use, suggesting that the same personality trait may create differential risk based on gender context.

Practical Applications:

Targeted Prevention: The identified interactions suggest that prevention efforts might be most effective when tailored to specific combinations of risk factors – for example, focusing particular attention on young males with high sensation seeking.
Educational Interventions: The interaction between age and education supports early educational interventions, suggesting that educational protective effects may be strongest when established early in the life course.
Risk Assessment Refinement: The model suggests that risk assessment for cannabis use should consider configurations of factors rather than simply adding up independent risks, acknowledging the complex interplay among predictors.

Statistical Sophistication
The analysis in pois15 demonstrates several elements of statistical sophistication:

Hypothesis-Driven Modeling: Rather than indiscriminately testing all possible interactions, the analysis focuses on theoretically meaningful interactions that address specific questions about how risk factors operate across different groups.
Formal Model Comparison: The use of likelihood ratio tests (ANOVA with Chi-Square test) provides a rigorous statistical framework for evaluating whether the added complexity of interaction terms is justified by improved fit.
Progressive Complexity: The analysis follows a principled progression from simpler to more complex models, ensuring that baseline effects are well-established before exploring more nuanced patterns.

Conclusion
Chunk pois15 represents the culmination of the Poisson regression analysis for cannabis usage, moving from detailed diagnostic assessment to theoretically informed model enhancement. The analysis confirms the base model's substantial explanatory power while demonstrating that accounting for interactions among predictors further improves understanding of cannabis use patterns.
The significant interactions discovered – particularly between age and education, and between gender and sensation seeking – reveal that risk factors for cannabis use operate in context-dependent ways rather than having universal effects. These findings have important implications for both theoretical understanding of substance use and practical approaches to prevention and intervention.
Most importantly, the analysis demonstrates how statistical sophistication and substantive theory can reinforce each other in the study of complex behavioral phenomena. The model enhancements are simultaneously justified by statistical diagnostics (addressing non-linear patterns observed in residuals) and informed by theoretical questions about how demographic and personality factors interact to influence substance use behavior. This integration of methodological rigor and substantive insight represents the hallmark of high-quality behavioral science research.

## Generalised Linear Model with family set to Binomial (Nhat Bui)

```{r glmbi_model, echo=FALSE, cache=TRUE}
# Fit the model 

model <- glm(cnb_use ~ Nscore + Escore + Oscore + Ascore + Cscore, family = binomial, data = df_cnb)
summary(model)

cnb_model <- broom::tidy(model) %>%
  mutate(
    raw_p = p.value,
    p.value = round(p.value, 3),
    p.value = ifelse(p.value < 0.001, sprintf("%.2e", raw_p), p.value),
    OR = round(exp(estimate), 2),
    lower_CI = round(exp(estimate - 1.96 * std.error), 2),
    upper_CI = round(exp(estimate + 1.96 * std.error), 2),
        term     = recode(term,
               `(Intercept)` = "Intc.",
               Nscore        = "Neuroticism",
               Escore        = "Extraversion",
               Oscore        = "Openness",
               Ascore        = "Agreeableness",
               Cscore        = "Conscientiousness"
             )
  ) %>%
  select(term, estimate, OR, lower_CI, upper_CI, p.value)

kable(cnb_model, 
      col.names = c("Term", "Estimate", "OR", "Lower 95%", "Upper 95%", "p-value"),
      digits = c( NA, 3, 2, 2, 2, NA),
      caption = "Logistic Regression (Binomial GLM) Results") %>%
  kable_styling(
    bootstrap_options = c("striped", "hover", "condensed"),
    full_width        = FALSE,
    position          = "center",
    font_size         = 12
  ) %>%
  row_spec(0, bold = TRUE) %>%        
  column_spec(1, width = "4cm") %>%   
  column_spec(2:5, width = "2.5cm") %>%
  column_spec(6, width = "3cm")      
```

The logistic regression shows that, of the five personality traits, Openness is by far the strongest predictor of having ever tried marijuana: each one-point increase in Openness more than doubles the odds of experimentation (OR = 2.51, 95% CI 2.18–2.89, p < 0.001). Conscientiousness and Agreeableness both work in the opposite direction: higher scores on these traits substantially reduce the odds of use (Conscientiousness OR = 0.57, 95% CI 0.49–0.66, p < 0.001; Agreeableness OR = 0.74, 95% CI 0.65–0.85, p < 0.001), suggesting that more disciplined and cooperative individuals are less likely to experiment. Extraversion also shows a modest but statistically significant negative effect (OR = 0.83, 95% CI 0.71–0.96, p = 0.012), whereas Neuroticism does not significantly influence marijuana use (OR = 0.92, 95% CI 0.80–1.07, p = 0.28). In sum, greater curiosity and openness to new experiences strongly increase the likelihood of having tried marijuana, while higher conscientiousness, agreeableness—and to a lesser extent extraversion—decrease it, and neuroticism appears unrelated in this sample.

## Generalised Additive Model

```{r gam_eda_plot_edu, echo=FALSE, cache=TRUE}
df_cnb <- df_cnb %>%
  mutate(
    cnb_past_year = if_else(Cannabis >= 3, 1, 0)
  )

ggplot(df_cnb, aes(x = Education,
                   y = cnb_past_year,
                   group = 1)) +          
  geom_point(alpha = 0.5) +
  stat_smooth(
    method      = "gam",
    formula     = y ~ s(x, k = 4),     
    method.args = list(family = binomial),  
    se          = TRUE
  ) +
  labs(
    title = "Past-Year Cannabis Use vs. Education (GAM, k = 4)",
    x     = "Education",
    y     = "Past-Year Cannabis Use (0/1)"
  ) +
  theme_minimal(base_size = 14)
```

This GAM‐derived curve describes how the probability of past-year cannabis use (vertical axis) changes as education rises from level 1 (“Not Provided/left before 16”) through level 10 (“Doctorate”). A few key takeaways emerge:

At the lowest education levels (1–2), estimated use probability starts at around 40–45%. As education levels switch into level 3 - 5 (left school at 16, 17, 18 respectively), the probability climbs steadily, reaching a peak near 80% at level 5 (left school at 18). Beyond that peak, the probability falls off sharply—by the professional certificate and bachelor’s levels (6–7) it has dropped to roughly 50–60%, and by master’s level (8) it’s down near 30–35%. Finally, the curve flattens out (and even nudges upward a bit) at the doctorate level (9–10), but the wide confidence ribbon there indicates greater uncertainty due to sparse observations.

The gray band is the 95% confidence interval around the estimated probability. It is narrowest in the middle education bands (levels 3–7), where most of your data lie—so those estimates are quite precise. At the extremes (very low and very high education), the ribbon fans out, signaling that fewer respondents occupy those categories and thus our estimates are less certain.

Taken together, this non-linear relationship shows that cannabis use probability does not simply rise or fall with education. Instead, it increases sharply through those that left school at 16, 17, 18 reflecting experimentation during teenage-age—and then declines among individuals with higher degrees, suggesting that the highest educational attainments are associated with lower recent use.

```{r gam_eda_plot_age, echo=FALSE, cache=TRUE}
ggplot(df_cnb, aes(x = Age,
                   y = cnb_past_year,
                   group = 1)) +          
  geom_point(alpha = 0.5) +
  stat_smooth(
    method      = "gam",
    formula     = y ~ s(x, k = 3),     
    method.args = list(family = binomial),  
    se          = TRUE
  ) +
  labs(
    title = "Past-Year Cannabis Use vs. Age (GAM, k = 3)",
    x     = "Age",
    y     = "Past-Year Cannabis Use (0/1)"
  ) +
  theme_minimal(base_size = 14)
```

The GAM‐smoothed curve reveals a clear, non‐linear decline in the probability of past‐year cannabis use as people age. At the youngest age category (18–24), use is highest—around 80–85%. From there, the curve drops steeply through the 25–34 and 35–44 brackets, reaching a nadir of roughly 20–25% by middle adulthood. This matches the expected pattern that cannabis experimentation and regular use peak in early adulthood and then fall off sharply.

Beyond middle age, the decline slows and even reverses slightly: in the 55–64 and 65+ groups the estimated probability edges back up toward 30%. The widening gray confidence band in those older bins reflects smaller sample sizes and greater uncertainty, but the gentle uptick suggests that a non‐negligible minority of older adults continue to report recent use.

Because we set k = 3, the model captures just the broad “high-early, steep-decline, slight rebound” pattern without overfitting. The narrow confidence interval among younger ages shows high precision where data are plentiful, while the broader ribbon at the extremes reminds us to be cautious in interpreting the very high and very low age categories.

```{r gam_eda_plot_age_edu, echo=FALSE, cache=TRUE,fig.width=12, fig.height=10, dpi=125}
education_levels <- c(
  "Not Provided",
  "Left school before 16",
  "Left school at 16",
  "Left school at 17",
  "Left school at 18",
  "College/University student",
  "Professional certificate/diploma",
  "University degree",
  "Masters degree",
  "Doctorate degree"
)
df_cnb <- df_cnb %>%
  mutate(
    Education = factor(
      Education,
      levels = 1:10,
      labels = education_levels
    )
  )
ggplot(df_cnb, 
       aes(x = Age, 
           y = cnb_past_year)) +   
  geom_point(alpha = 0.3) +
  stat_smooth(
    method      = "gam",
    formula     = y ~ s(x, k = 3),
    method.args = list(family = binomial),
    se          = TRUE
  ) +
  facet_wrap(~ Education, ncol = 3) +
  labs(
    title = "Past-Year Cannabis Use vs. Age, by Education Level",
    x     = "Age",
    y     = "Probability of Past-Year Use"
  ) +
  theme_minimal(base_size = 17)
```

The “less‐educated” group (e.g. “Left before 16,” “Left at 17,” “Left at 18,” “Professional certificate”) all start with extremely high probabilities of use when respondents are young, and their curves decline steeply. By midlife, those groups still often have somewhat higher past‐year use than the more‐educated strata. Whereas, the highest‐education respondent group (“University degree,” “Masters,” “Doctorate”) start at a lower baseline in the youngest age bracket, decline more gradually, and by the oldest ages are clustered down near 10–25%. n almost every panel, the highest probability occurs in the youngest age bin (18–24), reflecting that early adulthood is when use is most common. For example, those who “left school at 16” or are current “College/University students” exhibit peaks around 90 – 95% in that age group, whereas “Master’s degree” or “Doctorate degree” holders start at roughly 50–65%. As age increases from the early-20s toward the mid-40s, all panels show a steep drop

The one outlier in shape is “College/University student.” That group has a very high probability at the youngest (freshman/first‐year) ages, dips in the middle (around 35-40), then rebounds at older ages. Almost every other “education” stratum shows a decline.

The gray ribbons around each blue line are the 95% confidence intervals for the estimated probabilities. Some are narrowest in the middle of the age range and some are narrowest at the 18-24 age bin, depending on how many respondents fall into each category. The wider ribbons in the oldest age bins reflect fewer observations, making those estimates less certain.

Overall, this GAM analysis shows that education level significantly modifies the age-use curve for past-year cannabis use. Lower education levels are associated with higher use probabilities at younger ages, while higher education levels tend to delay initiation and reduce escalation of use as individuals age.

```{r gam_fit_summary, echo=FALSE, cache=TRUE}
set.seed(123)
gam.1 <- gam(
  cnb_past_year ~ 
    Education + 
    s(Age, by = Education, k = 5),
  family = binomial(link = "logit"),
  data   = df_cnb,
)

summary(gam.1)
```

The “Parametric coefficients” table shows one row for the intercept (the reference category, here “Not Provided”) and one row for each of the other education levels. The intercept row can be seen as “the starting probability of past‐year use for the ‘Not Provided’ group”, and other row tells how much higher or lower that starting probability is for each education level compared to “Not Provided.”

(Intercept) = 0.3230 (p = 0.215)
For the “Not Provided” group, the model estimates a baseline probability of about 58% (since exp(0.3230)/(1 + exp(0.3230)) = 0.58005). p = 0.215 is not significant.

Left school before 16: +1.289 (p = 0.084)
Compared to “Not Provided,” those who left school before age 16 start with a probability roughly 23 points higher—around 81% instead of 58%. The p‐value of 0.084 is just above the usual threshold of 0.05, so this is a somewhat weak signal. There is some indication that early dropouts have a higher starting chance of past‐year use, but it isn’t quite strong enough to be certain.

Left school at 17: +0.526 (p = 0.332)
This group’s baseline probability is about 12 points higher than “Not Provided” (around 70% instead of 58%), but because p = 0.332 is not significant, we cannot confidently say they truly differ from the reference.

Left school at 18: +0.028 (p = 0.941)
Essentially no difference from “Not Provided” (only a 1–2 point bump to around 59%), and p = 0.941 confirms there is no evidence of a real shift.

College/University student: +0.704 (p = 0.0148)
Students start with about an 18‐point higher probability than “Not Provided” (around 76% vs. 58%), and p = 0.0148 is below 0.05. In other words, being a current student is significantly associated with a higher baseline chance of past‐year use.

Professional certificate/diploma: –0.0003 (p = 0.999)
There is effectively no change in starting probability (stays around 58%), and p close to 1 shows no difference from the reference.

University degree: –0.578 (p = 0.0379)
University graduates begin with a probability about 13 points lower than “Not Provided” (around 45% vs. 58%). Because p = 0.0379 is below 0.05, this lower baseline is statistically significant.

Masters degree: –1.069 (p = 0.00026)
Master’s holders start with about a 27‐point lower probability at baseline (roughly 31% instead of 58%). The p‐value is very small, so this is a highly significant finding: master’s graduates are much less likely to report past‐year use at the reference age.

Doctorate degree: –0.130 (p = 0.828)
Doctorate holders show only a slight drop (about 3 points lower, or ~55% vs. 58%), and p = 0.828 indicates no significant difference from “Not Provided.”

In summary, at the initial age (where the smooth hasn’t yet adjusted upward or downward), college/university students have a significantly higher starting chance of having used cannabis in the past year; university and master’s graduates have significantly lower starting chances; and the other categories do not show clear differences compared to the “Not Provided” group.

Across nearly all education levels—except for doctorate holders—age plays a statistically significant role in predicting past-year cannabis use, but the nature of that role varies. Some groups (“Not Provided,” “Left school before 16,” and “Master’s degree”) exhibit a simple, linear decline (edf close to 1, p < 0.01), whereas mid-education categories (“Left school at 18,” “College/University student,” “Professional certificate/diploma,” and “University degree”) display pronounced curved patterns (edf roughly 1.9–2.9, p < 0.001), peaking in early adulthood before falling. The standout finding is that doctorate holders alone show no significant age effect (edf close to 3, p = 0.3427), implying their probability of past-year use remains essentially flat across all age bins.

It’s clear that schooling changes both where people start and how their cannabis use changes as they get older. For example, among 18–24 year‐olds, college and university students stand out as the most likely to report past‐year use, while those with bachelor’s or master’s degrees are far less likely. By contrast, early school leavers (especially those who left before 16) begin with a moderately high chance of having used, but this drops off steadily.

As people move into their late 20s and beyond, almost every education group demosntrates a real decline in use—except doctorate holders, whose already‐low probability stays nearly flat across all age bins. But the way that drop happens isn’t the same for everyone: some groups (like master’s graduates or those without any schooling info) simply decline in a straight line, while others (like those who left school at 18, certificate holders, or current students) have a noticeable “hump” in their late teens or early 20s before their use tails off. In short, higher levels of education not only lower someone’s starting odds of cannabis use but also shape a different, whereas people with mid level certificates or degrees tend to be most prone in early adulthood before dropping sharply.

## Neural Network

## Support Vector Machine

# How we used Generative AI in our project

– how you used generative AI in redacting the group work (code-related questions, generate text, explain concepts…)\
– what was easy/hard/impossible to do with generative AI\
– what you had to pay attention to/be critical about when using the results obtained through the use of generative AI

# Conclusion

# Source

<https://archive.ics.uci.edu/dataset/373/drug+consumption+quantified>
