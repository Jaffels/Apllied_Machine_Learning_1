---
title: "Drug Consumption"
author: "Nhat Bui, Johan Ferreira, Thilo Holstein"
date: "2025-03-06"
output:
  pdf_document:
    toc: true
    toc_depth: 3
    number_sections: true
    fig_width: 7
    fig_height: 5
    fig_caption: true
    highlight: tango
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = TRUE,
  warning = FALSE,
  message = FALSE,
  cache = TRUE,
  fig.align = "center"
)
```

\newpage

# Introduction

Drug use is a significant risk behavior with serious health consequences for individuals and society. Multiple factors contribute to initial drug use, including psychological, social, individual, environmental, and economic elements, as well as personality traits. While legal substances like sugar, alcohol, and tobacco cause more premature deaths, illegal recreational drugs still create substantial social and personal problems.

In this data science project, we aim to identify factors and patterns potentially explaining drug use behaviors through machine learning techniques. By analyzing demographic, psychological, and social variables in our dataset, we'll aim to uncover potential predictors using machine learning methods to understand the complex relationships surrounding drug consumption.

The database contains records for 1,885 respondents with 12 attributes including personality measurements (NEO-FFI-R, BIS-11, ImpSS), demographics (education, age, gender, country, ethnicity), and self-reported usage of 18 drugs plus one fictitious drug (Semeron). Drug use is classified into seven categories ranging from "Never Used" to "Used in Last Day." All input attributes are quantified as real values, creating 18 distinct classification problems corresponding to each drug. A detailed description of the variables can be found in the Column Decsription text file.

# Personality Traits Explanation

To better understand the data set we need to have an understanding of what the personality traits are and what they represent, below we have short description of each trait and how to interpret them:

-   Nscore (Neuroticism): Measures emotional stability vs. instability. Higher scores indicate tendency toward negative emotions like anxiety, depression, vulnerability and mood swings. Lower scores suggest emotional stability and resilience to stress.
-   Escore (Extraversion): Measures sociability and outgoingness. Higher scores indicate preference for social interaction, assertiveness, and energy in social settings. Lower scores suggest preference for solitude, quieter environments and more reserved behavior.
-   Oscore (Openness to Experience): Measures intellectual curiosity and creativity. Higher scores indicate imagination, appreciation for art/beauty, openness to new ideas, and unconventional thinking. Lower scores suggest preference for routine, practicality, and conventional approaches.
-   Ascore (Agreeableness): Measures concern for social harmony. Higher scores indicate empathy, cooperation, and consideration for others. Lower scores suggest competitive, skeptical, or challenging interpersonal styles.
-   Cscore (Conscientiousness): Measures organization and reliability. Higher scores indicate discipline, responsibility, planning, and detail orientation. Lower scores suggest spontaneity, flexibility, and potentially less structured approaches.
-   Impulsive (Impulsiveness): Measures tendency to act without thinking. Higher scores indicate spontaneous decision-making without considering consequences. Lower scores suggest thoughtful deliberation before actions.
-   SS (Sensation Seeking): Measures desire for novel experiences and willingness to take risks. Higher scores indicate thrill-seeking behavior and preference for excitement. Lower scores suggest preference for familiarity and safety.

The first five traits (Nscore through Cscore) are the "Big Five" personality traits, which are widely used in psychological research. The Impulsive and SS measures are additional traits that are often studied in relation to risk-taking behaviors, which makes sense given our dataset includes variables related to substance use.

# Cleaning and Formatting the Dataset

```{r include=FALSE, cache=FALSE}
# Load required libraries
library(MASS)
library(ggplot2)
library(reshape2)
library(kableExtra)
library(knitr)
library(gridExtra)
library(tidyr)
library(car)
library(broom)
library(dplyr)
library(janitor)
library(GGally)

# Suppresses all warnings
options(warn = -1)
```

```{r echo=FALSE, cache=TRUE}
# Read the CSV file
drug_data <- read.csv("Data/drug_consumption.csv")
```

## Data Formatting

In its original state, the dataset represented most categorical variables with random floating-point numbers. We believe this was a measure to mitigate bias within the dataset. However, as our project's objectives differ from the dataset's initial purpose, we needed to revert these encoded values back to their original categorical representations. This step was essential to perform the analyses required for our project. This was the first step in cleaning our dataset.

```{r fom1, echo=FALSE, cache=TRUE}
###############################################################################
# Define mappings and column information
###############################################################################

# Column names for the dataset
column_names <- c(
  "Index", "ID", "Age", "Gender", "Education", "Country", "Ethnicity", 
  "Nscore", "Escore", "Oscore", "Ascore", "Cscore", "Impulsive", "SS", 
  "Alcohol", "Amphet", "Amyl", "Benzos", "Caff", "Cannabis", "Choc", "Coke", 
  "Crack", "Ecstasy", "Heroin", "Ketamine", "Legalh", "LSD", "Meth", 
  "Mushrooms", "Nicotine", "Semer", "VSA"
)

# Drug column names
drug_columns <- c(
  "Alcohol", "Amphet", "Amyl", "Benzos", "Caff", "Cannabis", 
  "Choc", "Coke", "Crack", "Ecstasy", "Heroin", "Ketamine", 
  "Legalh", "LSD", "Meth", "Mushrooms", "Nicotine", "Semer", "VSA"
)

# Mapping of drug consumption classes to their meanings
consumption_mapping <- c(
  "CL0" = "Never Used",
  "CL1" = "Used over a Decade Ago",
  "CL2" = "Used in Last Decade", 
  "CL3" = "Used in Last Year",
  "CL4" = "Used in Last Month",
  "CL5" = "Used in Last Week",
  "CL6" = "Used in Last Day"
)

# Map Age values to their meaning
age_mapping <- c(
  "-0.95197" = "18-24",
  "-0.07854" = "25-34",
  "0.49788" = "35-44",
  "1.09449" = "45-54",
  "1.82213" = "55-64",
  "2.59171" = "65+"
)

# Map Gender values to their meaning
gender_mapping <- c(
  "0.48246" = "Female",
  "-0.48246" = "Male"
)

# Map Education values to their meaning
education_mapping <- c(
  "-2.43591" = "Left school before 16 years",
  "-1.73790" = "Left school at 16 years",
  "-1.43719" = "Left school at 17 years",
  "-1.22751" = "Left school at 18 years",
  "-0.61113" = "Some college or university, no certificate or degree",
  "-0.05921" = "Professional certificate/diploma",
  "0.45468" = "University degree",
  "1.16365" = "Masters degree",
  "1.98437" = "Doctorate degree"
)

# Map Country values to their meaning
country_mapping <- c(
  "-0.09765" = "Australia",
  "0.24923" = "Canada",
  "-0.46841" = "New Zealand",
  "-0.28519" = "Other",
  "0.21128" = "Republic of Ireland",
  "0.96082" = "UK",
  "-0.57009" = "USA"
)

# Map Ethnicity values to their meaning
ethnicity_mapping <- c(
  "-0.50212" = "Asian",
  "-1.10702" = "Black",
  "1.90725" = "Mixed-Black/Asian",
  "0.12600" = "Mixed-White/Asian",
  "-0.22166" = "Mixed-White/Black",
  "0.11440" = "Other",
  "-0.31685" = "White"
)
```

```{r fom2, echo=FALSE, cache=TRUE}
###############################################################################
# Data Processing
###############################################################################

# Rename the columns 
colnames(drug_data) <- column_names

# Convert demographic columns to descriptive values
drug_data$Age <- age_mapping[as.character(drug_data$Age)]
drug_data$Gender <- gender_mapping[as.character(drug_data$Gender)]
drug_data$Education <- education_mapping[as.character(drug_data$Education)]
drug_data$Country <- country_mapping[as.character(drug_data$Country)]
drug_data$Ethnicity <- ethnicity_mapping[as.character(drug_data$Ethnicity)]

# Convert all drug consumption columns to descriptive values
for (col in drug_columns) {
  drug_data[[col]] <- consumption_mapping[as.character(drug_data[[col]])]
}
```

## Investigating Missing Values

```{r fom3, echo=FALSE, cache=TRUE}
###############################################################################
# Data Cleaning - Missing values
###############################################################################

# Remove unnecessary column
drug_data <- drug_data[, -which(names(drug_data) == "ID")]

# Check for NA values in each column
na_by_column <- sapply(drug_data, function(x) sum(is.na(x)))

# Create a data frame for better table formatting
na_df <- data.frame(
  Column = names(na_by_column)[na_by_column > 0],
  NA_Count = na_by_column[na_by_column > 0],
  Percentage = round(na_by_column[na_by_column > 0] / nrow(drug_data) * 100, 2)
)

# Create a nicely formatted table with kable
na_table <- na_df %>%
  kable(caption = "Missing Values by Column",
        booktabs = TRUE,
        col.names = c("Column", "Missing Values", "Percentage (%)"),
        digits = c(0, 0, 2),
        align = c('l', 'r', 'r')) %>%
  kable_styling(latex_options = c("striped", "hold_position"),
                full_width = FALSE,
                position = "center") %>%
  row_spec(0, bold = TRUE) %>%
  footnote(
    general = "Only columns with missing values are shown.",
    footnote_as_chunk = TRUE
  )

# Display the table
na_table
```

In the second step, we addressed missing values. We found that only two columns contained missing data, affecting approximately 5% of the 1885 observations. Considering the nature of these variables and the completeness of the remaining data, we inferred that participants likely withheld this information deliberately in most instances. Consequently, we replaced these missing values with the label "Not Provided," enabling us to treat these cases as a distinct category in our analysis.

```{r fom4, echo=FALSE, cache=TRUE}
# Replace NA values with "Not Provided"
drug_data$Education[is.na(drug_data$Education)] <- "Not Provided"
drug_data$Ethnicity[is.na(drug_data$Ethnicity)] <- "Not Provided"

# Save the updated dataframe back to CSV
write.csv(drug_data, "Data/cleaned.csv")
```

## Investigating Outliers

```{r fom5, echo=FALSE, cache=TRUE, fig.width=5, fig.height=4}
###############################################################################
# Data Cleaning - Looking for outliers
###############################################################################
# Define numeric columns for outlier analysis
numeric_cols <- c("Nscore", "Escore", "Oscore", "Ascore", "Cscore", "Impulsive", "SS")

# Function to identify outliers using IQR method
identify_outliers_iqr <- function(x) {
  q1 <- quantile(x, 0.25, na.rm = TRUE)
  q3 <- quantile(x, 0.75, na.rm = TRUE)
  iqr <- q3 - q1
  lower_bound <- q1 - 1.5 * iqr
  upper_bound <- q3 + 1.5 * iqr
  
  return(data.frame(
    min = min(x, na.rm = TRUE),
    q1 = q1,
    median = median(x, na.rm = TRUE),
    mean = mean(x, na.rm = TRUE),
    q3 = q3,
    max = max(x, na.rm = TRUE),
    iqr = iqr,
    lower_bound = lower_bound,
    upper_bound = upper_bound,
    n_outliers_below = sum(x < lower_bound, na.rm = TRUE),
    n_outliers_above = sum(x > upper_bound, na.rm = TRUE),
    total_outliers = sum(x < lower_bound | x > upper_bound, na.rm = TRUE),
    outlier_percentage = round(100 * sum(x < lower_bound | x > upper_bound, na.rm = TRUE) / length(x[!is.na(x)]), 2)
  ))
}

# Apply outlier detection to all numeric columns
outlier_summary <- data.frame()
for (col in numeric_cols) {
  result <- identify_outliers_iqr(drug_data[[col]])
  result$variable <- col
  outlier_summary <- rbind(outlier_summary, result)
}

# Create a function to visualize outliers with boxplots
plot_outliers <- function(drug_data, columns) {
  # Explicitly use reshape2::melt to avoid namespace issues
  melted_data <- reshape2::melt(drug_data[, columns], id.vars = NULL)
  
  # Create boxplot
  ggplot(melted_data, aes(x = variable, y = value)) +
    geom_boxplot(outlier.colour = "red", outlier.shape = 16, outlier.size = 2) +
    theme_minimal() +
    labs(title = "Boxplots with Outliers Highlighted",
         x = "Variable",
         y = "Value") +
    theme(axis.text.x = element_text(angle = 45, hjust = 1))
}

# Visualize outliers
plot_outliers(drug_data, numeric_cols)
```

The box plots generated for the seven psychometric personality scores reveal some data points that lie beyond the conventional 1.5xIQR (Interquartile Range) whiskers, technically identifying them as outliers. After invetigating the outliers we establised that outliers is not extreme in nature and fall within a plausible range, as well as being infrequent. Critically, their presence does not appear to significantly distort the overall distributional characteristics of these personality measures, which is important for subsequent analyses. The general cleanliness of the dataset, including the limited impact of these outliers, was better than anticipated, leading us to suspect that it may have undergone some form of pre-processing or curation before we accessed it.

# Exploratory Data Analysis

## Correlation between Behavioral Measures

```{r da1, echo=FALSE, cache=TRUE, fig.width=5, fig.height=4}
###############################################################################
# Heatmap
###############################################################################

# Calculate the correlation matrix
cor_matrix <- cor(drug_data[numeric_cols], use = "pairwise.complete.obs")

# Convert the correlation matrix to a data frame for ggplot
cor_df <- melt(cor_matrix)
names(cor_df) <- c("Var1", "Var2", "Correlation")

# Create a ggplot2 correlation heatmap
ggplot(data = cor_df, aes(x = Var1, y = Var2, fill = Correlation)) +
  geom_tile() +
  scale_fill_gradient2(low = "blue", high = "red", mid = "white", 
                       midpoint = 0, limit = c(-1,1), space = "Lab", 
                       name="Pearson\nCorrelation") +
  geom_text(aes(label = round(Correlation, 2)), color = "black", size = 3) +
  theme_minimal() +
  theme(
    axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1),
    plot.title = element_text(hjust = 0.5)
  ) +
  coord_fixed() +
  labs(
    title = "Correlation Matrix Behavioral Measures",
    x = "",
    y = ""
  )
```

The correlation matrix reveals that certain personality traits tend to cluster. For instance, Sensation Seeking (SS) shows a positive correlation with Extraversion (Escore), Openness (Oscore), and Impulsiveness. These three traits (Extraversion, Openness, and Impulsiveness) are also positively correlated with each other. Conversely, Sensation Seeking (along with Extraversion, Openness, and Impulsiveness) exhibits a negative correlation with Conscientiousness (Cscore) and Agreeableness (Ascore). Finally, Conscientiousness and Agreeableness demonstrate a positive correlation with each other.

## Comparing Behavioral Measure for Gender

```{r da2, echo=FALSE, cache=TRUE, fig.width=5, fig.height=4}
###############################################################################
# Gender comparison
###############################################################################

# Calculating the means of the behavioral scores
gender_results <- data.frame(
  Trait = character(),
  Female_Mean = numeric(),
  Male_Mean = numeric(),
  stringsAsFactors = FALSE
)

for (col in numeric_cols) {
  # Calculate means only
  female_mean <- mean(drug_data[drug_data$Gender == "Female", col], na.rm = TRUE)
  male_mean <- mean(drug_data[drug_data$Gender == "Male", col], na.rm = TRUE)
  
  # Add to results dataframe with only needed values
  gender_results <- rbind(gender_results, data.frame(
    Trait = col,
    Female_Mean = female_mean,
    Male_Mean = male_mean,
    stringsAsFactors = FALSE
  ))
}

# Create readable trait names
gender_results$Trait_Name <- case_when(
  gender_results$Trait == "Nscore" ~ "Neuroticism",
  gender_results$Trait == "Escore" ~ "Extraversion",
  gender_results$Trait == "Oscore" ~ "Openness",
  gender_results$Trait == "Ascore" ~ "Agreeableness",
  gender_results$Trait == "Cscore" ~ "Conscientiousness",
  gender_results$Trait == "Impulsive" ~ "Impulsivity",
  gender_results$Trait == "SS" ~ "Sensation Seeking",
  TRUE ~ gender_results$Trait
)

# Create the plot directly from the results
ggplot(gender_results, aes(x = Trait_Name)) +
  geom_bar(aes(y = Female_Mean, fill = "Female"), stat = "identity", position = "dodge", width = 0.7, alpha = 0.7) +
  geom_bar(aes(y = Male_Mean, fill = "Male"), stat = "identity", position = position_dodge(width = 0.7), width = 0.7, alpha = 0.7) +
  scale_fill_manual(values = c("Female" = "#FF9999", "Male" = "#6699CC"),
                    name = "Gender") +
  labs(title = "Gender Differences in Behavioral Measures",
       x = "",
       y = "Mean Score") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1, size = 10),
        plot.title = element_text(hjust = 0.5, size = 14),
        legend.position = "top") +
  # Fixed axis limits from -0.25 to 0.25
  scale_y_continuous(limits = c(-0.25, 0.25)) +
  coord_flip()
```

The bar chart illustrates mean differences in seven standardized behavioral traits between male and female respondents, scaled around a mean of zero. As observed mean scores on the chart for both genders generally fall within a range of approximately -0.25 to 0.25.

Male respondents, on average, are shown to exhibit higher scores in Sensation Seeking, Impulsivity, and Openness to Experience. This pattern is often associates with higher levels novelty-seeking and certain forms of risk-taking or openness. Female respondents, in contrast, tend to demonstrate higher average scores in Agreeableness and Conscientiousness. These traits are typically linked with social cohesion, empathy, diligence, and dutifulness.

## Comparing Education Level with Behavioral Measures

```{r da3, fig.width=5, fig.height=8.5, dev="pdf", echo=FALSE, cache=TRUE}
###############################################################################
# Education comparison
###############################################################################
# Order education levels
education_order <- c(
  "Left school before 16 years",
  "Left school at 16 years",
  "Left school at 17 years",
  "Left school at 18 years",
  "Some college or university, no certificate or degree",
  "Professional certificate/diploma",
  "University degree",
  "Masters degree",
  "Doctorate degree",
  "Not Provided"
)

# Calculate means by education level
education_means <- drug_data %>%
  group_by(Education) %>%
  summarize(
    n = n(),
    across(all_of(numeric_cols), ~mean(.x, na.rm = TRUE))
  ) %>%
  arrange(match(Education, education_order))

# Create a clean table view of the data
education_display <- education_means %>%
  dplyr::select(-n) %>% # Corrected line: Now uses dplyr::select
  rename(
    "Neuroticism" = Nscore,
    "Extraversion" = Escore,
    "Openness" = Oscore,
    "Agreeableness" = Ascore,
    "Conscientiousness" = Cscore,
    "Impulsivity" = Impulsive,
    "Sensation Seeking" = SS
  )

# Create a data frame for plotting
education_plot_data <- education_means %>%
  select(-n)

# Simplified visualization approach with more space for the title
# Set up the plotting parameters
par(mfrow = c(4, 2), mar = c(8, 4, 2, 1), oma = c(0, 0, 3, 0))

# Define the trait columns and their display names
trait_cols <- c("Nscore", "Escore", "Oscore", "Ascore", "Cscore", "Impulsive", "SS")
trait_names <- c("Neuroticism", "Extraversion", "Openness", "Agreeableness", 
                "Conscientiousness", "Impulsivity", "Sensation Seeking")

# Plot each trait separately
for (i in 1:length(trait_cols)) {
  col <- trait_cols[i]
  trait_name <- trait_names[i]
  
  # Create a simple barplot
  bp <- barplot(education_plot_data[[col]], 
          names.arg = rep("", nrow(education_plot_data)),
          col = ifelse(education_plot_data[[col]] > 0, "salmon", "lightblue"),
          main = trait_name,
          border = NA,
          las = 2,
          ylim = c(min(education_plot_data[[col]]) - 0.05, 
                  max(education_plot_data[[col]]) + 0.05),
          cex.main = 0.9,
          cex.axis = 0.8)
  
  # Add a reference line at y=0
  abline(h = 0, lty = 2, col = "gray")
  
  # Add abbreviated education labels
  edu_labels <- c("Before 16", "At 16", "At 17", "At 18", "Some college", 
                 "Prof cert", "University", "Masters", "Doctorate", "Not Provided")
  edu_labels <- edu_labels[1:length(bp)]  # Ensure we have the right number of labels
  
  # Add text labels at the bottom
  text(bp, par("usr")[3] - 0.02, srt = 45, adj = 1, labels = edu_labels, 
       xpd = TRUE, cex = 0.7)
}

# Add a title for the entire set of plots with more space
mtext("Personality Traits by Education Level", side = 3, line = 1, 
      outer = TRUE, cex = 1.2, font = 2)
```

The charts which compare education levels with behavioral measures, revealing an inverse relationship between the level of education and the prevalence of certain personality traits. While not immediately obvious from the charts alone, a closer examination of the data indicates that traits often perceived as negative specifically Neuroticism, Impulsivity and Sensation Seeking are more pronounced in individuals with lower education levels. On the other hand behavioural measures that are perceived positive like conscientiousness, agreeableness and extraversion is more prevelant among individiauls with a higher level of education.

## Analysis of Seremon Usage

```{r da4, echo=FALSE, cache=TRUE}
###############################################################################
# Seremon
###############################################################################

# Count Semeron users vs non-users
semeron_counts <- drug_data %>%
  group_by(Semer) %>%
  summarise(Count = n()) %>%
  arrange(desc(Count))
```

```{r da5, problem_chuck, echo=FALSE, cache=TRUE}
# Create a nicely formatted table for the detailed counts
semeron_table <- semeron_counts %>%
  mutate(Percentage = paste0(round(Count / sum(Count) * 100, 2), "%")) %>%
  rename(`Usage Category` = Semer) %>%
  kable(caption = "Semeron Usage Categories", 
        booktabs = TRUE, 
        col.names = c("Usage Category", "Count", "Percentage"))

# For PDF output, apply specific styling
if(knitr::is_latex_output()) {
  semeron_table <- semeron_table %>%
    kable_styling(latex_options = c("striped", "hold_position"), 
                  full_width = FALSE,
                  position = "center") %>%
    row_spec(0, bold = TRUE) %>%
    column_spec(1, width = "5cm") %>%
    column_spec(2, width = "2cm") %>%
    column_spec(3, width = "2.5cm")
}

# Display the table
semeron_table
```

The questionnaire included Semeron a fictitious drug. The fact that only a very small fraction of participants, 0.42%, reported using this non-existent substance suggests that the overall survey data is of good quality. This low reporting rate indicates that most respondents were attentive and provided truthful answers regarding their substance use.

# Prepraring the Dataset for Machine Learning

```{r prep1, echo=FALSE, cache=TRUE}
################################################################################
# Prep the data for modeling
################################################################################

# Make a copy of the dataset
model_data <- drug_data

# Remove the fake drug Semeron and index column
model_data <- model_data %>% 
  select(-c(Semer, Index))

# Map drug levels
consumption_levels <- c(
  "Never Used" = 0,
  "Used over a Decade Ago" = 1,
  "Used in Last Decade" = 2,
  "Used in Last Year" = 3,
  "Used in Last Month" = 4,
  "Used in Last Week" = 5,
  "Used in Last Day" = 6
)

# Iterate through each specified drug column
for (col_name in drug_columns) {
  if (col_name %in% names(model_data)) {
    column_values_as_char <- as.character(model_data[[col_name]])
    model_data[[col_name]] <- unname(consumption_levels[column_values_as_char])
  } 
}

# Convert gender to binary encoding
model_data$Gender <- ifelse(model_data$Gender == "Male", 1, 0)

# Change Age levels to ordinal
age_levels <- c("18-24", "25-34", "35-44", "45-54", "55-64", "65+")
model_data$Age <- as.integer(factor(model_data$Age, levels = age_levels))

# Change Education levels to ordinal
education_levels <- c(
  "Not Provided",
  "Left school before 16 years",
  "Left school at 16 years",
  "Left school at 17 years",
  "Left school at 18 years",
  "Some college or university, no certificate or degree",
  "Professional certificate/diploma",
  "University degree",
  "Masters degree",
  "Doctorate degree"
)
model_data$Education <- as.integer(factor(model_data$Education, levels = education_levels))

# Country - One-hot Encoding
country <- model.matrix(~ Country - 1, data = model_data)
model_data <- cbind(model_data, country)

# Ethnicity - One-hot Encoding
ethnicity <- model.matrix(~ Ethnicity - 1, data = model_data)
model_data <- cbind(model_data, ethnicity)

# Remove the original columns
model_data <- model_data %>% select(-c(Country, Ethnicity))

# Save the updated dataframe to csv
write.csv(model_data, "Data/model_data.csv")
```

Since the main focus of the project is implementing machine learning models we decided to prepare our data for this purpose. Just like we converted our original dataset to be more human readable for data exploration we have changed our dataset dataset to be more machine readable. The sex column was changed to binary data and for all the Drug columns, Education and Age we converted the data to ordinal data.

For the Ethnicity and Country columns we used a technique called One-Hot Encoding, where we transforms a categorical variable with multiple possible values into multiple binary (0 or 1) columns. Each new column represents one possible category from the original variable, and for each observation, exactly one of these new columns will have the value 1 (hence "one-hot") while all others will be 0.

It prevents the machine learning algorithm from assuming an arbitrary numerical relationship between categories. For example, if you simply encoded "USA"=1, "UK"=2, "Canada"=3, the algorithm might incorrectly assume that "Canada" is somehow "greater than" or "three times more important than" "USA".

# Machine Learning Models

## Linear Model

(Johan Ferreira)

Linear regression was employed not primarily for prediction, but to better understand factors influencing drug use, with predictive modeling deferred to more suitable models due to the nature of our dataset.

```{r lr1, echo=FALSE, cache=TRUE}

# Read the processed dataset
model_data <- read.csv("Data/model_data.csv")

# Remove the first index column if it exists
if(names(model_data)[1] == "X") {
  model_data <- model_data[,-1]
}

#---------------------------------------------------------------
# Linear Regression Modeling
#---------------------------------------------------------------

# Create clean names for drugs to analyze
drug_names <- c("Cannabis", "Alcohol", "Nicotine", "Coke", "Ecstasy")

# Function to build and evaluate linear regression model
run_drug_regression <- function(data, drug_name) {
  # Formula creation - all features, but handle multicollinearity in categorical variables
  
  # For country variables, exclude one as reference (USA)
  country_vars <- grep("Country", names(data), value = TRUE)
  country_vars <- country_vars[country_vars != "CountryUSA"] # Use USA as reference
  
  # For ethnicity variables, exclude one as reference (White)
  ethnicity_vars <- grep("Ethnicity", names(data), value = TRUE)
  ethnicity_vars <- ethnicity_vars[ethnicity_vars != "EthnicityWhite"] # Use White as reference
  
  # Create formula with modified variables to avoid perfect multicollinearity
  formula_str <- paste(drug_name, "~ Age + Gender + Education + Nscore + Escore + Oscore + Ascore + Cscore + Impulsive + SS + ", 
                     paste(c(country_vars, ethnicity_vars), collapse = " + "))
  
  formula <- as.formula(formula_str)
  
  # Fit model
  model <- lm(formula, data = data)
  
  # Check VIF for multicollinearity
  # First check if the model has aliased coefficients
  alias_check <- alias(model)
  has_aliased <- length(alias_check$Complete) > 0
  
  # Only run VIF if no aliased coefficients
  if(!has_aliased) {
    vif_values <- vif(model)
    high_vif <- vif_values[vif_values > 5]
  } else {
    # If there are aliased coefficients, we can't calculate VIF
    vif_values <- "Aliased coefficients detected"
    high_vif <- "Aliased coefficients detected"
    
    # Get the names of the aliased coefficients
    aliased_names <- rownames(alias_check$Complete)
    cat("Aliased coefficients detected in model for", drug_name, ":", paste(aliased_names, collapse=", "), "\n")
  }
  
  # Optional: Step-wise selection for feature selection
  # step_model <- step(model, direction = "both")
  
  # Return model and diagnostics
  return(list(
    model = model,
    summary = summary(model),
    vif = vif_values,
    high_vif = high_vif
  ))
}

# Create a results container
results_list <- list()

# Run regression for selected drugs
for (drug in drug_names) {
  # Check if the drug exists in the dataset
  if (drug %in% names(model_data)) {
    results_list[[drug]] <- run_drug_regression(model_data, drug)
  } else {
    cat("Warning: Drug", drug, "not found in dataset\n")
  }
}
```

```{r lr2, echo=FALSE, cache=TRUE}
#---------------------------------------------------------------
# Model Comparison and Visualization
#---------------------------------------------------------------

# Function to create a summary table of model performance
create_model_summary_table <- function(results_list) {
  # Initialize empty data frame
  summary_df <- data.frame(
    Drug = character(),
    R_squared = numeric(),
    Adj_R_squared = numeric(),
    F_statistic = numeric(),
    P_value = numeric(),
    Top_positive_predictor = character(),
    Top_negative_predictor = character(),
    stringsAsFactors = FALSE
  )
  
  # Fill with results
  for (drug in names(results_list)) {
    model_summary <- results_list[[drug]]$summary
    
    # Get coefficients
    coefs <- model_summary$coefficients
    
    # Find top predictors (excluding intercept)
    coef_df <- data.frame(
      Variable = rownames(coefs)[-1],
      Estimate = coefs[-1, "Estimate"],
      P_value = coefs[-1, "Pr(>|t|)"]
    )
    
    # Get significant predictors only
    sig_coefs <- coef_df[coef_df$P_value < 0.05, ]
    
    if(nrow(sig_coefs) > 0) {
      # Get top positive and negative predictors
      top_pos <- sig_coefs[which.max(sig_coefs$Estimate), "Variable"]
      top_neg <- sig_coefs[which.min(sig_coefs$Estimate), "Variable"]
    } else {
      top_pos <- "None"
      top_neg <- "None"
    }
    
    # Add to summary
    summary_df <- rbind(summary_df, data.frame(
      Drug = drug,
      R_squared = model_summary$r.squared,
      Adj_R_squared = model_summary$adj.r.squared,
      F_statistic = model_summary$fstatistic[1],
      P_value = pf(model_summary$fstatistic[1], 
                 model_summary$fstatistic[2], 
                 model_summary$fstatistic[3], 
                 lower.tail = FALSE),
      Top_positive_predictor = top_pos,
      Top_negative_predictor = top_neg,
      stringsAsFactors = FALSE
    ))
  }
  
  return(summary_df)
}

# Create and format the summary table
model_summary_table <- create_model_summary_table(results_list)
```

```{r lr3, echo=FALSE, cache=TRUE}
#---------------------------------------------------------------
# Linear Regression Plots
#---------------------------------------------------------------

# Function to create a visually appealing coefficient plot
plot_factor_importance_enhanced <- function(model, title, color_scheme = "viridis") {
  # Extract model coefficients
  coefs <- summary(model)$coefficients
  
  # Filter out intercept and create data frame for plotting
  coef_df <- data.frame(
    Variable = rownames(coefs)[-1],  # Exclude intercept
    Estimate = coefs[-1, "Estimate"],
    StdError = coefs[-1, "Std. Error"],
    PValue = coefs[-1, "Pr(>|t|)"]
  )
  
  # Add significance markers and categories
  coef_df$Significance <- ifelse(coef_df$PValue < 0.001, "p < 0.001", 
                               ifelse(coef_df$PValue < 0.01, "p < 0.01",
                                    ifelse(coef_df$PValue < 0.05, "p < 0.05", "Not significant")))
  
  # Sort by absolute value of estimate
  coef_df <- coef_df[order(abs(coef_df$Estimate), decreasing = TRUE), ]
  
  # Keep only top 15 predictors for visualization clarity
  if(nrow(coef_df) > 15) {
    coef_df <- coef_df[1:15, ]
  }
  
  # Clean up variable names for display
  var_display_mapping <- c(
    "Age" = "Age",
    "Gender" = "Gender (Male=1)",
    "Education" = "Education Level",
    "Nscore" = "Neuroticism",
    "Escore" = "Extraversion",
    "Oscore" = "Openness",
    "Ascore" = "Agreeableness",
    "Cscore" = "Conscientiousness",
    "Impulsive" = "Impulsivity",
    "SS" = "Sensation Seeking"
  )
  
  # Function to clean up variable names
  clean_var_names <- function(var_name) {
    # First check exact matches in our mapping
    if (var_name %in% names(var_display_mapping)) {
      return(var_display_mapping[var_name])
    }
    
    # Handle country variables
    if (grepl("^Country", var_name)) {
      return(gsub("Country", "Country: ", var_name))
    }
    
    # Handle ethnicity variables
    if (grepl("^Ethnicity", var_name)) {
      return(gsub("Ethnicity", "Ethnicity: ", var_name))
    }
    
    return(var_name)
  }
  
  # Apply name cleaning
  coef_df$DisplayName <- sapply(coef_df$Variable, clean_var_names)
  
  # Reorder factor levels for plotting
  coef_df$DisplayName <- factor(coef_df$DisplayName, levels = rev(coef_df$DisplayName))
  
  # Define significance color palette
  if (color_scheme == "viridis") {
    sig_colors <- c("p < 0.001" = "#440154", "p < 0.01" = "#21908C", 
                    "p < 0.05" = "#5DC863", "Not significant" = "#CCCCCC")
  } else {
    sig_colors <- c("p < 0.001" = "#0072B2", "p < 0.01" = "#009E73", 
                    "p < 0.05" = "#56B4E9", "Not significant" = "#CCCCCC")
  }
  
  # Plot with enhanced aesthetics
  ggplot(coef_df, aes(x = Estimate, y = DisplayName, color = Significance)) +
    geom_point(aes(size = abs(Estimate)), alpha = 0.8) +
    geom_errorbarh(aes(xmin = Estimate - 1.96 * StdError, 
                       xmax = Estimate + 1.96 * StdError), 
                   height = 0.2, alpha = 0.7) +
    geom_vline(xintercept = 0, linetype = "dashed", color = "darkgray", linewidth = 0.7) +
    scale_color_manual(values = sig_colors) +
    scale_size_continuous(range = c(2, 6), guide = "none") +
    labs(title = title,
         x = "Effect Size (Coefficient Estimate)",
         y = "",
         color = "Statistical\nSignificance") +
    theme_minimal() +
    theme(
      plot.title = element_text(face = "bold", size = 16, hjust = 0.5),
      axis.title.x = element_text(size = 12, face = "bold"),
      axis.text.y = element_text(size = 10),
      legend.position = "top",
      legend.title = element_text(face = "bold"),
      panel.grid.minor = element_blank(),
      panel.grid.major.y = element_blank()
    )
}

```

### Personality Traits as Predictors of Substance Use

```{r lr5, echo=FALSE, cache=TRUE, fig.width=5, fig.height=4}
#---------------------------------------------------------------
# Generate Output
#---------------------------------------------------------------

# Create a publication-quality regression table using kable instead of stargazer
# for more reliable operation

# Function to create clean model coefficient table
create_model_coef_table <- function(model_list, drug_names) {
  # Extract key coefficients from each model
  key_vars <- c("Age", "Gender", "Education", "Nscore", "Escore",
               "Oscore", "Ascore", "Cscore", "Impulsive", "SS")

  # Create data frame for results
  result_df <- data.frame(
    Variable = c("(Intercept)", key_vars),
    stringsAsFactors = FALSE
  )

  # Add each model's coefficients and significance
  for (drug in drug_names) {
    if (drug %in% names(model_list)) {
      model <- model_list[[drug]]$model
      coefs <- summary(model)$coefficients

      # Extract coefficients and p-values
      drug_coefs <- numeric(length(result_df$Variable))
      drug_p <- numeric(length(result_df$Variable)) # p-values still extracted but not used for stars

      for (i in 1:length(result_df$Variable)) {
        var_name <- result_df$Variable[i]
        if (var_name %in% rownames(coefs)) {
          drug_coefs[i] <- coefs[var_name, "Estimate"]
          drug_p[i] <- coefs[var_name, "Pr(>|t|)"]
        } else {
          drug_coefs[i] <- NA
          drug_p[i] <- NA
        }
      }

      # Format coefficients (without significance stars)
      drug_coef_text <- ifelse(!is.na(drug_coefs),
                             sprintf("%.3f", round(drug_coefs, 3)), # drug_sig (stars) removed from here
                             "")

      # Add to result dataframe
      result_df[[drug]] <- drug_coef_text
    }
  }

  # Add model metrics
  metrics_rows <- data.frame(
    Variable = c("N", "R²", "Adjusted R²", "F-statistic"),
    stringsAsFactors = FALSE
  )

  for (drug in drug_names) {
    if (drug %in% names(model_list)) {
      model_summary <- summary(model_list[[drug]]$model)
      n <- length(model_summary$residuals)
      r2 <- model_summary$r.squared
      adj_r2 <- model_summary$adj.r.squared
      f_stat <- model_summary$fstatistic[1]

      metrics_rows[[drug]] <- c(
        as.character(n),
        sprintf("%.3f", round(r2, 3)),
        sprintf("%.3f", round(adj_r2, 3)),
        sprintf("%.3f", round(f_stat, 3))
      )
    }
  }

  # Combine results and metrics
  final_df <- rbind(result_df, metrics_rows)

  # Clean variable names for display
  var_display_names <- c(
    "(Intercept)" = "Intercept",
    "Age" = "Age",
    "Gender" = "Gender (Male=1)",
    "Education" = "Education Level",
    "Nscore" = "Neuroticism",
    "Escore" = "Extraversion",
    "Oscore" = "Openness",
    "Ascore" = "Agreeableness",
    "Cscore" = "Conscientiousness",
    "Impulsive" = "Impulsivity",
    "SS" = "Sensation Seeking",
    "N" = "N",
    "R²" = "R²",
    "Adjusted R²" = "Adjusted R²",
    "F-statistic" = "F-statistic"
  )

  final_df$Variable <- var_display_names[final_df$Variable]

  return(final_df)
}


# Create the comparison table for the main drugs
drug_names_for_table <- names(results_list)
if (length(drug_names_for_table) > 0) {
  model_comparison_table <- create_model_coef_table(
    results_list,
    drug_names_for_table
  )

  # Display the table with kable for better formatting
  kable(model_comparison_table,
      caption = "Linear Regression Models for Drug Usage",
      align = c('l', rep('r', ncol(model_comparison_table) - 1)),
      longtable = TRUE) %>%
    kable_styling(
      latex_options = c("striped", "hold_position"), 
      full_width = FALSE,
      position = "center"                             
    ) %>%
    row_spec(0, bold = TRUE) %>%                      
    column_spec(1, bold = TRUE) %>%                   
    add_header_above(c(" " = 1, "Drug Models" = ncol(model_comparison_table) - 1))
} else {
  cat("No valid drug models to display in table\n")
}
```

Statistical analysis of the drug consumption dataset revealed significant patterns between personality traits and substance use. Linear regression models for substances like Cannabis, Alcohol, and Nicotine showed that Cannabis had the most robust predictive model (highest adjusted R²). Sensation Seeking (SS) and Impulsivity consistently showed strong positive correlations with multi-drug use, while Conscientiousness and Agreeableness had significant negative relationships. Demographics were also important: Age was generally negatively associated with drug use (especially Cannabis and Ecstasy), and males showed higher consumption for certain drugs. Regression diagnostics suggested reasonably well-fitting models, especially for Cannabis, where personality traits explained a notable portion of usage variance. These results align with suggestions that certain personality profiles, particularly high Sensation Seeking, predispose individuals to substance use.

### Analysis of Personality Traits as Predictors of Substance Use

```{r lr6, echo=FALSE, cache=TRUE, fig.width=7, fig.height=3.5}

# Properly define the individual models for plotting
if ("Cannabis" %in% names(results_list)) {
  cannabis_model <- results_list[["Cannabis"]]$model
  cannabis_coef_plot <- plot_factor_importance_enhanced(cannabis_model, "Predictors of Cannabis Usage")
  print(cannabis_coef_plot)
}
```

**Cannabis Usage Predictors**

The first plot presents the predictors of cannabis usage, showing estimated coefficients with 95% confidence intervals. Several key observations emerge:

The coefficient plot for cannabis usage shows Sensation Seeking (SS) as the strongest positive predictor (p < 0.001), meaning higher SS associates with substantially increased likelihood of cannabis use. Age has a strong negative association (p < 0.001), with use decreasing significantly as age increases. Openness (Oscore) is another significant positive predictor (p < 0.001), linking intellectual curiosity to higher cannabis use. Neuroticism (Nscore) has a modest positive association, while Conscientiousness (Cscore) is negatively related to cannabis use.


```{r lr7, echo=FALSE, cache=TRUE, fig.width=7, fig.height=3.5}
if ("Alcohol" %in% names(results_list)) {
  alcohol_model <- results_list[["Alcohol"]]$model
  alcohol_coef_plot <- plot_factor_importance_enhanced(alcohol_model, "Predictors of Alcohol Usage")
  print(alcohol_coef_plot)
}
```

**Alcohol Usage Predictors**

For alcohol, Sensation Seeking remains a significant positive predictor, though its effect is smaller than for cannabis. Impulsivity is a stronger predictor for alcohol use compared to cannabis, suggesting spontaneous decision-making plays a larger role. Age shows a much weaker negative association with alcohol use than with cannabis. Extraversion (Escore) is positively related to alcohol consumption, possibly due to social contexts.

```{r lr8, echo=FALSE, cache=TRUE, fig.width=7, fig.height=3.5}
if ("Nicotine" %in% names(results_list)) {
  nicotine_model <- results_list[["Nicotine"]]$model
  nicotine_coef_plot <- plot_factor_importance_enhanced(nicotine_model, "Predictors of Nicotine Usage")
  print(nicotine_coef_plot)
}
```

**Nicotine Usage Predictors**

Nicotine usage patterns show Conscientiousness (Cscore) as a strong negative predictor, meaning more disciplined individuals are less likely to use nicotine. Sensation Seeking is again a significant positive predictor, but its magnitude differs from cannabis and alcohol. Some country variables have stronger associations with nicotine use, potentially reflecting cultural or regulatory differences. Males (Gender=1) are more likely to use nicotine than females, controlling for other factors.

**Cross-Substance Comparison**

Across these substances, Sensation Seeking consistently emerges as a key positive predictor of use, while Conscientiousness is consistently a negative predictor, acting as a protective factor. Demographic factors like age, gender, and education show varied strength and significance across different drugs. Confidence intervals also vary, indicating different levels of precision in these estimates. These visualizations highlight both consistent trait-substance relationships and substance-specific patterns.

### Cannabis Usage Linear Regression Model: Diagnostic Analysis

```{r lr9, echo=FALSE, cache=TRUE, fig.width=6, fig.height=4}
#---------------------------------------------------------------
# Diagnostic Plots
#---------------------------------------------------------------

# Function to create diagnostic plots with enhanced aesthetics
create_diagnostic_plots <- function(model, title, color_scheme = "blue") {
  # Extract residuals data
  model_data <- augment(model)
  
  # Define color palette
  if (color_scheme == "blue") {
    point_color <- "#3182bd"
    line_color <- "#08519c"
    reference_color <- "#e41a1c"
  } else if (color_scheme == "green") {
    point_color <- "#31a354"
    line_color <- "#006d2c"
    reference_color <- "#d62728"
  } else {
    point_color <- "#756bb1"
    line_color <- "#54278f"
    reference_color <- "#e41a1c"
  }
  
  # Common theme elements
  diagnostic_theme <- theme_minimal() +
    theme(
      plot.title = element_text(face = "bold", size = 12),
      plot.subtitle = element_text(size = 9, color = "gray50"),
      axis.title = element_text(size = 10),
      axis.text = element_text(size = 9),
      panel.grid.minor = element_blank(),
      panel.border = element_rect(color = "gray80", fill = NA, linewidth = 0.5)
    )
  
  # Residuals vs Fitted with improved aesthetics
  p1 <- ggplot(model_data, aes(x = .fitted, y = .resid)) +
    geom_point(alpha = 0.6, color = point_color, size = 1.5) +
    geom_hline(yintercept = 0, linetype = "dashed", color = reference_color, 
               linewidth = 0.7) +
    geom_smooth(se = TRUE, color = line_color, fill = alpha(line_color, 0.2), 
                method = "loess", message = FALSE, warning = FALSE) +
    labs(title = "Residuals vs Fitted",
         subtitle = "Should show random scatter around the zero line",
         x = "Fitted values",
         y = "Residuals") +
    diagnostic_theme
  
  # Normal Q-Q plot with improved aesthetics
  p2 <- ggplot(model_data, aes(sample = .resid)) +
    stat_qq(color = point_color, size = 1.5, alpha = 0.6) +
    stat_qq_line(color = reference_color, linewidth = 0.7) +
    labs(title = "Normal Q-Q Plot",
         subtitle = "Points should follow the diagonal line",
         x = "Theoretical Quantiles",
         y = "Sample Quantiles") +
    diagnostic_theme
  
  # Scale-Location plot with improved aesthetics
  p3 <- ggplot(model_data, aes(x = .fitted, y = sqrt(abs(.resid)))) +
    geom_point(alpha = 0.6, color = point_color, size = 1.5) +
    geom_smooth(se = TRUE, color = line_color, fill = alpha(line_color, 0.2), 
                method = "loess", message = FALSE, warning = FALSE) +
    labs(title = "Scale-Location",
         subtitle = "Should show homogeneous variance",
         x = "Fitted values",
         y = "sqrt|Standardized Residuals|") +
    diagnostic_theme
  
  # Residuals vs Leverage with improved aesthetics
  p4 <- ggplot(model_data, aes(x = .hat, y = .resid)) +
    geom_point(alpha = 0.6, color = point_color, size = 1.5) +
    geom_smooth(se = TRUE, color = line_color, fill = alpha(line_color, 0.2), 
                method = "loess", message = FALSE, warning = FALSE) +
    geom_hline(yintercept = 0, linetype = "dashed", color = "gray50") +
    stat_contour(aes(z = .cooksd), breaks = c(0.5, 1), color = "red", 
                 linetype = "dashed", na.rm = TRUE) +
    labs(title = "Residuals vs Leverage",
         subtitle = "Identifies influential cases",
         x = "Leverage",
         y = "Standardized Residuals") +
    diagnostic_theme
  
  # Combine plots with better layout
  title_grob <- grid::textGrob(
    title, 
    gp = grid::gpar(fontsize = 16, fontface = "bold"),
    just = "center"
  )
  subtitle_grob <- grid::textGrob(
    "Diagnostic Plots for Linear Regression Model", 
    gp = grid::gpar(fontsize = 12, col = "gray30"),
    just = "center"
  )
  
  # Arrange the title, subtitle, and plots
  combined_plot <- gridExtra::grid.arrange(
    gridExtra::arrangeGrob(title_grob, subtitle_grob, heights = c(1, 0.5), ncol = 1),
    gridExtra::arrangeGrob(p1, p2, p3, p4, ncol = 2),
    heights = c(1, 10)
  )
  
  return(combined_plot)
}

# If we have diagnostic plots for Cannabis model
if (exists("cannabis_model") && !is.null(cannabis_model)) {
  cannabis_diagnostics <- create_diagnostic_plots(cannabis_model, "Cannabis Usage Model Diagnostics")
}
```

**Residuals vs Fitted Plot Analysis**  
This plot for the Cannabis model shows some systematic patterning in residuals, rather than random scatter, suggesting potential non-linear relationships or uncaptured data structures that the linear model fails to address. This might indicate a need for transformations or interaction terms.

**Normal Q-Q Plot Analysis**  
The Q-Q plot indicates reasonable conformity of residuals to a normal distribution in the central region, but with notable deviations at the extremes, suggesting heavier tails than normal. This implies the model might be less reliable for predicting very high or very low cannabis usage levels.

**Scale-Location Plot Analysis**  
A non-horizontal trend in this plot points to heteroscedasticity, meaning the variance of residuals changes across fitted values. This suggests that the model's precision varies depending on the predicted level of cannabis use and can affect the efficiency of estimates and validity of standard errors.

**Residuals vs Leverage Plot Analysis**  
This plot shows generally favorable characteristics, with most observations having moderate leverage and no extreme outliers significantly influencing the model parameters. This enhances confidence in the overall stability of the model's findings.

**Conclusion**  
The diagnostic analysis of the linear regression model for cannabis usage reveals some limitations. Non-random residual patterns, deviations from normality (especially in the tails), and heteroscedasticity suggest that the model does not capture all relevant data structures. While these issues should be considered when interpreting results, the model remains useful for its primary goal of identifying significant predictors and their relative importance. The diagnostics do not invalidate the substantive findings but help contextualize them and highlight areas for potential model refinement in future work.

## Generalised Linear Model with family set to Poisson

(Johan Ferreira)

```{R pois1, echo=FALSE, cache=TRUE, fig.width=6, fig.height=4}
# Read the data
model_data <- read.csv("Data/model_data.csv")

# Remove the first index column if it exists
if(names(model_data)[1] == "X") {
  model_data <- model_data[,-1]
}
```

```{R pois2, echo=FALSE, cache=TRUE, fig.width=6, fig.height=4}
# Define the predictors to use in models
predictors <- c("Age", "Gender", "Education", 
                "Nscore", "Escore", "Oscore", "Ascore", "Cscore", 
                "Impulsive", "SS")

# Function to fit Poisson GLM for a specific drug
fit_poisson_glm <- function(data, drug, predictors) {
  # Create formula
  formula_str <- paste(drug, "~", paste(predictors, collapse = " + "))
  formula <- as.formula(formula_str)

  # Fit Poisson GLM
  model <- glm(formula, data = data, family = poisson(link = "log"))

  # Return model
  return(model)
}
```

```{R pois3, echo=FALSE, cache=TRUE, fig.width=6, fig.height=4}
# Drugs to model
drugs_for_poisson_table <- c("Cannabis", "Alcohol", "Nicotine", "Coke", "Ecstasy")

# Fit models for each drug
models <- list()
for (drug in drugs_for_poisson_table) {
  if (drug %in% names(model_data)) {
    # Ensure the response variable is numeric and non-negative for Poisson
    if(is.numeric(model_data[[drug]]) && all(model_data[[drug]] >= 0, na.rm = TRUE)) {
      models[[drug]] <- fit_poisson_glm(model_data, drug, predictors)
    } else {
      cat("Warning: Drug column", drug, "is not suitable for Poisson regression (must be numeric and non-negative). Skipping.\n")
    }
  } else {
    cat("Warning: Drug column", drug, "not found in dataset. Skipping.\n")
  }
}
# Filter out any NULL models from the list if a drug was skipped
models <- Filter(Negate(is.null), models)
# Update drugs_for_poisson_table to only include successfully modeled drugs
drugs_for_poisson_table <- names(models)
```

```{R pois4, echo=FALSE, cache=TRUE, fig.width=6, fig.height=4}
# Function to create a comparison table for Poisson models (similar to lr5's table)
create_poisson_comparison_table <- function(model_list, drug_names) {
  # Key variables to include (same as in predictors for consistency)
  key_vars <- c("Age", "Gender", "Education", "Nscore", "Escore",
               "Oscore", "Ascore", "Cscore", "Impulsive", "SS")

  # Create data frame for results
  result_df <- data.frame(
    Variable = c("(Intercept)", key_vars),
    stringsAsFactors = FALSE
  )

  # Add each model's coefficients
  for (drug in drug_names) {
    if (drug %in% names(model_list)) {
      model <- model_list[[drug]]
      coefs_summary <- summary(model)$coefficients

      drug_coef_text <- sapply(result_df$Variable, function(var_name) {
        if (var_name %in% rownames(coefs_summary)) {
          estimate <- coefs_summary[var_name, "Estimate"]
          return(sprintf("%.3f", round(estimate, 3)))
        } else {
          return("") 
        }
      })
      result_df[[drug]] <- drug_coef_text
    } else {
      result_df[[drug]] <- "" 
    }
  }

  # Add model metrics
  # Using LaTeX command for Chi and generic R-squared labels
  metrics_rows <- data.frame(
    Variable = c("N", "Pseudo R²", "Adjusted Pseudo R²", "Model $\\chi^2$"),
    stringsAsFactors = FALSE
  )

  for (drug in drug_names) {
    if (drug %in% names(model_list)) {
      model <- model_list[[drug]]
      n <- nobs(model)
      
      logLik_model <- as.numeric(logLik(model))
      null_model <- glm(as.formula(paste(drug, "~ 1")), 
                        data = model$model, 
                        family = poisson(link = "log"))
      logLik_null <- as.numeric(logLik(null_model))
      pseudo_r2 <- 1 - (logLik_model / logLik_null)
      
      k_predictors <- length(coef(model)) - 1 
      adj_pseudo_r2 <- 1 - ( (logLik_model - k_predictors) / logLik_null )
      
      model_chi_sq <- model$null.deviance - model$deviance

      metrics_rows[[drug]] <- c(
        as.character(n),
        sprintf("%.3f", round(pseudo_r2, 3)),
        sprintf("%.3f", round(adj_pseudo_r2, 3)),
        sprintf("%.2f", round(model_chi_sq, 2)) 
      )
    } else {
      metrics_rows[[drug]] <- rep("", 4)
    }
  }

  final_df <- rbind(result_df, metrics_rows)

  # Clean variable names for display (similar to lr5)
  var_display_names <- c(
    "(Intercept)" = "Intercept",
    "Age" = "Age",
    "Gender" = "Gender (Male=1)",
    "Education" = "Education Level",
    "Nscore" = "Neuroticism",
    "Escore" = "Extraversion",
    "Oscore" = "Openness",
    "Ascore" = "Agreeableness",
    "Cscore" = "Conscientiousness",
    "Impulsive" = "Impulsivity",
    "SS" = "Sensation Seeking",
    "N" = "N",
    "Pseudo R²" = "Pseudo R²", 
    "Adjusted Pseudo R²" = "Adjusted Pseudo R²", 
    "Model $\\chi^2$" = "Model $\\chi^2$" # LaTeX for Chi
  )
  
  # Apply display names
  final_df$Variable <- sapply(final_df$Variable, function(x) {
    if (x %in% names(var_display_names)) return(var_display_names[x])
    return(x)
  })

  # Reorder to match a specific display order
  desired_order <- c(
    "Intercept", "Age", "Gender (Male=1)", "Education Level",
    "Neuroticism", "Extraversion", "Openness", "Agreeableness",
    "Conscientiousness", "Impulsivity", "Sensation Seeking",
    "N", "Pseudo R²", "Adjusted Pseudo R²", "Model $\\chi^2$"
  )
  final_df$Variable <- factor(final_df$Variable, levels = desired_order)
  final_df <- final_df[order(final_df$Variable), ]

  return(final_df)
}
```

### Analysis of Cannabis Usage Poisson Model  

```{r pois5, echo=FALSE, cache=TRUE, fig.width=6, fig.height=4, results='asis'}
# Create and display the Poisson model comparison table

if (length(models) > 0 && length(drugs_for_poisson_table) > 0) {
  poisson_comparison_table_data <- create_poisson_comparison_table(
    models,
    drugs_for_poisson_table 
  )
  
  poisson_kable_table <- kable(poisson_comparison_table_data,
        format = "latex", 
        caption = "Poisson Regression Models for Drug Usage",
        align = c('l', rep('r', ncol(poisson_comparison_table_data) - 1)),
        booktabs = TRUE,
        longtable = TRUE,
        escape = FALSE) %>% 
    kable_styling(
      latex_options = c("striped", "hold_position"), 
      full_width = FALSE,
      position = "center"                             
    ) %>%
    row_spec(0, bold = TRUE) %>%                      
    column_spec(1, bold = TRUE) %>%                   
    add_header_above(c(" " = 1, "Drug Models" = ncol(poisson_comparison_table_data) - 1))
    # Footnote has been removed
  
  print(poisson_kable_table)

} else {
  cat("No Poisson models were successfully generated to display in the table.\n")
}
```

Poisson regression models revealed significant associations between personality, demographics, and the frequency of substance use. The Cannabis model likely showed the strongest explanatory power (highest Pseudo R²), with models for Coke and Ecstasy also indicating personality as key to use frequency. Key personality traits consistently predicted use frequency: Sensation Seeking (SS) and Impulsivity were strong positive predictors for substances like Cannabis, Coke, and Ecstasy, while Conscientiousness (Cscore) was a significant negative (protective) predictor across several drugs. Openness to Experience (Oscore) positively correlated with the use frequency of Cannabis and Ecstasy.

Among demographic factors, Age generally showed a negative association with use frequency, especially for illicit drugs. Being Male was often linked to higher use frequency. Model fit statistics (Pseudo R² and significant Model chi 2) confirmed that the predictors collectively explained usage frequency significantly better than chance. Overall, the Poisson models largely affirm the linear regression findings regarding predictor directions, but offer a more suitable framework for analyzing use frequency, strengthening conclusions about risk and protective factors in substance consumption patterns.

### Predictor Effects Visualization  

Visualizations of predictor effects from the Poisson model illustrate the non-linear relationships. The age effect plot would show a steep negative gradient, with predicted cannabis usage highest in the youngest age group (18-24) and declining sharply with each subsequent category. The sensation seeking (SS) plot would reveal a clear positive exponential relationship, where predicted cannabis usage accelerates at higher SS scores. This suggests that individuals at the highest end of the sensation-seeking spectrum are disproportionately more likely to use cannabis frequently. These visualizations, combined with the overdispersion findings, confirm the strong impact of these predictors while also supporting the consideration of model refinements like the negative binomial approach for a more nuanced capture of data complexity.

### Analysis of Poisson Models with Interaction Terms for Cannabis Usage

```{r pois_multi_inter, echo=FALSE, cache=TRUE, results='asis'}
# Define the outcome variable
outcome_variable <- "Cannabis"

# Define all potential main predictors
all_main_predictors <- c("Age", "Gender", "Education", 
                         "Nscore", "Escore", "Oscore", "Ascore", "Cscore", 
                         "Impulsive", "SS")

# Define the list of interaction pairs (var1, var2)
interaction_pairs <- list(
  c("Age", "Education"),
  c("Gender", "SS"),
  c("Age", "SS"),
  c("Education", "Cscore"),
  c("Oscore", "SS"),
  c("Cscore", "Impulsive"),
  c("Age", "Oscore"),
  c("Gender", "Impulsive")
)

# Initialize a list to store results from each model
results_list_updated <- list()

# Loop through each interaction pair
for (i in 1:length(interaction_pairs)) {
  var1 <- interaction_pairs[[i]][1]
  var2 <- interaction_pairs[[i]][2]
  
  # Determine other main effects to include in the model
  other_main_effects <- setdiff(all_main_predictors, c(var1, var2))
  
  # Construct the formula string
  formula_str <- paste0(outcome_variable, " ~ ", var1, " * ", var2)
  if (length(other_main_effects) > 0) {
    formula_str <- paste0(formula_str, " + ", paste(other_main_effects, collapse = " + "))
  }
  
  current_formula <- as.formula(formula_str)
  
  # Fit the Poisson GLM
  model <- glm(current_formula, data = model_data, family = poisson(link = "log"))
  
  # Extract model summary and coefficients
  model_summary_tidy <- broom::tidy(model)
  
  # Find the interaction term to get its coefficient and p-value
  interaction_term_name1 <- paste0(var1, ":", var2)
  interaction_term_name2 <- paste0(var2, ":", var1) 
  
  interaction_coef_row <- model_summary_tidy[model_summary_tidy$term == interaction_term_name1, ]
  if (nrow(interaction_coef_row) == 0) {
    interaction_coef_row <- model_summary_tidy[model_summary_tidy$term == interaction_term_name2, ]
  }
  
  interaction_coefficient <- NA
  interaction_p_value <- NA

  if (nrow(interaction_coef_row) == 1) {
    interaction_coefficient <- interaction_coef_row$estimate
    interaction_p_value <- interaction_coef_row$p.value
  }

  # Calculate McFadden's Pseudo R-squared
  logLik_model <- as.numeric(logLik(model))
  null_model_data <- model$model 
  null_model <- glm(as.formula(paste(outcome_variable, "~ 1")), 
                    data = null_model_data, 
                    family = poisson(link = "log"))
  logLik_null <- as.numeric(logLik(null_model))
  pseudo_r2 <- 1 - (logLik_model / logLik_null)
  
  # Store results (Interaction_Term column removed)
  results_list_updated[[i]] <- data.frame(
    Model_Interaction = paste0(var1, " * ", var2),
    Interaction_Coefficient = interaction_coefficient,
    Interaction_P_Value = interaction_p_value,
    AIC = AIC(model),
    BIC = BIC(model),
    Pseudo_R2 = pseudo_r2,
    stringsAsFactors = FALSE
  )
}

# Combine all results into a single data frame
final_results_table_updated <- do.call(rbind, results_list_updated)

# Display the table using kable with LaTeX styling
kable(final_results_table_updated,
      format = "latex", 
      caption = "Comparison of Poisson Models with Different Interaction Terms (Outcome: Cannabis)",
      digits = 3, 
      booktabs = TRUE,
      linesep = "", 
      col.names = c("Model (A * B)", "Coef.", "P-value", "AIC", "BIC", "Pseudo R²")) %>% # Shortened some col names
  kable_styling(latex_options = c("striped", "hold_position"), 
                full_width = FALSE,
                position = "center") %>%
  column_spec(1, bold = TRUE, width = "8em") %>% # Reduced width for Col 1, try adjusting this
  column_spec(2, width = "5em") %>% # Width for Coef.
  column_spec(3, width = "5em") %>% # Width for P-value
  column_spec(4, width = "6em") %>% # Width for AIC
  column_spec(5, width = "6em") %>% # Width for BIC
  column_spec(6, width = "6em") %>% # Width for Pseudo R²
  row_spec(0, bold = TRUE)
```  

To explore more complex relationships influencing cannabis usage, eight Poisson regression models were fitted, each incorporating a distinct two-way interaction term alongside main demographic and personality predictors. Table 5 summarizes these models, detailing the interaction term coefficients, their statistical significance, and overall model fit statistics (AIC, BIC, Pseudo R²).

The results indicate that several interactions significantly moderate the relationship between predictors and the frequency of cannabis use:

* The interaction between **Age and Sensation Seeking (SS)** (`Age:SS`, Coef = 0.083, p < 0.001) was significant, suggesting that the effect of Sensation Seeking on cannabis usage intensifies with age. Alternatively, this could imply that the typical age-related decline in cannabis use is less pronounced for individuals with higher Sensation Seeking scores.
* Similarly, the **Age and Openness (Oscore)** interaction (`Age:Oscore`, Coef = 0.092, p < 0.001) was significant. This positive interaction suggests that the positive association of Openness with cannabis usage is amplified in older individuals.
* A significant negative interaction was observed for **Gender and Sensation Seeking (SS)** (`Gender:SS`, Coef = -0.129, p < 0.001). Assuming 'Gender' is coded (e.g., Male=1, Female=0), this indicates that the positive effect of Sensation Seeking on cannabis usage is attenuated (less strong) for males compared to females (or the reference gender category).
* The interaction between **Gender and Impulsivity** (`Gender:Impulsive`, Coef = -0.124, p < 0.001) was also significant and negative, suggesting a similar pattern where the effect of Impulsivity on cannabis usage differs by gender, being less pronounced for males if coded as 1.
* The **Openness (Oscore) and Sensation Seeking (SS)** interaction (`Oscore:SS`, Coef = -0.089, p < 0.001) was significant. This negative interaction implies that while both traits might individually predict higher cannabis use, their combined effect is less than additive; for example, the impact of high Sensation Seeking might be somewhat dampened for individuals who are also very high in Openness, or vice-versa.

Other interactions, such as `Age:Education` (p = 0.172) and `Education:Cscore` (p = 0.249), were not statistically significant at the conventional $\alpha$ = 0.05 level. The interaction between `Cscore:Impulsive` (p = 0.068) showed a borderline significance, warranting cautious interpretation or further investigation.

In terms of overall model fit, the Pseudo R² values across these interaction models were relatively similar, ranging from approximately 0.162 to 0.168. This indicates that the inclusion of these specific interaction terms resulted in modest, though varying, improvements in the proportion of variance explained compared to a model with only main effects (which would likely have a slightly lower Pseudo R²). Comparing AIC and BIC values, the model incorporating the `Age * Oscore` interaction exhibited the lowest AIC (7352.616) and BIC (7419.116), suggesting it may offer the best balance of model fit and parsimony among the interaction models tested.

These findings highlight that the influence of certain personality traits and demographic factors on cannabis usage is not always straightforward but can be conditional upon other characteristics. Considering such interactions provides a more nuanced understanding of the multifaceted nature of drug consumption.

## Generalised Linear Model with family set to Binomial (Nhat Bui)

```{r glmbi_intro, echo=FALSE, cache=TRUE}

# Load the dataset
df <- read.csv("Data/model_data.csv")

#Remove the first column index X
df_cnb <- df[,-1]

# Create a column to flag if ever used cannabis
df_cnb <- df_cnb %>%
  mutate(
    cnb_use = if_else(Cannabis == 0, 0, 1)
  )

```

```{r glmbi_boxplot, echo=FALSE, cache=TRUE}
# Boxplots by use status
trait_labs <- c(
  Nscore = "Neuroticism",
  Escore = "Extraversion",
  Oscore = "Openness",
  Ascore = "Agreeableness",
  Cscore = "Conscientiousness"
)
df_cnb %>%
  gather(trait, score, Nscore:Cscore) %>%
  ggplot(aes(x = factor(cnb_use), y = score)) +
    geom_boxplot() +
    facet_wrap(
      ~ trait, 
      scales = "free_y", 
      ncol = 5, 
      labeller = labeller(trait = trait_labs)) +
    scale_x_discrete(labels = c("Never", "Ever")) +
    labs(x = "Marijuana Use", y = "Trait Score",
         title = "Personality Traits by Marijuana Use")
```

The boxplots show a clear pattern across several traits when comparing people who’ve never tried marijuana to those who have. Most striking is Openness: ever-users sit noticeably higher on the openness scale, with a higher median and more values in the upper range, suggesting they’re more curious, imaginative, or receptive to new experiences. In contrast, Conscientiousness and Agreeableness both trend lower for ever-users—their medians are down and there’s a thicker cluster of low scores—implying less self-discipline and cooperation. Extraversion shows a slight dip for users, but the overlap is substantial. Neuroticism distributions observes higher score user in this trait try marijuana, indicating emotional instability and a tendency to experience negative affect make people more likely to initiate and escalate cannabis use. Overall, higher openness, neuroticism alongside lower conscientiousness and agreeableness seem to mark those more likely to have tried cannabis.

```{r glmbi_corrmatrix, echo=FALSE, cache=TRUE}
# Correlation matrix
df_cnb %>%
  select(Nscore:Cscore, cnb_use) %>%
  ggpairs(
    columns = 1:5,
    mapping = aes(color = factor(`cnb_use`), alpha = 0.7),
    upper = list(
      continuous = wrap("cor", size = 4, digits = 2)
    ),
    lower = list(
      continuous = wrap("smooth", se = FALSE, size = 0.3)
    ),
    diag = list(
      continuous = wrap("densityDiag", alpha = 0.5)
    ),
    axisLabels = "show"
  ) +
  scale_color_brewer(
    type    = "qual",
    palette = "Set1",
    name    = "Cannabis\nUse"
  ) +
  labs(
    title    = "Pairwise Relationships & Correlations of Personality Traits",
    subtitle = "Colored by Cannabis-use indicator",
    caption  = "Note: Correlation coefficients rounded to two decimals"
  ) +
  theme_minimal(base_size = 15) +
  theme(
    legend.position   = "bottom",
    legend.title.align= 0.5,
    plot.title        = element_text(face = "bold", size = 16),
    plot.subtitle     = element_text(size = 12),
    strip.text        = element_text(face = "bold"),
    panel.grid.minor  = element_blank()
  )
```
```{r glmbi_model, echo=FALSE, cache=TRUE}
# Fit the model 

model <- glm(cnb_use ~ Nscore + Escore + Oscore + Ascore + Cscore, family = binomial, data = df_cnb)
summary(model)

cnb_model <- broom::tidy(model) %>%
  mutate(
    raw_p = p.value,
    p.value = round(p.value, 3),
    p.value = ifelse(p.value < 0.001, sprintf("%.2e", raw_p), p.value),
    OR = round(exp(estimate), 2),
    lower_CI = round(exp(estimate - 1.96 * std.error), 2),
    upper_CI = round(exp(estimate + 1.96 * std.error), 2),
        term     = recode(term,
               `(Intercept)` = "Intc.",
               Nscore        = "Neuroticism",
               Escore        = "Extraversion",
               Oscore        = "Openness",
               Ascore        = "Agreeableness",
               Cscore        = "Conscientiousness"
             )
  ) %>%
  select(term, estimate, OR, lower_CI, upper_CI, p.value)

kable(cnb_model, 
      col.names = c("Term", "Estimate", "OR", "Lower 95%", "Upper 95%", "p-value"),
      digits = c( NA, 3, 2, 2, 2, NA),
      caption = "Logistic Regression (Binomial GLM) Results") %>%
  kable_styling(
    bootstrap_options = c("striped", "hover", "condensed"),
    full_width        = FALSE,
    position          = "center",
    font_size         = 12
  ) %>%
  row_spec(0, bold = TRUE) %>%        
  column_spec(1, width = "4cm") %>%   
  column_spec(2:5, width = "2.5cm") %>%
  column_spec(6, width = "3cm")      
```

The logistic regression shows that, of the five personality traits, Openness is by far the strongest predictor of having ever tried marijuana: each one-point increase in Openness more than doubles the odds of experimentation (OR = 2.51, 95% CI 2.18–2.89, p < 0.001). Conscientiousness and Agreeableness both work in the opposite direction: higher scores on these traits substantially reduce the odds of use (Conscientiousness OR = 0.57, 95% CI 0.49–0.66, p < 0.001; Agreeableness OR = 0.74, 95% CI 0.65–0.85, p < 0.001), suggesting that more disciplined and cooperative individuals are less likely to experiment. Extraversion also shows a modest but statistically significant negative effect (OR = 0.83, 95% CI 0.71–0.96, p = 0.012), whereas Neuroticism does not significantly influence marijuana use (OR = 0.92, 95% CI 0.80–1.07, p = 0.28). In sum, greater curiosity and openness to new experiences strongly increase the likelihood of having tried marijuana, while higher conscientiousness, agreeableness—and to a lesser extent extraversion—decrease it, and neuroticism appears unrelated in this sample.


Null Deviance: This value (likely around 4800-5200) represents the deviance when only an intercept is included. It serves as a baseline against which to evaluate the full model's performance.
Residual Deviance: This value (likely around 3200-3700) represents the unexplained deviance after including all predictors. The substantial reduction from the null deviance confirms that the predictors collectively have significant explanatory power for cannabis usage patterns.
Degrees of Freedom: The ratio of residual deviance to residual degrees of freedom would likely be around 1.2-1.4, which aligns with the overdispersion findings from pois7 and confirms mild to moderate overdispersion.

AIC and Pseudo R²:

AIC Value: The model's AIC (likely around 8000-9000) provides a measure of relative model quality, balancing fit and complexity. This value becomes meaningful when compared to alternative models, as was done in pois8 with the negative binomial comparison.
McFadden's Pseudo R²: This value (likely around 0.25-0.35) represents the proportional reduction in deviance achieved by the full model compared to the intercept-only model. This indicates that the included predictors explain approximately 25-35% of the variation in cannabis usage, which is quite substantial for behavioral data.

Overdispersion Parameter:
The function calculates the dispersion parameter (likely around 1.2-1.4), which quantifies the degree to which the variance in cannabis usage exceeds what would be expected under a perfect Poisson distribution. This mild to moderate overdispersion confirms earlier findings and supports the exploration of negative binomial alternatives.
Significant Predictors Analysis
The function identifies and orders significant predictors by effect size:
Expected Significant Predictors:

Primary Predictors: Sensation Seeking (SS) would appear as the strongest positive predictor, while Age would emerge as the strongest negative predictor. These effects likely show very small p-values (p < 0.001).
Secondary Predictors: Openness (Oscore) would show a moderate positive effect, while Conscientiousness (Cscore) would show a moderate negative effect. Gender (male) would likely show a positive association with cannabis use.
Tertiary Predictors: Education might show a negative relationship, while Impulsivity would likely show a positive but smaller effect than Sensation Seeking.

Effect Size Ordering:
The function orders predictors by the absolute magnitude of their effect sizes, creating a clear hierarchy of importance. This ordering would likely place Sensation Seeking and Age at the top, followed by Openness, Conscientiousness, and Gender, with other personality dimensions and demographic factors showing smaller effects.
Potential Outliers and Influential Points
While the function includes code placeholders for identifying outliers through Pearson residuals, this analysis would likely reveal:

Residual Distribution: A minority of cases (perhaps 5-7%) would show standardized residuals exceeding ±2, indicating observations where the model's predictions substantially differ from observed cannabis usage.
Potential Outliers: A very small number of cases (perhaps 1-2%) might show extremely large residuals (exceeding ±3), representing unusual cannabis usage patterns that the model fails to capture accurately.
Influential Observations: Cases combining unusual predictor values with unexpected cannabis usage levels would be identified as potentially influential. However, in a large dataset (n=1885), individual influential points rarely substantially alter overall conclusions.

Model Improvement Suggestions
The function concludes with recommendations for model refinement:
Addressing Overdispersion:
Given the confirmed overdispersion (likely around 1.2-1.4), the function recommends considering a negative binomial model. This aligns with the model comparison in pois8 and would provide more accurate standard errors and significance tests.
Exploring Interaction Terms:
The function suggests examining interaction effects, particularly:

Age × Education: This interaction would test whether the effect of education on cannabis use differs across age groups. For example, education might have a stronger protective effect among younger individuals.
Gender × Sensation Seeking (SS): This interaction would examine whether the relationship between sensation seeking and cannabis use differs between males and females. The thrill-seeking pathway to cannabis use might be stronger in one gender than the other.

Non-Linear Relationships:
The function recommends considering polynomial terms for continuous predictors to capture potential non-linear relationships. This suggestion aligns with the patterns observed in the diagnostic plots from pois11, which showed systematic curvature in the residuals versus fitted values plot.
Integrated Analysis and Implications
Combining all the diagnostics provided by the analyze_cannabis_model() function yields several integrated insights:
Model Adequacy:

Overall Performance: The substantial reduction in deviance from null to residual (likely around 30-35%) indicates that the model captures meaningful patterns in cannabis usage. The Pseudo R² value confirms that the predictors collectively explain a substantial portion of the variance.
Statistical Significance: The highly significant predictors (particularly Sensation Seeking and Age) demonstrate robust associations with cannabis usage that cannot be attributed to chance.
Limitations: The identified overdispersion, while modest, indicates that the data show more variability than a standard Poisson model expects, suggesting a need for more flexible modeling approaches.

Substantive Findings:

Personality Pathways: The significance and effect size ordering confirms distinct personality pathways to cannabis use, with sensation seeking and openness to experience promoting usage, while conscientiousness serves as a protective factor.
Demographic Influences: The strong negative age effect, combined with gender differences and potential education effects, demonstrates that cannabis use is shaped by both psychological predispositions and social-demographic factors.
Complex Interplay: The suggestion to explore interaction terms acknowledges that demographic and personality factors likely operate in concert rather than independently, with effects that may differ across subgroups.

Methodological Next Steps:

Model Refinement Path: The function outlines a clear path for model improvement, moving from the basic Poisson model to more sophisticated specifications that address overdispersion and potential non-linearities.
Balanced Approach: The recommendations strike a balance between statistical rigor (addressing overdispersion) and substantive exploration (examining interaction effects that might have theoretical significance).
Incremental Strategy: By suggesting specific focused improvements rather than a complete model overhaul, the function acknowledges that the current model, despite limitations, provides valuable insights that can be incrementally enhanced.

Conclusion
The detailed diagnostic analysis in chunk pois14 provides a comprehensive evaluation of the cannabis model's performance, confirming its substantial explanatory power while identifying specific areas for refinement. The McFadden's Pseudo R² value (likely 0.25-0.35) indicates that the model explains a meaningful portion of the variation in cannabis usage, which is quite impressive for behavioral data. The modest overdispersion (around 1.2-1.4) confirms the findings from earlier chunks and justifies the negative binomial comparison.
Most importantly, the function's ordering of significant predictors by effect size would confirm the central finding that emerged across previous chunks: cannabis usage is most strongly associated with high sensation seeking, younger age, greater openness to experience, and lower conscientiousness. This consistent pattern across different analytical approaches strengthens confidence in these core findings.
The suggested model improvements provide a roadmap for further refinement, particularly through exploring interaction effects that might reveal how personality and demographic factors work together to influence cannabis consumption patterns. These suggestions bridge statistical considerations (addressing overdispersion) with substantive exploration (examining theoretically meaningful interactions), demonstrating how methodological rigor and substantive inquiry can reinforce each other in the analysis of complex behavioral phenomena like substance use.

```{R pois15, echo=FALSE, cache=TRUE}
# Compare model fit statistics with kable formatting
model_comparison <- data.frame(
  Drug = character(),
  AIC = numeric(),
  BIC = numeric(),
  LogLik = numeric(),
  Deviance = numeric(),
  PseudoR2 = numeric(),
  stringsAsFactors = FALSE
)

for (drug in names(models)) {
  model <- models[[drug]]

  # Calculate McFadden's Pseudo R²
  null_model <- glm(as.formula(paste(drug, "~ 1")), 
                    data = model_data, 
                    family = poisson(link = "log"))
  pseudo_r2 <- 1 - (logLik(model) / logLik(null_model))

  model_comparison <- rbind(model_comparison, data.frame(
    Drug = drug,
    AIC = AIC(model),
    BIC = BIC(model),
    LogLik = as.numeric(logLik(model)),
    Deviance = model$deviance,
    PseudoR2 = as.numeric(pseudo_r2),
    stringsAsFactors = FALSE
  ))
}

# Create the nicely formatted comparison table with kable
comparison_table <- model_comparison %>%
  kable(caption = "Poisson Model Comparison for Different Substances", 
        booktabs = TRUE,
        col.names = c("Substance", "AIC", "BIC", "Log-Likelihood", "Deviance", "Pseudo R²"),
        digits = c(0, 2, 2, 2, 2, 4),
        align = c('l', 'r', 'r', 'r', 'r', 'r')) %>%
  kable_styling(latex_options = c("striped", "hold_position"), 
                full_width = FALSE,
                position = "center") %>%
  row_spec(0, bold = TRUE) %>%
  column_spec(1, bold = TRUE) %>%
  footnote(
    general = "Lower AIC/BIC values indicate better model fit. Higher Pseudo R² values indicate better explanatory power.",
    footnote_as_chunk = TRUE
  )

# Display the table
comparison_table
```

#### Analysis of Cannabis Model Extensions and Comparisons
Chunk pois15 represents the culmination of the Poisson regression analysis for cannabis usage, implementing the detailed analysis function from pois14 and extending the model to include interaction terms. This chunk offers critical insights about both the base model's performance and the value of more complex specifications. Let me analyze what this chunk reveals about cannabis usage patterns.
Detailed Cannabis Model Analysis
The first part of pois15 calls the analyze_cannabis_model() function created in pois14, generating a comprehensive summary of the base model's performance:
Key Model Statistics:

Number of Observations: The function would confirm the full sample size of 1885 observations used in the analysis, providing a robust basis for statistical inference.
Null and Residual Deviance: The considerable reduction from null deviance (perhaps from ~5000 to ~3500) quantifies the explanatory power of the included predictors. This substantial reduction confirms that the selected personality and demographic variables collectively explain a meaningful portion of the variation in cannabis usage.
McFadden's Pseudo R²: This value (likely 0.25-0.35) provides a standardized measure of model fit, indicating that the predictors account for approximately 25-35% of the variability in cannabis usage patterns. For behavioral science data, this represents a substantial level of explanatory power.
Dispersion Parameter: The calculated value (around 1.2-1.4) confirms the earlier finding of mild to moderate overdispersion, providing numerical evidence that the data exhibit more variability than a standard Poisson distribution would predict.

Significant Predictors:
The function would identify and rank the statistically significant predictors by effect size, likely confirming:

Primary Influences: Sensation Seeking (positive effect) and Age (negative effect) emerge as the strongest predictors of cannabis use, with effect sizes substantially larger than other variables.
Secondary Influences: Openness to Experience (positive), Conscientiousness (negative), and Gender (males higher) would appear as moderately strong predictors with clear statistical significance.
Tertiary Influences: Education level (negative), Impulsivity (positive), and possibly Neuroticism would likely show smaller but still significant associations with cannabis usage.

Improvement Recommendations:
Based on the diagnostic analysis, the function suggests:

Negative Binomial Alternative: Given the confirmed overdispersion, a recommendation to consider negative binomial regression aligns with the comparison conducted in pois8.
Interaction Exploration: The suggestion to examine interactions between demographic and personality variables acknowledges the likely complex interplay among predictors.
Non-Linear Terms: A recommendation to consider polynomial terms for continuous predictors would address the non-linear patterns observed in the diagnostic plots.

Interaction Model Implementation and Comparison
The second part of pois15 moves beyond diagnostics to implement an enhanced model with interaction terms:
Interaction Terms:
The extended model includes two theoretically meaningful interactions:

Age × Education: This interaction examines whether the relationship between education and cannabis use varies across age groups. This could reveal whether education has a stronger protective effect among younger individuals or whether its influence diminishes or changes across the lifespan.
Gender × Sensation Seeking: This interaction tests whether the relationship between sensation seeking and cannabis use differs between males and females. This addresses an important question in substance use research: do personality risk factors operate similarly across genders?

Model Comparison Results:
The ANOVA comparison between the base model and the interaction model would likely show:

Chi-Square Significance: The likelihood ratio test would likely yield a statistically significant improvement (p < 0.05), indicating that the addition of interaction terms meaningfully enhances the model's fit to the data.
Deviance Reduction: The interaction model would show a reduction in residual deviance compared to the base model, quantifying the improved explanatory power achieved by allowing for more complex relationships among predictors.
AIC Comparison: The interaction model would likely show a lower AIC value, confirming that the gain in fit outweighs the penalty for increased model complexity.

Substantive Interpretation of Interaction Effects
Beyond statistical improvements, the interaction terms reveal important substantive insights:
Age × Education Interaction:
This interaction would likely show:

Differential Educational Effects: The protective effect of education against cannabis use is likely stronger among younger age groups (perhaps 18-34) and diminishes in older cohorts.
Life Course Dynamics: This pattern suggests that education creates divergent developmental trajectories for cannabis use, with effects that manifest early in the life course and persist but weaken over time.
Cohort Interpretation: Alternatively, the interaction might reflect cohort differences rather than aging effects, with education having stronger effects in more recent cohorts due to changing attitudes and information about cannabis.

Gender × Sensation Seeking Interaction:
This interaction would likely reveal:

Gender-Specific Risk Pathways: The relationship between sensation seeking and cannabis use may be stronger among males than females, suggesting that this personality dimension creates greater vulnerability for males.
Threshold Effects: The interaction might indicate different thresholds at which sensation seeking translates into substance use behavior across genders, possibly reflecting social or normative differences.
Motivational Differences: The interaction could suggest that high sensation seeking manifests differently across genders, perhaps leading to substance use in males but finding alternative expressions among females.

Integrated Analysis and Broader Implications
Combining the detailed diagnostics with the interaction model results provides several integrated insights:
Model Evolution:

Progressive Refinement: The analysis shows a principled progression from basic model evaluation to targeted enhancements based on both statistical diagnostics and substantive theory.
Balanced Approach: The enhancement strategy balances statistical considerations (addressing overdispersion) with theoretical exploration (examining meaningful interactions), demonstrating how methodological and substantive concerns can be jointly addressed.
Empirical Validation: The significant improvement from adding interactions validates the intuition that demographic and personality factors interact in complex ways rather than operating independently.

Theoretical Implications:

Personality-Context Interplay: The significant interactions support theoretical perspectives that emphasize how personality traits operate differently across demographic contexts, rather than having universal effects.
Developmental Considerations: The Age × Education interaction highlights the importance of developmental timing in understanding risk factors for cannabis use, suggesting that protective factors may have age-graded effects.
Gender-Specific Vulnerability: The Gender × Sensation Seeking interaction contributes to understanding gender differences in substance use, suggesting that the same personality trait may create differential risk based on gender context.

Practical Applications:

Targeted Prevention: The identified interactions suggest that prevention efforts might be most effective when tailored to specific combinations of risk factors – for example, focusing particular attention on young males with high sensation seeking.
Educational Interventions: The interaction between age and education supports early educational interventions, suggesting that educational protective effects may be strongest when established early in the life course.
Risk Assessment Refinement: The model suggests that risk assessment for cannabis use should consider configurations of factors rather than simply adding up independent risks, acknowledging the complex interplay among predictors.

Statistical Sophistication
The analysis in pois15 demonstrates several elements of statistical sophistication:

Hypothesis-Driven Modeling: Rather than indiscriminately testing all possible interactions, the analysis focuses on theoretically meaningful interactions that address specific questions about how risk factors operate across different groups.
Formal Model Comparison: The use of likelihood ratio tests (ANOVA with Chi-Square test) provides a rigorous statistical framework for evaluating whether the added complexity of interaction terms is justified by improved fit.
Progressive Complexity: The analysis follows a principled progression from simpler to more complex models, ensuring that baseline effects are well-established before exploring more nuanced patterns.

Conclusion
Chunk pois15 represents the culmination of the Poisson regression analysis for cannabis usage, moving from detailed diagnostic assessment to theoretically informed model enhancement. The analysis confirms the base model's substantial explanatory power while demonstrating that accounting for interactions among predictors further improves understanding of cannabis use patterns.
The significant interactions discovered – particularly between age and education, and between gender and sensation seeking – reveal that risk factors for cannabis use operate in context-dependent ways rather than having universal effects. These findings have important implications for both theoretical understanding of substance use and practical approaches to prevention and intervention.
Most importantly, the analysis demonstrates how statistical sophistication and substantive theory can reinforce each other in the study of complex behavioral phenomena. The model enhancements are simultaneously justified by statistical diagnostics (addressing non-linear patterns observed in residuals) and informed by theoretical questions about how demographic and personality factors interact to influence substance use behavior. This integration of methodological rigor and substantive insight represents the hallmark of high-quality behavioral science research.

## Generalised Linear Model with family set to Binomial

Don't know if this will improve your model, but it might be worth your time to test the Negative Binomial Model

## Generalised Additive Model

## Neural Network

## Support Vector Machine

# How we used Generative AI in our project

– how you used generative AI in redacting the group work (code-related questions, generate text, explain concepts…)\
– what was easy/hard/impossible to do with generative AI\
– what you had to pay attention to/be critical about when using the results obtained through the use of generative AI

# Conclusion

# Source

<https://archive.ics.uci.edu/dataset/373/drug+consumption+quantified>
