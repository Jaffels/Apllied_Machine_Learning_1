---
title: "Drug Consumption"
author: "Nhat Bui, Johan Ferreira, Thilo Holstein"
date: "2025-03-06"
output:
  pdf_document:
    toc: true
    toc_depth: 2
    number_sections: true
    fig_width: 6
    fig_height: 4
    fig_caption: true
    keep_tex: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

\newpage

# Introduction

Drug use is a significant risk behavior with serious health consequences for individuals and society. Multiple factors contribute to initial drug use, including psychological, social, individual, environmental, and economic elements, as well as personality traits. While legal substances like sugar, alcohol, and tobacco cause more premature deaths, illegal recreational drugs still create substantial social and personal problems.

In this data science project, we aim to identify factors and patterns potentially explaining drug use behaviors through machine learning techniques. By analyzing demographic, psychological, and social variables in our dataset, we'll aim to uncover potential predictors, use machine learning methods to understand the complex relationships surrounding drug consumption, demonstrating how machine learning can reveal insights into behavioral patterns. While our findings won't directly inform interventions, this project showcases how data-driven approaches can enhance our understanding of complex social phenomena and provide valuable practice in applying machine learning to real-world datasets.

The database contains records for 1,885 respondents with 12 attributes including personality measurements (NEO-FFI-R, BIS-11, ImpSS), demographics (education, age, gender, country, ethnicity), and self-reported usage of 18 drugs plus one fictitious drug (Semeron). Drug use is classified into seven categories ranging from "Never Used" to "Used in Last Day." All input attributes are quantified as real values, creating 18 distinct classification problems corresponding to each drug. A detailed description of the variables can be found in the Column Decsription text file.

# Personality Traits Explanation

To better understand the data set we need to have an understanding of what the personality traits are and what the represent, we will have short description of each trait and how to interpret them:

-   Nscore (Neuroticism): Measures emotional stability vs. instability. Higher scores indicate tendency toward negative emotions like anxiety, depression, vulnerability, and mood swings. Lower scores suggest emotional stability and resilience to stress.
-   Escore (Extraversion): Measures sociability and outgoingness. Higher scores indicate preference for social interaction, assertiveness, and energy in social settings. Lower scores suggest preference for solitude, quieter environments, and more reserved behavior.
-   Oscore (Openness to Experience): Measures intellectual curiosity and creativity. Higher scores indicate imagination, appreciation for art/beauty, openness to new ideas, and unconventional thinking. Lower scores suggest preference for routine, practicality, and conventional approaches.
-   Ascore (Agreeableness): Measures concern for social harmony. Higher scores indicate empathy, cooperation, and consideration for others. Lower scores suggest competitive, skeptical, or challenging interpersonal styles.
-   Cscore (Conscientiousness): Measures organization and reliability. Higher scores indicate discipline, responsibility, planning, and detail orientation. Lower scores suggest spontaneity, flexibility, and potentially less structured approaches.
-   Impulsive (Impulsiveness): Measures tendency to act without thinking. Higher scores indicate spontaneous decision-making without considering consequences. Lower scores suggest thoughtful deliberation before actions.
-   SS (Sensation Seeking): Measures desire for novel experiences and willingness to take risks. Higher scores indicate thrill-seeking behavior and preference for excitement. Lower scores suggest preference for familiarity and safety.

The first five traits (Nscore through Cscore) are the "Big Five" personality traits, which are widely used in psychological research. The Impulsive and SS measures are additional traits that are often studied in relation to risk-taking behaviors, which would make sense given our dataset includes variables related to substance use.

# Cleaning and Formatting the Dataset

```{r include=FALSE, cache=FALSE}
# Load required libraries
library(MASS)
library(ggplot2)
library(reshape2)
library(kableExtra)
library(knitr)
library(gridExtra)
library(tidyr)
library(car)
library(broom)
library(dplyr)

# Suppresses all warnings
options(warn = -1)
```

```{r echo=FALSE, cache=TRUE}
# Read the CSV file
drug_data <- read.csv("Data/drug_consumption.csv")
```

## Fomatting the Dataset

The original data set had all the values for most of the variables set to a random floating number representing a specific categorical value, we believe this was done in order to remove bias from the dataset. As the requirements of this project is different form the data set's original intention we had to replace these values with the original values in order to complete all the required steps for our project.

```{r echo=FALSE, cache=TRUE}
###############################################################################
# Define mappings and column information
###############################################################################

# Column names for the dataset
column_names <- c(
  "Index", "ID", "Age", "Gender", "Education", "Country", "Ethnicity", 
  "Nscore", "Escore", "Oscore", "Ascore", "Cscore", "Impulsive", "SS", 
  "Alcohol", "Amphet", "Amyl", "Benzos", "Caff", "Cannabis", "Choc", "Coke", 
  "Crack", "Ecstasy", "Heroin", "Ketamine", "Legalh", "LSD", "Meth", 
  "Mushrooms", "Nicotine", "Semer", "VSA"
)

# Drug column names
drug_columns <- c(
  "Alcohol", "Amphet", "Amyl", "Benzos", "Caff", "Cannabis", 
  "Choc", "Coke", "Crack", "Ecstasy", "Heroin", "Ketamine", 
  "Legalh", "LSD", "Meth", "Mushrooms", "Nicotine", "Semer", "VSA"
)

# Mapping of drug consumption classes to their meanings
consumption_mapping <- c(
  "CL0" = "Never Used",
  "CL1" = "Used over a Decade Ago",
  "CL2" = "Used in Last Decade", 
  "CL3" = "Used in Last Year",
  "CL4" = "Used in Last Month",
  "CL5" = "Used in Last Week",
  "CL6" = "Used in Last Day"
)

# Map Age values to their meaning
age_mapping <- c(
  "-0.95197" = "18-24",
  "-0.07854" = "25-34",
  "0.49788" = "35-44",
  "1.09449" = "45-54",
  "1.82213" = "55-64",
  "2.59171" = "65+"
)

# Map Gender values to their meaning
gender_mapping <- c(
  "0.48246" = "Female",
  "-0.48246" = "Male"
)

# Map Education values to their meaning
education_mapping <- c(
  "-2.43591" = "Left school before 16 years",
  "-1.73790" = "Left school at 16 years",
  "-1.43719" = "Left school at 17 years",
  "-1.22751" = "Left school at 18 years",
  "-0.61113" = "Some college or university, no certificate or degree",
  "-0.05921" = "Professional certificate/diploma",
  "0.45468" = "University degree",
  "1.16365" = "Masters degree",
  "1.98437" = "Doctorate degree"
)

# Map Country values to their meaning
country_mapping <- c(
  "-0.09765" = "Australia",
  "0.24923" = "Canada",
  "-0.46841" = "New Zealand",
  "-0.28519" = "Other",
  "0.21128" = "Republic of Ireland",
  "0.96082" = "UK",
  "-0.57009" = "USA"
)

# Map Ethnicity values to their meaning
ethnicity_mapping <- c(
  "-0.50212" = "Asian",
  "-1.10702" = "Black",
  "1.90725" = "Mixed-Black/Asian",
  "0.12600" = "Mixed-White/Asian",
  "-0.22166" = "Mixed-White/Black",
  "0.11440" = "Other",
  "-0.31685" = "White"
)
```

```{r echo=FALSE, cache=TRUE}
###############################################################################
# Data Processing
###############################################################################

# Rename the columns 
colnames(drug_data) <- column_names

# Convert demographic columns to descriptive values
drug_data$Age <- age_mapping[as.character(drug_data$Age)]
drug_data$Gender <- gender_mapping[as.character(drug_data$Gender)]
drug_data$Education <- education_mapping[as.character(drug_data$Education)]
drug_data$Country <- country_mapping[as.character(drug_data$Country)]
drug_data$Ethnicity <- ethnicity_mapping[as.character(drug_data$Ethnicity)]

# Convert all drug consumption columns to descriptive values
for (col in drug_columns) {
  drug_data[[col]] <- consumption_mapping[as.character(drug_data[[col]])]
}
```

## Investigating Missing Values

```{r echo=FALSE, cache=TRUE}
###############################################################################
# Data Cleaning - Missing values
###############################################################################

# Remove unnecessary column
drug_data <- drug_data[, -which(names(drug_data) == "ID")]

# Check for NA values in each column
na_by_column <- sapply(drug_data, function(x) sum(is.na(x)))
cat("NA values by column:")

# Print only columns with NA values
print(na_by_column[na_by_column > 0])  
```

Only two columns contain missing values, affecting approximately 5% of the 1885 observations. Given the nature of these variables and the completeness of the rest of the data, we assume participants deliberately withheld this information. Therefore, we replaced the missing values with "Not Provided", allowing us to treat these instances as a distinct category.

```{r echo=FALSE, cache=TRUE}
# Replace NA values with "Not Provided"
drug_data$Education[is.na(drug_data$Education)] <- "Not Provided"
drug_data$Ethnicity[is.na(drug_data$Ethnicity)] <- "Not Provided"

# Save the updated dataframe back to CSV
write.csv(drug_data, "Data/cleaned.csv")
```

## Investigating Outliers

```{r echo=FALSE, cache=TRUE}
###############################################################################
# Data Cleaning - Looking for outliers
###############################################################################
# Define numeric columns for outlier analysis
numeric_cols <- c("Nscore", "Escore", "Oscore", "Ascore", "Cscore", "Impulsive", "SS")

# Function to identify outliers using IQR method
identify_outliers_iqr <- function(x) {
  q1 <- quantile(x, 0.25, na.rm = TRUE)
  q3 <- quantile(x, 0.75, na.rm = TRUE)
  iqr <- q3 - q1
  lower_bound <- q1 - 1.5 * iqr
  upper_bound <- q3 + 1.5 * iqr
  
  return(data.frame(
    min = min(x, na.rm = TRUE),
    q1 = q1,
    median = median(x, na.rm = TRUE),
    mean = mean(x, na.rm = TRUE),
    q3 = q3,
    max = max(x, na.rm = TRUE),
    iqr = iqr,
    lower_bound = lower_bound,
    upper_bound = upper_bound,
    n_outliers_below = sum(x < lower_bound, na.rm = TRUE),
    n_outliers_above = sum(x > upper_bound, na.rm = TRUE),
    total_outliers = sum(x < lower_bound | x > upper_bound, na.rm = TRUE),
    outlier_percentage = round(100 * sum(x < lower_bound | x > upper_bound, na.rm = TRUE) / length(x[!is.na(x)]), 2)
  ))
}

# Apply outlier detection to all numeric columns
outlier_summary <- data.frame()
for (col in numeric_cols) {
  result <- identify_outliers_iqr(drug_data[[col]])
  result$variable <- col
  outlier_summary <- rbind(outlier_summary, result)
}

# Create a function to visualize outliers with boxplots
plot_outliers <- function(drug_data, columns) {
  # Explicitly use reshape2::melt to avoid namespace issues
  melted_data <- reshape2::melt(drug_data[, columns], id.vars = NULL)
  
  # Create boxplot
  ggplot(melted_data, aes(x = variable, y = value)) +
    geom_boxplot(outlier.colour = "red", outlier.shape = 16, outlier.size = 2) +
    theme_minimal() +
    labs(title = "Boxplots with Outliers Highlighted",
         x = "Variable",
         y = "Value") +
    theme(axis.text.x = element_text(angle = 45, hjust = 1))
}

# Visualize outliers
plot_outliers(drug_data, numeric_cols)
```

As can be seen from the box plots our data set has some values that are outside of the upper and lower bounds. All thought these values are technically outliers they are not extreme, still fall inside of the range of our expected values and conforms to a normal distribution.

# Exploratory Data Analysis

## Correlation between Behavioral Measures

```{r echo=FALSE, cache=TRUE}
###############################################################################
# Heatmap
###############################################################################

# Calculate the correlation matrix
cor_matrix <- cor(drug_data[numeric_cols], use = "pairwise.complete.obs")

# Convert the correlation matrix to a data frame for ggplot
cor_df <- melt(cor_matrix)
names(cor_df) <- c("Var1", "Var2", "Correlation")

# Create a ggplot2 correlation heatmap
ggplot(data = cor_df, aes(x = Var1, y = Var2, fill = Correlation)) +
  geom_tile() +
  scale_fill_gradient2(low = "blue", high = "red", mid = "white", 
                       midpoint = 0, limit = c(-1,1), space = "Lab", 
                       name="Pearson\nCorrelation") +
  geom_text(aes(label = round(Correlation, 2)), color = "black", size = 3) +
  theme_minimal() +
  theme(
    axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1),
    plot.title = element_text(hjust = 0.5)
  ) +
  coord_fixed() +
  labs(
    title = "Correlation Matrix Behavioral Measures",
    x = "",
    y = ""
  )
```

The correlation matrix shows that certain personality traits tend to cluster thougher for example SS (Sensation Seeking) has a positive correlation with Escore (Extraversion), Oscore (Openness) and Impulsive while they in turn also have positive correlations which other and a negative correlation to Cscore (Conscientiousness) and Ascore (Agreeableness) while they have a positive correlation with each other.

## Comparing Behavioral Measure for Gender

```{r echo=FALSE, cache=TRUE}
###############################################################################
# Gender comparison
###############################################################################

# Calculating the means of the behavioral scores
gender_results <- data.frame(
  Trait = character(),
  Female_Mean = numeric(),
  Male_Mean = numeric(),
  stringsAsFactors = FALSE
)

for (col in numeric_cols) {
  # Calculate means only
  female_mean <- mean(drug_data[drug_data$Gender == "Female", col], na.rm = TRUE)
  male_mean <- mean(drug_data[drug_data$Gender == "Male", col], na.rm = TRUE)
  
  # Add to results dataframe with only needed values
  gender_results <- rbind(gender_results, data.frame(
    Trait = col,
    Female_Mean = female_mean,
    Male_Mean = male_mean,
    stringsAsFactors = FALSE
  ))
}

# Create readable trait names
gender_results$Trait_Name <- case_when(
  gender_results$Trait == "Nscore" ~ "Neuroticism",
  gender_results$Trait == "Escore" ~ "Extraversion",
  gender_results$Trait == "Oscore" ~ "Openness",
  gender_results$Trait == "Ascore" ~ "Agreeableness",
  gender_results$Trait == "Cscore" ~ "Conscientiousness",
  gender_results$Trait == "Impulsive" ~ "Impulsivity",
  gender_results$Trait == "SS" ~ "Sensation Seeking",
  TRUE ~ gender_results$Trait
)

# Create the plot directly from the results
ggplot(gender_results, aes(x = Trait_Name)) +
  geom_bar(aes(y = Female_Mean, fill = "Female"), stat = "identity", position = "dodge", width = 0.7, alpha = 0.7) +
  geom_bar(aes(y = Male_Mean, fill = "Male"), stat = "identity", position = position_dodge(width = 0.7), width = 0.7, alpha = 0.7) +
  scale_fill_manual(values = c("Female" = "#FF9999", "Male" = "#6699CC"),
                    name = "Gender") +
  labs(title = "Gender Differences in Behavioral Measures",
       x = "",
       y = "Mean Score") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1, size = 10),
        plot.title = element_text(hjust = 0.5, size = 14),
        legend.position = "top") +
  # Fixed axis limits from -0.25 to 0.25
  scale_y_continuous(limits = c(-0.25, 0.25)) +
  coord_flip()
```

The mean of all the Behavioral Measures is 0, the chart show the mean score broken down by gender for each Behavioral Measures. That chart shows that males tend to be more sensation seeking and impulsive but also more open, where females tend to be more impulsive but also more agreeable and conscientious.

## Comparing Education Level with Behavioral Measures

```{r, fig.width=7.5, fig.height=8.5, dev="pdf", echo=FALSE, cache=TRUE}
###############################################################################
# Education comparison
###############################################################################
# Order education levels
education_order <- c(
  "Left school before 16 years",
  "Left school at 16 years",
  "Left school at 17 years",
  "Left school at 18 years",
  "Some college or university, no certificate or degree",
  "Professional certificate/diploma",
  "University degree",
  "Masters degree",
  "Doctorate degree",
  "Not Provided"
)

# Calculate means by education level
education_means <- drug_data %>%
  group_by(Education) %>%
  summarize(
    n = n(),
    across(all_of(numeric_cols), ~mean(.x, na.rm = TRUE))
  ) %>%
  arrange(match(Education, education_order))

# Create a clean table view of the data
education_display <- education_means %>%
  dplyr::select(-n) %>% # Corrected line: Now uses dplyr::select
  rename(
    "Neuroticism" = Nscore,
    "Extraversion" = Escore,
    "Openness" = Oscore,
    "Agreeableness" = Ascore,
    "Conscientiousness" = Cscore,
    "Impulsivity" = Impulsive,
    "Sensation Seeking" = SS
  )

# Create a data frame for plotting
education_plot_data <- education_means %>%
  select(-n)

# Simplified visualization approach with more space for the title
# Set up the plotting parameters
par(mfrow = c(4, 2), mar = c(8, 4, 2, 1), oma = c(0, 0, 3, 0))

# Define the trait columns and their display names
trait_cols <- c("Nscore", "Escore", "Oscore", "Ascore", "Cscore", "Impulsive", "SS")
trait_names <- c("Neuroticism", "Extraversion", "Openness", "Agreeableness", 
                "Conscientiousness", "Impulsivity", "Sensation Seeking")

# Plot each trait separately
for (i in 1:length(trait_cols)) {
  col <- trait_cols[i]
  trait_name <- trait_names[i]
  
  # Create a simple barplot
  bp <- barplot(education_plot_data[[col]], 
          names.arg = rep("", nrow(education_plot_data)),
          col = ifelse(education_plot_data[[col]] > 0, "salmon", "lightblue"),
          main = trait_name,
          border = NA,
          las = 2,
          ylim = c(min(education_plot_data[[col]]) - 0.05, 
                  max(education_plot_data[[col]]) + 0.05),
          cex.main = 0.9,
          cex.axis = 0.8)
  
  # Add a reference line at y=0
  abline(h = 0, lty = 2, col = "gray")
  
  # Add abbreviated education labels
  edu_labels <- c("Before 16", "At 16", "At 17", "At 18", "Some college", 
                 "Prof cert", "University", "Masters", "Doctorate", "Not Provided")
  edu_labels <- edu_labels[1:length(bp)]  # Ensure we have the right number of labels
  
  # Add text labels at the bottom
  text(bp, par("usr")[3] - 0.02, srt = 45, adj = 1, labels = edu_labels, 
       xpd = TRUE, cex = 0.7)
}

# Add a title for the entire set of plots with more space
mtext("Personality Traits by Education Level", side = 3, line = 1, 
      outer = TRUE, cex = 1.2, font = 2)
```

The charts show the mean score for the behavioral traits broken down by educational level. It is not very clear at first glance but when you study the table closely it becomes clear that traits that can be precived as bad like Neuroticism, Impulsivity and Sensation Seeking is more prevelent with lower education levels inccluding Not Provided and steadily decrease as the level of education increases.

## Analysis of Seremon Usage

```{r echo=FALSE, cache=TRUE}
###############################################################################
# Seremon
###############################################################################

# Count Semeron users vs non-users
semeron_counts <- drug_data %>%
  group_by(Semer) %>%
  summarise(Count = n()) %>%
  arrange(desc(Count))
```

```{r problem_chuck, echo=FALSE, cache=TRUE}
# Load pacakges again
library(dplyr)
library(kableExtra)

# Create a nicely formatted table for the detailed counts
semeron_table <- semeron_counts %>%
  mutate(Percentage = paste0(round(Count / sum(Count) * 100, 2), "%")) %>%
  rename(`Usage Category` = Semer) %>%
  kable(caption = "Semeron Usage Categories", 
        booktabs = TRUE, 
        col.names = c("Usage Category", "Count", "Percentage"))

# For PDF output, apply specific styling
if(knitr::is_latex_output()) {
  semeron_table <- semeron_table %>%
    kable_styling(latex_options = c("striped", "hold_position"), 
                  full_width = FALSE,
                  position = "center") %>%
    row_spec(0, bold = TRUE) %>%
    column_spec(1, width = "5cm") %>%
    column_spec(2, width = "2cm") %>%
    column_spec(3, width = "2.5cm")
}

# Display the table
semeron_table
```

Semeron is a non exsisting drug that was introduced to the questonaire. With only 0.42% of resopndents reporting usage of Semeron. This would indicate that the survey data is likely of good quality, with most respondents providing attentive and truthful answers regarding their substance use.

# Prepraring the Dataset for Machine Learning

```{r echo=FALSE, cache=TRUE}
################################################################################
# Prep the data for modeling
################################################################################

# Make a copy of the dataset
model_data <- drug_data

# Remove the fake drug Semeron and index column
model_data <- model_data %>% 
  select(-c(Semer, Index))

# Map drug levels
consumption_levels <- c(
  "Never Used" = 0,
  "Used over a Decade Ago" = 1,
  "Used in Last Decade" = 2,
  "Used in Last Year" = 3,
  "Used in Last Month" = 4,
  "Used in Last Week" = 5,
  "Used in Last Day" = 6
)

# Iterate through each specified drug column
for (col_name in drug_columns) {
  if (col_name %in% names(model_data)) {
    column_values_as_char <- as.character(model_data[[col_name]])
    model_data[[col_name]] <- unname(consumption_levels[column_values_as_char])
  } 
}

# Convert gender to binary encoding
model_data$Gender <- ifelse(model_data$Gender == "Male", 1, 0)

# Change Age levels to ordinal
age_levels <- c("18-24", "25-34", "35-44", "45-54", "55-64", "65+")
model_data$Age <- as.integer(factor(model_data$Age, levels = age_levels))

# Change Education levels to ordinal
education_levels <- c(
  "Not Provided",
  "Left school before 16 years",
  "Left school at 16 years",
  "Left school at 17 years",
  "Left school at 18 years",
  "Some college or university, no certificate or degree",
  "Professional certificate/diploma",
  "University degree",
  "Masters degree",
  "Doctorate degree"
)
model_data$Education <- as.integer(factor(model_data$Education, levels = education_levels))

# Country - One-hot Encoding
country <- model.matrix(~ Country - 1, data = model_data)
model_data <- cbind(model_data, country)

# Ethnicity - One-hot Encoding
ethnicity <- model.matrix(~ Ethnicity - 1, data = model_data)
model_data <- cbind(model_data, ethnicity)

# Remove the original columns
model_data <- model_data %>% select(-c(Country, Ethnicity))

# Save the updated dataframe to csv
write.csv(model_data, "Data/model_data.csv")
```

Since the main focus of the project is implementing machine learning models we decided to prepare our data for this purpose. Just like we converted our original dataset to be more human readable for data exploration we have changed our dataset dataset to be more machine readable. The sex column was changed to binary data and for all the Drug columns, Education and Age we converted the data to ordinal data.

For the Ethnicity and Country columns we used a technique called One-Hot Encoding, where we transforms a categorical variable with multiple possible values into multiple binary (0 or 1) columns. Each new column represents one possible category from the original variable, and for each observation, exactly one of these new columns will have the value 1 (hence "one-hot") while all others will be 0.

It prevents the machine learning algorithm from assuming an arbitrary numerical relationship between categories. For example, if you simply encoded "USA"=1, "UK"=2, "Canada"=3, the algorithm might incorrectly assume that "Canada" is somehow "greater than" or "three times more important than" "USA".

# Machine Learning Models

## Linear Model

(Johan Ferreira)

As linear regression is not the ideal model for our dataset when making predictions we decided to use linear regression to better understand what factors influences drug use and focus in the better suited models on making predictions.

```{r lr1, echo=FALSE, cache=TRUE}

# Read the processed dataset
model_data <- read.csv("Data/model_data.csv")

# Remove the first index column if it exists
if(names(model_data)[1] == "X") {
  model_data <- model_data[,-1]
}

#---------------------------------------------------------------
# Linear Regression Modeling
#---------------------------------------------------------------

# Create clean names for drugs to analyze
drug_names <- c("Cannabis", "Alcohol", "Nicotine", "Coke", "Ecstasy")

# Function to build and evaluate linear regression model
run_drug_regression <- function(data, drug_name) {
  # Formula creation - all features, but handle multicollinearity in categorical variables
  
  # For country variables, exclude one as reference (USA)
  country_vars <- grep("Country", names(data), value = TRUE)
  country_vars <- country_vars[country_vars != "CountryUSA"] # Use USA as reference
  
  # For ethnicity variables, exclude one as reference (White)
  ethnicity_vars <- grep("Ethnicity", names(data), value = TRUE)
  ethnicity_vars <- ethnicity_vars[ethnicity_vars != "EthnicityWhite"] # Use White as reference
  
  # Create formula with modified variables to avoid perfect multicollinearity
  formula_str <- paste(drug_name, "~ Age + Gender + Education + Nscore + Escore + Oscore + Ascore + Cscore + Impulsive + SS + ", 
                     paste(c(country_vars, ethnicity_vars), collapse = " + "))
  
  formula <- as.formula(formula_str)
  
  # Fit model
  model <- lm(formula, data = data)
  
  # Check VIF for multicollinearity
  # First check if the model has aliased coefficients
  alias_check <- alias(model)
  has_aliased <- length(alias_check$Complete) > 0
  
  # Only run VIF if no aliased coefficients
  if(!has_aliased) {
    vif_values <- vif(model)
    high_vif <- vif_values[vif_values > 5]
  } else {
    # If there are aliased coefficients, we can't calculate VIF
    vif_values <- "Aliased coefficients detected"
    high_vif <- "Aliased coefficients detected"
    
    # Get the names of the aliased coefficients
    aliased_names <- rownames(alias_check$Complete)
    cat("Aliased coefficients detected in model for", drug_name, ":", paste(aliased_names, collapse=", "), "\n")
  }
  
  # Optional: Step-wise selection for feature selection
  # step_model <- step(model, direction = "both")
  
  # Return model and diagnostics
  return(list(
    model = model,
    summary = summary(model),
    vif = vif_values,
    high_vif = high_vif
  ))
}

# Create a results container
results_list <- list()

# Run regression for selected drugs
for (drug in drug_names) {
  # Check if the drug exists in the dataset
  if (drug %in% names(model_data)) {
    results_list[[drug]] <- run_drug_regression(model_data, drug)
  } else {
    cat("Warning: Drug", drug, "not found in dataset\n")
  }
}
```

```{r lr2, echo=FALSE, cache=TRUE}
#---------------------------------------------------------------
# Model Comparison and Visualization
#---------------------------------------------------------------

# Function to create a summary table of model performance
create_model_summary_table <- function(results_list) {
  # Initialize empty data frame
  summary_df <- data.frame(
    Drug = character(),
    R_squared = numeric(),
    Adj_R_squared = numeric(),
    F_statistic = numeric(),
    P_value = numeric(),
    Top_positive_predictor = character(),
    Top_negative_predictor = character(),
    stringsAsFactors = FALSE
  )
  
  # Fill with results
  for (drug in names(results_list)) {
    model_summary <- results_list[[drug]]$summary
    
    # Get coefficients
    coefs <- model_summary$coefficients
    
    # Find top predictors (excluding intercept)
    coef_df <- data.frame(
      Variable = rownames(coefs)[-1],
      Estimate = coefs[-1, "Estimate"],
      P_value = coefs[-1, "Pr(>|t|)"]
    )
    
    # Get significant predictors only
    sig_coefs <- coef_df[coef_df$P_value < 0.05, ]
    
    if(nrow(sig_coefs) > 0) {
      # Get top positive and negative predictors
      top_pos <- sig_coefs[which.max(sig_coefs$Estimate), "Variable"]
      top_neg <- sig_coefs[which.min(sig_coefs$Estimate), "Variable"]
    } else {
      top_pos <- "None"
      top_neg <- "None"
    }
    
    # Add to summary
    summary_df <- rbind(summary_df, data.frame(
      Drug = drug,
      R_squared = model_summary$r.squared,
      Adj_R_squared = model_summary$adj.r.squared,
      F_statistic = model_summary$fstatistic[1],
      P_value = pf(model_summary$fstatistic[1], 
                 model_summary$fstatistic[2], 
                 model_summary$fstatistic[3], 
                 lower.tail = FALSE),
      Top_positive_predictor = top_pos,
      Top_negative_predictor = top_neg,
      stringsAsFactors = FALSE
    ))
  }
  
  return(summary_df)
}

# Create and format the summary table
model_summary_table <- create_model_summary_table(results_list)
```

```{r lr3, echo=FALSE, cache=TRUE}
#---------------------------------------------------------------
# Linear Regression Plots
#---------------------------------------------------------------

# Function to create a visually appealing coefficient plot
plot_factor_importance_enhanced <- function(model, title, color_scheme = "viridis") {
  # Extract model coefficients
  coefs <- summary(model)$coefficients
  
  # Filter out intercept and create data frame for plotting
  coef_df <- data.frame(
    Variable = rownames(coefs)[-1],  # Exclude intercept
    Estimate = coefs[-1, "Estimate"],
    StdError = coefs[-1, "Std. Error"],
    PValue = coefs[-1, "Pr(>|t|)"]
  )
  
  # Add significance markers and categories
  coef_df$Significance <- ifelse(coef_df$PValue < 0.001, "p < 0.001", 
                               ifelse(coef_df$PValue < 0.01, "p < 0.01",
                                    ifelse(coef_df$PValue < 0.05, "p < 0.05", "Not significant")))
  
  # Sort by absolute value of estimate
  coef_df <- coef_df[order(abs(coef_df$Estimate), decreasing = TRUE), ]
  
  # Keep only top 15 predictors for visualization clarity
  if(nrow(coef_df) > 15) {
    coef_df <- coef_df[1:15, ]
  }
  
  # Clean up variable names for display
  var_display_mapping <- c(
    "Age" = "Age",
    "Gender" = "Gender (Male=1)",
    "Education" = "Education Level",
    "Nscore" = "Neuroticism",
    "Escore" = "Extraversion",
    "Oscore" = "Openness",
    "Ascore" = "Agreeableness",
    "Cscore" = "Conscientiousness",
    "Impulsive" = "Impulsivity",
    "SS" = "Sensation Seeking"
  )
  
  # Function to clean up variable names
  clean_var_names <- function(var_name) {
    # First check exact matches in our mapping
    if (var_name %in% names(var_display_mapping)) {
      return(var_display_mapping[var_name])
    }
    
    # Handle country variables
    if (grepl("^Country", var_name)) {
      return(gsub("Country", "Country: ", var_name))
    }
    
    # Handle ethnicity variables
    if (grepl("^Ethnicity", var_name)) {
      return(gsub("Ethnicity", "Ethnicity: ", var_name))
    }
    
    return(var_name)
  }
  
  # Apply name cleaning
  coef_df$DisplayName <- sapply(coef_df$Variable, clean_var_names)
  
  # Reorder factor levels for plotting
  coef_df$DisplayName <- factor(coef_df$DisplayName, levels = rev(coef_df$DisplayName))
  
  # Define significance color palette
  if (color_scheme == "viridis") {
    sig_colors <- c("p < 0.001" = "#440154", "p < 0.01" = "#21908C", 
                    "p < 0.05" = "#5DC863", "Not significant" = "#CCCCCC")
  } else {
    sig_colors <- c("p < 0.001" = "#0072B2", "p < 0.01" = "#009E73", 
                    "p < 0.05" = "#56B4E9", "Not significant" = "#CCCCCC")
  }
  
  # Plot with enhanced aesthetics
  ggplot(coef_df, aes(x = Estimate, y = DisplayName, color = Significance)) +
    geom_point(aes(size = abs(Estimate)), alpha = 0.8) +
    geom_errorbarh(aes(xmin = Estimate - 1.96 * StdError, 
                       xmax = Estimate + 1.96 * StdError), 
                   height = 0.2, alpha = 0.7) +
    geom_vline(xintercept = 0, linetype = "dashed", color = "darkgray", linewidth = 0.7) +
    scale_color_manual(values = sig_colors) +
    scale_size_continuous(range = c(2, 6), guide = "none") +
    labs(title = title,
         subtitle = "Estimated coefficients with 95% confidence intervals",
         x = "Effect Size (Coefficient Estimate)",
         y = "",
         color = "Statistical\nSignificance") +
    theme_minimal() +
    theme(
      plot.title = element_text(face = "bold", size = 16, hjust = 0.5),
      plot.subtitle = element_text(size = 12, color = "darkgray", hjust = 0.5),
      axis.title.x = element_text(size = 12, face = "bold"),
      axis.text.y = element_text(size = 10),
      legend.position = "top",
      legend.title = element_text(face = "bold"),
      panel.grid.minor = element_blank(),
      panel.grid.major.y = element_blank()
    )
}

```

### Personality Traits as Predictors of Substance Use

```{r lr5, echo=FALSE, cache=TRUE}
#---------------------------------------------------------------
# Generate Output
#---------------------------------------------------------------

# Create a publication-quality regression table using kable instead of stargazer
# for more reliable operation

# Function to create clean model coefficient table
create_model_coef_table <- function(model_list, drug_names) {
  # Extract key coefficients from each model
  key_vars <- c("Age", "Gender", "Education", "Nscore", "Escore", 
               "Oscore", "Ascore", "Cscore", "Impulsive", "SS")
  
  # Create data frame for results
  result_df <- data.frame(
    Variable = c("(Intercept)", key_vars),
    stringsAsFactors = FALSE
  )
  
  # Add each model's coefficients and significance
  for (drug in drug_names) {
    if (drug %in% names(model_list)) {
      model <- model_list[[drug]]$model
      coefs <- summary(model)$coefficients
      
      # Extract coefficients and p-values
      drug_coefs <- numeric(length(result_df$Variable))
      drug_p <- numeric(length(result_df$Variable))
      
      for (i in 1:length(result_df$Variable)) {
        var_name <- result_df$Variable[i]
        if (var_name %in% rownames(coefs)) {
          drug_coefs[i] <- coefs[var_name, "Estimate"]
          drug_p[i] <- coefs[var_name, "Pr(>|t|)"]
        } else {
          drug_coefs[i] <- NA
          drug_p[i] <- NA
        }
      }
      
      # Add significance stars
      drug_sig <- ifelse(drug_p < 0.001, "***", 
                       ifelse(drug_p < 0.01, "**", 
                            ifelse(drug_p < 0.05, "*", "")))
      
      # Format coefficients with significance stars
      drug_coef_text <- ifelse(!is.na(drug_coefs), 
                             paste0(sprintf("%.3f", round(drug_coefs, 3)), drug_sig), 
                             "")
      
      # Add to result dataframe
      result_df[[drug]] <- drug_coef_text
    }
  }
  
  # Add model metrics
  metrics_rows <- data.frame(
    Variable = c("N", "R²", "Adjusted R²", "F-statistic"),
    stringsAsFactors = FALSE
  )
  
  for (drug in drug_names) {
    if (drug %in% names(model_list)) {
      model_summary <- summary(model_list[[drug]]$model)
      n <- length(model_summary$residuals)
      r2 <- model_summary$r.squared
      adj_r2 <- model_summary$adj.r.squared
      f_stat <- model_summary$fstatistic[1]
      
      metrics_rows[[drug]] <- c(
        as.character(n),
        sprintf("%.3f", round(r2, 3)),
        sprintf("%.3f", round(adj_r2, 3)),
        sprintf("%.3f", round(f_stat, 3))
      )
    }
  }
  
  # Combine results and metrics
  final_df <- rbind(result_df, metrics_rows)
  
  # Clean variable names for display
  var_display_names <- c(
    "(Intercept)" = "Intercept",
    "Age" = "Age",
    "Gender" = "Gender (Male=1)",
    "Education" = "Education Level",
    "Nscore" = "Neuroticism",
    "Escore" = "Extraversion",
    "Oscore" = "Openness",
    "Ascore" = "Agreeableness",
    "Cscore" = "Conscientiousness",
    "Impulsive" = "Impulsivity",
    "SS" = "Sensation Seeking",
    "N" = "N",
    "R²" = "R²",
    "Adjusted R²" = "Adjusted R²",
    "F-statistic" = "F-statistic"
  )
  
  final_df$Variable <- var_display_names[final_df$Variable]
  
  return(final_df)
}


# Create the comparison table for the main drugs
drug_names_for_table <- names(results_list)
if (length(drug_names_for_table) > 0) {
  model_comparison_table <- create_model_coef_table(
    results_list, 
    drug_names_for_table
  )
  
  # Display the table with kable for better formatting
  kable(model_comparison_table, 
      caption = "Linear Regression Models for Drug Usage (Usage Level 0-6)",
      align = c('l', rep('r', ncol(model_comparison_table) - 1)),
      longtable = TRUE) %>%  # Add this parameter
    kable_styling(bootstrap_options = c("striped", "hover"), 
                  full_width = FALSE,
                  latex_options = c("hold_position")) %>%  # Add this parameter
    add_header_above(c(" " = 1, "Drug Models" = ncol(model_comparison_table) - 1)) %>%
    footnote(
      symbol = c("* p<0.05; ** p<0.01; *** p<0.001"),
      symbol_title = "Significance levels:",
      footnote_as_chunk = TRUE
    )
} else {
  cat("No valid drug models to display in table\n")
}
```

Based on the comprehensive statistical analysis of the drug consumption dataset, several significant patterns emerged in the relationship between personality traits and substance use. Linear regression models were developed for various substances including Cannabis, Alcohol, Nicotine, Cocaine, and Ecstasy, with the most robust predictive model being developed for Cannabis (highest adjusted R² value). The analysis revealed that Sensation Seeking (SS) and Impulsivity consistently showed strong positive correlations with substance use across multiple drugs, while Conscientiousness and Agreeableness demonstrated significant negative relationships. Demographic factors also played important roles, with Age showing a generally negative association with drug use, particularly for Cannabis and Ecstasy. Gender differences were observed across several substances, with males showing higher consumption patterns for certain drugs. The regression diagnostics indicated reasonably well-fitting models, particularly for Cannabis, where personality traits explained a substantial portion of the variance in usage patterns. These findings support existing literature suggesting that certain personality profiles may predispose individuals to higher substance use behaviors, with Sensation Seeking emerging as the strongest personality predictor across multiple substances.

### Analysis of Personality Traits as Predictors of Substance Use

```{r lr6, echo=FALSE, cache=TRUE}

# Properly define the individual models for plotting
if ("Cannabis" %in% names(results_list)) {
  cannabis_model <- results_list[["Cannabis"]]$model
  cannabis_coef_plot <- plot_factor_importance_enhanced(cannabis_model, "Predictors of Cannabis Usage")
  print(cannabis_coef_plot)
}

if ("Alcohol" %in% names(results_list)) {
  alcohol_model <- results_list[["Alcohol"]]$model
  alcohol_coef_plot <- plot_factor_importance_enhanced(alcohol_model, "Predictors of Alcohol Usage")
  print(alcohol_coef_plot)
}

if ("Nicotine" %in% names(results_list)) {
  nicotine_model <- results_list[["Nicotine"]]$model
  nicotine_coef_plot <- plot_factor_importance_enhanced(nicotine_model, "Predictors of Nicotine Usage")
  print(nicotine_coef_plot)
}
```

#### Cannabis Usage Predictors

The first plot presents the predictors of cannabis usage, showing estimated coefficients with 95% confidence intervals. Several key observations emerge:

Sensation Seeking (SS) stands out as the strongest positive predictor of cannabis use with high statistical significance (p \< 0.001). This indicates that individuals with higher sensation-seeking tendencies are substantially more likely to use cannabis.

Age shows a strong negative association (p \< 0.001), indicating that cannabis use decreases significantly with advancing age, which aligns with established patterns of drug use being more prevalent among younger populations.

Openness (Oscore) also emerges as a significant positive predictor (p \< 0.001), suggesting that individuals who are more intellectually curious and open to new experiences are more likely to use cannabis.

Neuroticism (Nscore) shows a modest positive association, while Conscientiousness (Cscore) demonstrates a negative relationship - people who are more organized and reliable tend to use cannabis less.

#### Alcohol Usage Predictors

The second plot reveals different personality dynamics for alcohol consumption:

Sensation Seeking remains significant, though with a smaller coefficient than for cannabis, suggesting that thrill-seeking behavior correlates with alcohol use but less strongly than with cannabis use.

Impulsivity appears as a stronger predictor for alcohol than it did for cannabis, indicating that spontaneous decision-making may play a larger role in alcohol consumption patterns.

Age shows a much weaker negative association compared to cannabis, which reflects alcohol's wider acceptance across age groups in many societies.

Extraversion (Escore) demonstrates a positive relationship with alcohol consumption, suggesting that more socially outgoing individuals may consume more alcohol, possibly due to its role in social interactions.

#### Nicotine Usage Predictors

The third plot for nicotine usage shows distinctive patterns:

Conscientious (Cscore) exhibits a strong negative association with nicotine use, suggesting that more disciplined, organized individuals are significantly less likely to use nicotine products.

Sensation Seeking again appears as a significant positive predictor, though with a different magnitude compared to cannabis and alcohol.

Certain country variables show stronger associations with nicotine use than they did with other substances, potentially reflecting cultural or regulatory differences in nicotine availability and social acceptance across regions.

The gender variable shows a positive coefficient, indicating that males (coded as 1) are more likely to use nicotine than females (coded as 0) when controlling for other factors.

#### Cross-Substance Comparison

Across all three substances, several consistent patterns emerge:

1.  Sensation Seeking consistently appears as a significant positive predictor across all substances, reinforcing its role as a key personality trait associated with various forms of substance use.

2.  Conscientiousness consistently shows negative associations with substance use, highlighting how personal organization and self-discipline may serve as protective factors.

3.  The strength and significance of demographic factors (age, gender, education) vary across substances, reflecting different usage patterns and societal attitudes.

4.  The confidence intervals (error bars) reveal varying levels of certainty in these predictions, with some relationships being more precisely estimated than others.

These visualizations effectively illustrate how different personality traits and demographic factors relate to substance use patterns, with some traits (particularly Sensation Seeking and Conscientiousness) showing consistent relationships across multiple substances, while others exhibit substance-specific patterns.

### Cannabis Usage Linear Regression Model: Diagnostic Analysis

```{r lr7, echo=FALSE, cache=TRUE}
#---------------------------------------------------------------
# Diagnostic Plots
#---------------------------------------------------------------

# Function to create diagnostic plots with enhanced aesthetics
create_diagnostic_plots <- function(model, title, color_scheme = "blue") {
  # Extract residuals data
  model_data <- augment(model)
  
  # Define color palette
  if (color_scheme == "blue") {
    point_color <- "#3182bd"
    line_color <- "#08519c"
    reference_color <- "#e41a1c"
  } else if (color_scheme == "green") {
    point_color <- "#31a354"
    line_color <- "#006d2c"
    reference_color <- "#d62728"
  } else {
    point_color <- "#756bb1"
    line_color <- "#54278f"
    reference_color <- "#e41a1c"
  }
  
  # Common theme elements
  diagnostic_theme <- theme_minimal() +
    theme(
      plot.title = element_text(face = "bold", size = 12),
      plot.subtitle = element_text(size = 9, color = "gray50"),
      axis.title = element_text(size = 10),
      axis.text = element_text(size = 9),
      panel.grid.minor = element_blank(),
      panel.border = element_rect(color = "gray80", fill = NA, linewidth = 0.5)
    )
  
  # 1. Residuals vs Fitted with improved aesthetics
  p1 <- ggplot(model_data, aes(x = .fitted, y = .resid)) +
    geom_point(alpha = 0.6, color = point_color, size = 1.5) +
    geom_hline(yintercept = 0, linetype = "dashed", color = reference_color, 
               linewidth = 0.7) +
    geom_smooth(se = TRUE, color = line_color, fill = alpha(line_color, 0.2), 
                method = "loess", message = FALSE, warning = FALSE) +
    labs(title = "Residuals vs Fitted",
         subtitle = "Should show random scatter around the zero line",
         x = "Fitted values",
         y = "Residuals") +
    diagnostic_theme
  
  # 2. Normal Q-Q plot with improved aesthetics
  p2 <- ggplot(model_data, aes(sample = .resid)) +
    stat_qq(color = point_color, size = 1.5, alpha = 0.6) +
    stat_qq_line(color = reference_color, linewidth = 0.7) +
    labs(title = "Normal Q-Q Plot",
         subtitle = "Points should follow the diagonal line",
         x = "Theoretical Quantiles",
         y = "Sample Quantiles") +
    diagnostic_theme
  
  # 3. Scale-Location plot with improved aesthetics
  p3 <- ggplot(model_data, aes(x = .fitted, y = sqrt(abs(.resid)))) +
    geom_point(alpha = 0.6, color = point_color, size = 1.5) +
    geom_smooth(se = TRUE, color = line_color, fill = alpha(line_color, 0.2), 
                method = "loess", message = FALSE, warning = FALSE) +
    labs(title = "Scale-Location",
         subtitle = "Should show homogeneous variance",
         x = "Fitted values",
         y = "sqrt|Standardized Residuals|") +
    diagnostic_theme
  
  # 4. Residuals vs Leverage with improved aesthetics
  # Add Cook's distance contour lines
  p4 <- ggplot(model_data, aes(x = .hat, y = .resid)) +
    geom_point(alpha = 0.6, color = point_color, size = 1.5) +
    geom_smooth(se = TRUE, color = line_color, fill = alpha(line_color, 0.2), 
                method = "loess", message = FALSE, warning = FALSE) +
    geom_hline(yintercept = 0, linetype = "dashed", color = "gray50") +
    # Add Cook's distance contours
    stat_contour(aes(z = .cooksd), breaks = c(0.5, 1), color = "red", 
                 linetype = "dashed", na.rm = TRUE) +
    labs(title = "Residuals vs Leverage",
         subtitle = "Identifies influential cases",
         x = "Leverage",
         y = "Standardized Residuals") +
    diagnostic_theme
  
  # Combine plots with better layout
  title_grob <- grid::textGrob(
    title, 
    gp = grid::gpar(fontsize = 16, fontface = "bold"),
    just = "center"
  )
  subtitle_grob <- grid::textGrob(
    "Diagnostic Plots for Linear Regression Model", 
    gp = grid::gpar(fontsize = 12, col = "gray30"),
    just = "center"
  )
  
  # Arrange the title, subtitle, and plots
  combined_plot <- gridExtra::grid.arrange(
    gridExtra::arrangeGrob(title_grob, subtitle_grob, heights = c(1, 0.5), ncol = 1),
    gridExtra::arrangeGrob(p1, p2, p3, p4, ncol = 2),
    heights = c(1, 10)
  )
  
  return(combined_plot)
}

# If we have diagnostic plots for Cannabis model
if (exists("cannabis_model") && !is.null(cannabis_model)) {
  cannabis_diagnostics <- create_diagnostic_plots(cannabis_model, "Cannabis Usage Model Diagnostics")
  print(cannabis_diagnostics)
}
```

1.  Residuals vs Fitted Plot Analysis

The Residuals vs Fitted plot examines the relationship between model predictions and their errors. In an ideal linear regression model, residuals should display random scatter around the zero line with no discernible pattern. The Cannabis model exhibits some systematic patterning in the residual distribution rather than purely random dispersion. This non-random pattern suggests the presence of unexplained structure in the data that the current linear specification fails to capture. The deviation of the smoothed blue line from horizontal indicates potential non-linear relationships between predictors and cannabis usage that warrant further investigation. Such patterns may suggest the need for polynomial terms, interaction effects, or transformation of variables to improve model specification.

2.  Normal Q-Q Plot Analysis

The Normal Q-Q plot evaluates whether model residuals conform to a normal distribution, a key assumption in linear regression. Points should ideally follow the diagonal reference line throughout the distribution. The Cannabis model shows reasonable conformity in the central region but notable departures at both extremes of the distribution. These deviations, particularly visible in the tails, indicate that the residuals exhibit heavier tails than expected under normality. This pattern suggests that the model may produce less reliable predictions for individuals with very high or very low cannabis usage levels. The non-normality could affect the validity of confidence intervals and hypothesis tests, though the regression coefficients themselves remain unbiased estimators.

3.  Scale-Location Plot Analysis

The Scale-Location plot assesses homoscedasticity—whether residual variance remains constant across all fitted values. The square root transformation of absolute standardized residuals helps visualize variance patterns. In the Cannabis model, the non-horizontal trend in the smoothed line indicates heteroscedasticity, with residual variance appearing to change across the range of predicted values. This uneven spread suggests that model precision varies depending on the level of cannabis use being predicted. The presence of heteroscedasticity does not bias coefficient estimates but may affect their efficiency and the validity of standard errors. Potential remedies include robust standard errors, weighted least squares, or variable transformations to stabilize variance.

4.  Residuals vs Leverage Plot Analysis

The Residuals vs Leverage plot identifies observations that disproportionately influence model parameters. Points with both high leverage (ability to influence) and large residuals (poor fit) warrant careful examination. Cook's distance contours (red dashed lines) demarcate thresholds for highly influential points. The Cannabis model demonstrates relatively favorable characteristics in this regard, with most observations exhibiting moderate leverage and no extreme outliers beyond the Cook's distance boundaries. This indicates that the regression results are not unduly influenced by a small number of anomalous data points, enhancing confidence in the overall stability of the model findings.

Conclusion

The diagnostic analysis reveals several limitations in the linear regression model for cannabis usage. The presence of non-random residual patterns, departures from normality, and heteroscedasticity suggest that while the model provides valuable insights into factors associated with cannabis consumption, it does not capture all relevant structures in the data. These limitations should be considered when interpreting the model's findings. Despite these limitations, the model maintains utility for its primary purpose—identifying significant predictors and their relative importance. The diagnostic results do not invalidate the substantive findings but rather contextualize their interpretation and highlight opportunities for model refinement. Future modeling efforts might benefit from exploring non-linear specifications, variable transformations, or alternative estimation methods to address the issues identified in this diagnostic assessment.

## Generalised Linear Model with family set to Poisson

(Johan Ferreira)

```{r pos1, echo=FALSE, cache=TRUE}
# Read the data
model_data <- read.csv("Data/model_data.csv")

# Remove the first index column if it exists
if(names(model_data)[1] == "X") {
  model_data <- model_data[,-1]
}

# Check structure of data
str(model_data)

# Distribution of Cannabis usage
table(model_data$Cannabis)

# Visualize Cannabis usage distribution
ggplot(model_data, aes(x = factor(Cannabis))) +
  geom_bar(fill = "steelblue") +
  theme_minimal() +
  labs(title = "Distribution of Cannabis Usage",
       x = "Usage Level (0-6)",
       y = "Count") 

# Define the predictors to use in models
predictors <- c("Age", "Gender", "Education", 
                "Nscore", "Escore", "Oscore", "Ascore", "Cscore", 
                "Impulsive", "SS")

# Function to fit Poisson GLM for a specific drug
fit_poisson_glm <- function(data, drug, predictors) {
  # Create formula
  formula_str <- paste(drug, "~", paste(predictors, collapse = " + "))
  formula <- as.formula(formula_str)
  
  # Fit Poisson GLM
  model <- glm(formula, data = data, family = poisson(link = "log"))
  
  # Return model
  return(model)
}

# Drugs to model
drugs <- c("Cannabis", "Alcohol", "Nicotine", "Coke")

# Fit models for each drug
models <- list()
for (drug in drugs) {
  if (drug %in% names(model_data)) {
    models[[drug]] <- fit_poisson_glm(model_data, drug, predictors)
    cat("Model fitted for", drug, "\n")
  } else {
    cat("Drug column", drug, "not found in dataset\n")
  }
}

# Function to create a summary table for a model
create_model_summary <- function(model) {
  # Extract model summary
  model_summary <- summary(model)
  
  # Extract coefficients and statistics
  coefs <- model_summary$coefficients
  
  # Calculate exp(Coefficients) for interpretation
  exp_coefs <- exp(coefs[, "Estimate"])
  
  # Create data frame for display
  results <- data.frame(
    Variable = rownames(coefs),
    Estimate = coefs[, "Estimate"],
    Std_Error = coefs[, "Std. Error"],
    z_value = coefs[, "z value"],
    p_value = coefs[, "Pr(>|z|)"],
    exp_Estimate = exp_coefs,
    stringsAsFactors = FALSE
  )
  
  # Add significance stars
  results$significance <- ""
  results$significance[results$p_value < 0.1] <- "."
  results$significance[results$p_value < 0.05] <- "*"
  results$significance[results$p_value < 0.01] <- "**"
  results$significance[results$p_value < 0.001] <- "***"
  
  # Add percentage change column for non-intercept terms
  results$percent_change <- NA
  for (i in 2:nrow(results)) {  # Skip intercept
    if (results$exp_Estimate[i] > 1) {
      results$percent_change[i] <- paste0("+", round((results$exp_Estimate[i] - 1) * 100, 2), "%")
    } else {
      results$percent_change[i] <- paste0("-", round((1 - results$exp_Estimate[i]) * 100, 2), "%")
    }
  }
  
  return(results)
}

# Create summary tables for all models
model_summaries <- list()
for (drug in names(models)) {
  model_summaries[[drug]] <- create_model_summary(models[[drug]])
}

# Print Cannabis model summary
print(model_summaries[["Cannabis"]])

# Compare model fit statistics
model_comparison <- data.frame(
  Drug = character(),
  AIC = numeric(),
  BIC = numeric(),
  LogLik = numeric(),
  Deviance = numeric(),
  PseudoR2 = numeric(),
  stringsAsFactors = FALSE
)

for (drug in names(models)) {
  model <- models[[drug]]
  
  # Calculate McFadden's Pseudo R²
  null_model <- glm(as.formula(paste(drug, "~ 1")), 
                    data = model_data, 
                    family = poisson(link = "log"))
  pseudo_r2 <- 1 - (logLik(model) / logLik(null_model))
  
  model_comparison <- rbind(model_comparison, data.frame(
    Drug = drug,
    AIC = AIC(model),
    BIC = BIC(model),
    LogLik = as.numeric(logLik(model)),
    Deviance = model$deviance,
    PseudoR2 = as.numeric(pseudo_r2),
    stringsAsFactors = FALSE
  ))
}

# Print model comparison
print(model_comparison)

# Check for overdispersion in Cannabis model
cannabis_model <- models[["Cannabis"]]
dispersion_param <- sum(residuals(cannabis_model, type = "pearson")^2) / cannabis_model$df.residual
cat("Dispersion parameter for Cannabis model:", round(dispersion_param, 4), "\n")

if (dispersion_param > 1.5) {
  cat("Evidence of overdispersion. Consider using a negative binomial model instead.\n")
} else {
  cat("No strong evidence of overdispersion. Poisson model appears appropriate.\n")
}

# Fit negative binomial model for comparison (if MASS package is available)
if (requireNamespace("MASS", quietly = TRUE)) {
  nb_model <- MASS::glm.nb(Cannabis ~ Age + Gender + Education + 
                             Nscore + Escore + Oscore + Ascore + Cscore + 
                             Impulsive + SS, data = model_data)
  
  # Compare AIC between Poisson and negative binomial
  cat("\nModel comparison - Cannabis:\n")
  cat("Poisson AIC:", AIC(cannabis_model), "\n")
  cat("Negative Binomial AIC:", AIC(nb_model), "\n")
  cat("Theta value in NB model:", nb_model$theta, "\n")
}

# Visualize effects of key predictors on Cannabis usage
# Use coefficients from the model to calculate predicted values

# For Age effect
age_range <- 1:6  # Age categories 18-24 through 65+
age_effect <- data.frame(
  Age = age_range,
  Predicted = exp(coef(cannabis_model)["(Intercept)"] + coef(cannabis_model)["Age"] * age_range)
)

ggplot(age_effect, aes(x = Age, y = Predicted)) +
  geom_line(color = "blue", size = 1) +
  geom_point(color = "blue", size = 3) +
  theme_minimal() +
  labs(title = "Effect of Age on Predicted Cannabis Usage",
       x = "Age Category (1=18-24, 6=65+)",
       y = "Predicted Cannabis Usage Level") +
  scale_x_continuous(breaks = 1:6)

# For Sensation Seeking effect (keeping other variables at their means)
ss_range <- seq(min(model_data$SS), max(model_data$SS), length.out = 100)
mean_values <- colMeans(model_data[, predictors[predictors != "SS"]])

ss_effect <- data.frame(
  SS = ss_range,
  Predicted = exp(coef(cannabis_model)["(Intercept)"] + 
                    coef(cannabis_model)["Age"] * mean_values["Age"] +
                    coef(cannabis_model)["Gender"] * mean_values["Gender"] +
                    coef(cannabis_model)["Education"] * mean_values["Education"] +
                    coef(cannabis_model)["Nscore"] * mean_values["Nscore"] +
                    coef(cannabis_model)["Escore"] * mean_values["Escore"] +
                    coef(cannabis_model)["Oscore"] * mean_values["Oscore"] +
                    coef(cannabis_model)["Ascore"] * mean_values["Ascore"] +
                    coef(cannabis_model)["Cscore"] * mean_values["Cscore"] +
                    coef(cannabis_model)["Impulsive"] * mean_values["Impulsive"] +
                    coef(cannabis_model)["SS"] * ss_range)
)

ggplot(ss_effect, aes(x = SS, y = Predicted)) +
  geom_line(color = "red", size = 1) +
  theme_minimal() +
  labs(title = "Effect of Sensation Seeking on Predicted Cannabis Usage",
       x = "Sensation Seeking Score",
       y = "Predicted Cannabis Usage Level")

# Create enhanced coefficient plot for Cannabis model similar to the Drug Consumption.Rmd file
# Function to create a visually appealing coefficient plot
plot_factor_importance_enhanced <- function(model, title, color_scheme = "viridis") {
  # Extract model coefficients
  coefs <- summary(model)$coefficients
  
  # Filter out intercept and create data frame for plotting
  coef_df <- data.frame(
    Variable = rownames(coefs)[-1],  # Exclude intercept
    Estimate = coefs[-1, "Estimate"],
    StdError = coefs[-1, "Std. Error"],
    PValue = coefs[-1, "Pr(>|z|)"]
  )
  
  # Add significance markers and categories
  coef_df$Significance <- ifelse(coef_df$PValue < 0.001, "p < 0.001", 
                                 ifelse(coef_df$PValue < 0.01, "p < 0.01",
                                        ifelse(coef_df$PValue < 0.05, "p < 0.05", "Not significant")))
  
  # Sort by absolute value of estimate
  coef_df <- coef_df[order(abs(coef_df$Estimate), decreasing = TRUE), ]
  
  # Clean up variable names for display
  var_display_mapping <- c(
    "Age" = "Age",
    "Gender" = "Gender (Male=1)",
    "Education" = "Education Level",
    "Nscore" = "Neuroticism",
    "Escore" = "Extraversion",
    "Oscore" = "Openness",
    "Ascore" = "Agreeableness",
    "Cscore" = "Conscientiousness",
    "Impulsive" = "Impulsivity",
    "SS" = "Sensation Seeking"
  )
  
  # Function to clean up variable names
  clean_var_names <- function(var_name) {
    # First check exact matches in our mapping
    if (var_name %in% names(var_display_mapping)) {
      return(var_display_mapping[var_name])
    }
    
    # Handle country variables
    if (grepl("^Country", var_name)) {
      return(gsub("Country", "Country: ", var_name))
    }
    
    # Handle ethnicity variables
    if (grepl("^Ethnicity", var_name)) {
      return(gsub("Ethnicity", "Ethnicity: ", var_name))
    }
    
    return(var_name)
  }
  
  # Apply name cleaning
  coef_df$DisplayName <- sapply(coef_df$Variable, clean_var_names)
  
  # Reorder factor levels for plotting
  coef_df$DisplayName <- factor(coef_df$DisplayName, levels = rev(coef_df$DisplayName))
  
  # Define significance color palette
  if (color_scheme == "viridis") {
    sig_colors <- c("p < 0.001" = "#440154", "p < 0.01" = "#21908C", 
                    "p < 0.05" = "#5DC863", "Not significant" = "#CCCCCC")
  } else {
    sig_colors <- c("p < 0.001" = "#0072B2", "p < 0.01" = "#009E73", 
                    "p < 0.05" = "#56B4E9", "Not significant" = "#CCCCCC")
  }
  
  # Plot with enhanced aesthetics
  ggplot(coef_df, aes(x = Estimate, y = DisplayName, color = Significance)) +
    geom_point(aes(size = abs(Estimate)), alpha = 0.8) +
    geom_errorbarh(aes(xmin = Estimate - 1.96 * StdError, 
                       xmax = Estimate + 1.96 * StdError), 
                   height = 0.2, alpha = 0.7) +
    geom_vline(xintercept = 0, linetype = "dashed", color = "darkgray", linewidth = 0.7) +
    scale_color_manual(values = sig_colors) +
    scale_size_continuous(range = c(2, 6), guide = "none") +
    labs(title = title,
         subtitle = "Estimated coefficients with 95% confidence intervals",
         x = "Effect Size (Coefficient Estimate)",
         y = "",
         color = "Statistical\nSignificance") +
    theme_minimal() +
    theme(
      plot.title = element_text(face = "bold", size = 16, hjust = 0.5),
      plot.subtitle = element_text(size = 12, color = "darkgray", hjust = 0.5),
      axis.title.x = element_text(size = 12, face = "bold"),
      axis.text.y = element_text(size = 10),
      legend.position = "top",
      legend.title = element_text(face = "bold"),
      panel.grid.minor = element_blank(),
      panel.grid.major.y = element_blank()
    )
}

# Create diagnostic plots with enhanced aesthetics similar to Drug Consumption.Rmd
create_diagnostic_plots <- function(model, title, color_scheme = "blue") {
  # Extract residuals data for plotting
  model_data <- broom::augment(model)
  
  # Define color palette
  if (color_scheme == "blue") {
    point_color <- "#3182bd"
    line_color <- "#08519c"
    reference_color <- "#e41a1c"
  } else if (color_scheme == "green") {
    point_color <- "#31a354"
    line_color <- "#006d2c"
    reference_color <- "#d62728"
  } else {
    point_color <- "#756bb1"
    line_color <- "#54278f"
    reference_color <- "#e41a1c"
  }
  
  # Common theme elements
  diagnostic_theme <- theme_minimal() +
    theme(
      plot.title = element_text(face = "bold", size = 12),
      plot.subtitle = element_text(size = 9, color = "gray50"),
      axis.title = element_text(size = 10),
      axis.text = element_text(size = 9),
      panel.grid.minor = element_blank(),
      panel.border = element_rect(color = "gray80", fill = NA, linewidth = 0.5)
    )
  
  # 1. Residuals vs Fitted with improved aesthetics
  p1 <- ggplot(model_data, aes(x = .fitted, y = .resid)) +
    geom_point(alpha = 0.6, color = point_color, size = 1.5) +
    geom_hline(yintercept = 0, linetype = "dashed", color = reference_color, 
               linewidth = 0.7) +
    geom_smooth(se = TRUE, color = line_color, fill = alpha(line_color, 0.2), 
                method = "loess") +
    labs(title = "Residuals vs Fitted",
         subtitle = "Should show random scatter around the zero line",
         x = "Fitted values",
         y = "Residuals") +
    diagnostic_theme
  
  # 2. Normal Q-Q plot with improved aesthetics
  p2 <- ggplot(model_data, aes(sample = .resid)) +
    stat_qq(color = point_color, size = 1.5, alpha = 0.6) +
    stat_qq_line(color = reference_color, linewidth = 0.7) +
    labs(title = "Normal Q-Q Plot",
         subtitle = "Points should follow the diagonal line",
         x = "Theoretical Quantiles",
         y = "Sample Quantiles") +
    diagnostic_theme
  
  # 3. Scale-Location plot with improved aesthetics
  p3 <- ggplot(model_data, aes(x = .fitted, y = sqrt(abs(.resid)))) +
    geom_point(alpha = 0.6, color = point_color, size = 1.5) +
    geom_smooth(se = TRUE, color = line_color, fill = alpha(line_color, 0.2), 
                method = "loess") +
    labs(title = "Scale-Location",
         subtitle = "Should show homogeneous variance",
         x = "Fitted values",
         y = "sqrt|Standardized Residuals|") +
    diagnostic_theme
  
  # 4. Residuals vs Leverage with improved aesthetics
  # Add Cook's distance contour lines
  p4 <- ggplot(model_data, aes(x = .hat, y = .resid)) +
    geom_point(alpha = 0.6, color = point_color, size = 1.5) +
    geom_smooth(se = TRUE, color = line_color, fill = alpha(line_color, 0.2), 
                method = "loess") +
    geom_hline(yintercept = 0, linetype = "dashed", color = "gray50") +
    # Add Cook's distance contours if available
    stat_contour(aes(z = .cooksd), breaks = c(0.5, 1), color = "red", 
                 linetype = "dashed", na.rm = TRUE) +
    labs(title = "Residuals vs Leverage",
         subtitle = "Identifies influential cases",
         x = "Leverage",
         y = "Standardized Residuals") +
    diagnostic_theme
  
  # Combine plots with better layout using gridExtra
  title_grob <- grid::textGrob(
    title, 
    gp = grid::gpar(fontsize = 16, fontface = "bold"),
    just = "center"
  )
  subtitle_grob <- grid::textGrob(
    "Diagnostic Plots for Poisson GLM", 
    gp = grid::gpar(fontsize = 12, col = "gray30"),
    just = "center"
  )
  
  # Arrange the title, subtitle, and plots
  combined_plot <- gridExtra::grid.arrange(
    gridExtra::arrangeGrob(title_grob, subtitle_grob, heights = c(1, 0.5), ncol = 1),
    gridExtra::arrangeGrob(p1, p2, p3, p4, ncol = 2),
    heights = c(1, 10)
  )
  
  return(combined_plot)
}

# Generate the enhanced plots
cannabis_coef_plot <- plot_factor_importance_enhanced(cannabis_model, 
                                                      "Predictors of Cannabis Usage")
print(cannabis_coef_plot)

# Generate enhanced diagnostic plots
cannabis_diagnostics <- create_diagnostic_plots(cannabis_model, 
                                                "Cannabis Usage Model Diagnostics")
print(cannabis_diagnostics)

# Check for multicollinearity
car::vif(cannabis_model)

# Function to run a more detailed analysis for the Cannabis model
analyze_cannabis_model <- function(model) {
  # Get model summary
  model_summary <- summary(model)
  
  # Print key statistics
  cat("\n=== Cannabis Model Analysis ===\n")
  cat("Number of observations:", nrow(model$model), "\n")
  cat("Null deviance:", model$null.deviance, "on", model$df.null, "degrees of freedom\n")
  cat("Residual deviance:", model$deviance, "on", model$df.residual, "degrees of freedom\n")
  cat("AIC:", model$aic, "\n")
  
  # Calculate and print McFadden's Pseudo R²
  null_model <- glm(Cannabis ~ 1, data = model_data, family = poisson(link = "log"))
  pseudo_r2 <- 1 - (logLik(model) / logLik(null_model))
  cat("McFadden's Pseudo R²:", round(as.numeric(pseudo_r2), 4), "\n")
  
  # Check for overdispersion
  dispersion <- sum(residuals(model, type = "pearson")^2) / model$df.residual
  cat("Dispersion parameter:", round(dispersion, 4), "\n")
  
  # Identify significant predictors
  sig_coefs <- coef(model_summary)[, c("Estimate", "Pr(>|z|)")]
  sig_coefs <- sig_coefs[2:nrow(sig_coefs), ]  # Remove intercept
  sig_coefs <- sig_coefs[sig_coefs[, "Pr(>|z|)"] < 0.05, ]
  sig_coefs <- sig_coefs[order(abs(sig_coefs[, "Estimate"]), decreasing = TRUE), ]
  
  cat("\nSignificant predictors (in order of effect size):\n")
  for (i in 1:nrow(sig_coefs)) {
    var_name <- rownames(sig_coefs)[i]
    estimate <- sig_coefs[i, "Estimate"]
    exp_est <- exp(estimate)
    direction <- ifelse(estimate > 0, "positive", "negative")
    
    if (exp_est > 1) {
      effect <- paste0(round((exp_est - 1) * 100, 2), "% increase")
    } else {
      effect <- paste0(round((1 - exp_est) * 100, 2), "% decrease")
    }
    
    cat(paste0(i, ". ", var_name, ": ", direction, " effect (", effect, ", p=", 
               round(sig_coefs[i, "Pr(>|z|)"], 4), ")\n"))
  }
  
  # Identify potential outliers
  pearson_resid <- residuals(model, type = "pearson")
  potential_outliers <- which(abs(pearson_resid) > 2)
  
  if (length(potential_outliers) > 0) {
    cat("\nPotential outliers (observations with |Pearson residual| > 2):\n")
    outlier_data <- data.frame(
      Observation = potential_outliers,
      Residual = pearson_resid[potential_outliers],
      Actual = model$model$Cannabis[potential_outliers],
      Predicted = fitted(model)[potential_outliers]
    )
    outlier_data <- outlier_data[order(abs(outlier_data$Residual), decreasing = TRUE), ]
    print(head(outlier_data, 5))
  }
  
  # Suggest possible model improvements
  cat("\nPossible model improvements:\n")
  if (dispersion > 1.2) {
    cat("- Consider using a negative binomial model to address overdispersion\n")
  }
  cat("- Consider interaction terms (e.g., Age × Education, Gender × SS)\n")
  cat("- Consider polynomial terms for continuous predictors if relationship is non-linear\n")
  
  return(invisible(NULL))
}

# Run detailed analysis for Cannabis model
analyze_cannabis_model(cannabis_model)

# If requested, fit models with interaction terms
cannabis_model_interactions <- glm(
  Cannabis ~ Age + Gender + Education + 
    Nscore + Escore + Oscore + Ascore + Cscore + Impulsive + SS +
    Age:Education + Gender:SS,
  data = model_data, 
  family = poisson(link = "log")
)

# Compare models with and without interactions
anova(cannabis_model, cannabis_model_interactions, test = "Chisq")
```

## Generalised Linear Model with family set to Binomial

Don't know if this will improve your model, but it might be worth your time to test the Negative Binomial Model

## Generalised Additive Model

## Neural Network

## Support Vector Machine

# How we used Generative AI in our project

– how you used generative AI in redacting the group work (code-related questions, generate text, explain concepts…)\
– what was easy/hard/impossible to do with generative AI\
– what you had to pay attention to/be critical about when using the results obtained through the use of generative AI

# Conclusion

# Source

<https://archive.ics.uci.edu/dataset/373/drug+consumption+quantified>
